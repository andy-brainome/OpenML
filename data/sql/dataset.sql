INSERT INTO `dataset` (`did`, `uploader`, `source`, `name`, `version`, `version_label`, `description`, `format`, `creator`, `contributor`, `collection_date`, `upload_date`, `language`, `licence`, `citation`, `collection`, `url`, `isOriginal`, `file_id`, `default_target_attribute`, `row_id_attribute`, `ignore_attribute`, `paper_url`, `visibility`, `original_data_id`, `original_data_url`, `update_comment`, `last_update`) VALUES
(1, 1, 0, 'anneal', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title of Database: Annealing Data\n \n 2. Source Information: donated by David Sterling and Wray Buntine.\n \n 3. Past Usage: unknown\n \n 4. Relevant Information:\n    -- Explanation: I suspect this was left by Ross Quinlan in 1987 at the\n       4th Machine Learning Workshop.  I\'d have to check with Jeff Schlimmer\n       to double check this.\n \n 5. Number of Instances: 798\n \n 6. Number of Attributes: 38\n    -- 6 continuously-valued\n    -- 3 integer-valued\n    -- 29 nominal-valued\n \n 7. Attribute Information:\n     1. family:          --,GB,GK,GS,TN,ZA,ZF,ZH,ZM,ZS\n     2. product-type:    C, H, G\n     3. steel:           -,R,A,U,K,M,S,W,V\n     4. carbon:          continuous\n     5. hardness:        continuous\n     6. temper_rolling:  -,T\n     7. condition:       -,S,A,X\n     8. formability:     -,1,2,3,4,5\n     9. strength:        continuous\n    10. non-ageing:      -,N\n    11. surface-finish:  P,M,-\n    12. surface-quality: -,D,E,F,G\n    13. enamelability:   -,1,2,3,4,5\n    14. bc:              Y,-\n    15. bf:              Y,-\n    16. bt:              Y,-\n    17. bw/me:           B,M,-\n    18. bl:              Y,-\n    19. m:               Y,-\n    20. chrom:           C,-\n    21. phos:            P,-\n    22. cbond:           Y,-\n    23. marvi:           Y,-\n    24. exptl:           Y,-\n    25. ferro:           Y,-\n    26. corr:            Y,-\n    27. blue/bright/varn/clean:          B,R,V,C,-\n    28. lustre:          Y,-\n    29. jurofm:          Y,-\n    30. s:               Y,-\n    31. p:               Y,-\n    32. shape:           COIL, SHEET\n    33. thick:           continuous\n    34. width:           continuous\n    35. len:             continuous\n    36. oil:             -,Y,N\n    37. bore:            0000,0500,0600,0760\n    38. packing: -,1,2,3\n    classes:        1,2,3,4,5,U\n  \n    -- The \'-\' values are actually \'not_applicable\' values rather than\n       \'missing_values\' (and so can be treated as legal discrete\n       values rather than as showing the absence of a discrete value).\n \n 8. Missing Attribute Values: Signified with \"?\"\n    Attribute:  Number of instances missing its value:\n    1           0\n    2           0\n    3           70\n    4           0\n    5           0\n    6           675\n    7           271\n    8           283\n    9           0\n   10           703\n   11           790\n   12           217\n   13           785\n   14           797\n   15           680\n   16           736\n   17           609\n   18           662\n   19           798\n   20           775\n   21           791\n   22           730\n   23           798\n   24           796\n   25           772\n   26           798\n   27           793\n   28           753\n   29           798\n   30           798\n   31           798\n   32           0\n   33           0\n   34           0\n   35           0\n   36           740\n   37           0\n   38           789\n   39           0\n \n 9. Distribution of Classes\n      Class Name:   Number of Instances:\n      1               8\n      2              88\n      3             608\n      4               0\n      5              60\n      U              34\n                    ---\n                    798', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:19:24', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/1666876/phpFsFYVN', 'true', 1, 'class', NULL, NULL, NULL, 'public', NULL, NULL, 'Restoring dataset file', '2015-09-02 00:49:04'),
(2, 1, 0, 'kr-vs-kp', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Chess End-Game -- King+Rook versus King+Pawn on a7\n    (usually abbreviated KRKPA7).  The pawn on a7 means it is one square\n    away from queening.  It is the King+Rook\'s side (white) to move.\n \n 2. Sources:\n     (a) Database originally generated and described by Alen Shapiro.\n     (b) Donor/Coder: Rob Holte (holte@uottawa.bitnet).  The database\n         was supplied to Holte by Peter Clark of the Turing Institute\n         in Glasgow (pete@turing.ac.uk).\n     (c) Date: 1 August 1989\n \n 3. Past Usage:\n      - Alen D. Shapiro (1983,1987), \"Structured Induction in Expert Systems\",\n        Addison-Wesley.  This book is based on Shapiro\'s Ph.D. thesis (1983)\n        at the University of Edinburgh entitled \"The Role of Structured\n        Induction in Expert Systems\".\n      - Stephen Muggleton (1987), \"Structuring Knowledge by Asking Questions\",\n        pp.218-229 in \"Progress in Machine Learning\", edited by I. Bratko\n        and Nada Lavrac, Sigma Press, Wilmslow, England  SK9 5BB.\n      - Robert C. Holte, Liane Acker, and Bruce W. Porter (1989),\n        \"Concept Learning and the Problem of Small Disjuncts\",\n        Proceedings of IJCAI.  Also available as technical report AI89-106,\n        Computer Sciences Department, University of Texas at Austin,\n        Austin, Texas 78712.\n \n 4. Relevant Information:\n       The dataset format is described below.  Note: the format of this\n     database was modified on 2/26/90 to conform with the format of all\n     the other databases in the UCI repository of machine learning databases.\n \n 5. Number of Instances: 3196 total\n \n 6. Number of Attributes: 36\n \n 7. Attribute Summaries:\n     Classes (2):  -- White-can-win (\"won\") and White-cannot-win (\"nowin\").\n           I believe that White is deemed to be unable to win if the Black pawn\n           can safely advance.\n     Attributes: see Shapiro\'s book.\n \n 8. Missing Attributes: --  none\n \n 9. Class Distribution:\n     In 1669 of the positions (52%), White can win.\n     In 1527 of the positions (48%), White cannot win.\n \n The format for instances in this database is a sequence of 37 attribute values.\n Each instance is a board-descriptions for this chess endgame.  The first\n 36 attributes describe the board.  The last (37th) attribute is the\n classification: \"win\" or \"nowin\".  There are 0 missing values.\n A typical board-description is\n \n f,f,f,f,f,f,f,f,f,f,f,f,l,f,n,f,f,t,f,f,f,f,f,f,f,t,f,f,f,f,f,f,f,t,t,n,won\n \n The names of the features do not appear in the board-descriptions.\n Instead, each feature correponds to a particular position in the\n feature-value list.  For example, the head of this list is the value\n for the feature \"bkblk\".  The following is the list of features, in\n the order in which their values appear in the feature-value list:\n \n [bkblk,bknwy,bkon8,bkona,bkspr,bkxbq,bkxcr,bkxwp,blxwp,bxqsq,cntxt,dsopp,dwipd,\n  hdchk,katri,mulch,qxmsq,r2ar8,reskd,reskr,rimmx,rkxwp,rxmsq,simpl,skach,skewr,\n  skrxp,spcop,stlmt,thrsk,wkcti,wkna8,wknck,wkovl,wkpos,wtoeg]\n \n In the file, there is one instance (board position) per line.\n \n \n Num Instances:     3196\n Num Attributes:    37\n Num Continuous:    0 (Int 0 / Real 0)\n Num Discrete:      37\n Missing values:    0 /  0.0%\n\n     name                      type enum ints real     missing    distinct  (1)\n   1 \'bkblk\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   2 \'bknwy\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   3 \'bkon8\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   4 \'bkona\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   5 \'bkspr\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   6 \'bkxbq\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   7 \'bkxcr\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   8 \'bkxwp\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   9 \'blxwp\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  10 \'bxqsq\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  11 \'cntxt\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  12 \'dsopp\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  13 \'dwipd\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  14 \'hdchk\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  15 \'katri\'                   Enum 100%   0%   0%     0 /  0%     3 /  0%   0% \n  16 \'mulch\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  17 \'qxmsq\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  18 \'r2ar8\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  19 \'reskd\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  20 \'reskr\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  21 \'rimmx\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  22 \'rkxwp\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  23 \'rxmsq\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  24 \'simpl\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  25 \'skach\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  26 \'skewr\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  27 \'skrxp\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  28 \'spcop\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  29 \'stlmt\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  30 \'thrsk\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  31 \'wkcti\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  32 \'wkna8\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  33 \'wknck\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  34 \'wkovl\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  35 \'wkpos\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  36 \'wtoeg\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  37 \'class\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0%', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:19:28', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3/dataset_3_kr-vs-kp.arff', 'true', 2, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:19:28'),
(3, 1, 0, 'letter', '1', '1', '**Author**: David J. Slate  \r\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition) - 01-01-1991  \r\n**Please cite**: P. W. Frey and D. J. Slate. \"Letter Recognition Using Holland-style Adaptive Classifiers\". Machine Learning 6(2), 1991  \r\n\r\n1. TITLE: \r\n  Letter Image Recognition Data \r\n \r\n    The objective is to identify each of a large number of black-and-white\r\n    rectangular pixel displays as one of the 26 capital letters in the English\r\n    alphabet.  The character images were based on 20 different fonts and each\r\n    letter within these 20 fonts was randomly distorted to produce a file of\r\n    20,000 unique stimuli.  Each stimulus was converted into 16 primitive\r\n    numerical attributes (statistical moments and edge counts) which were then\r\n    scaled to fit into a range of integer values from 0 through 15.  We\r\n    typically train on the first 16000 items and then use the resulting model\r\n    to predict the letter category for the remaining 4000.  See the article\r\n    cited above for more details.', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:19:41', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/6/dataset_6_letter.arff', 'true', 3, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:19:41'),
(4, 1, 0, 'balance-scale', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Balance Scale Weight & Distance Database\n\n2. Source Information: \n    (a) Source: Generated to model psychological experiments reported\n		by Siegler, R. S. (1976).  Three Aspects of Cognitive\n		Development.  Cognitive Psychology, 8, 481-520.\n    (b) Donor: Tim Hume (hume@ics.uci.edu)\n    (c) Date: 22 April 1994\n\n3. Past Usage: (possibly different formats of this data)\n   - Publications\n	1. Klahr, D., & Siegler, R.S. (1978).  The Representation of\n	   Children\'s Knowledge.  In H. W. Reese & L. P. Lipsitt (Eds.),\n	   Advances in Child Development and Behavior, pp. 61-116.  New\n	   York: Academic Press \n	2. Langley,P. (1987).  A General Theory of Discrimination\n	   Learning.  In D. Klahr, P. Langley, & R. Neches (Eds.),\n	   Production System Models of Learning and Development, pp.\n	   99-161. Cambridge, MA: MIT Press\n	3. Newell, A. (1990).  Unified Theories of Cognition.\n	   Cambridge, MA: Harvard University Press\n	4. McClelland, J.L. (1988).  Parallel Distibuted Processing:\n	   Implications for Cognition and Development.  Technical\n	   Report AIP-47, Department of Psychology, Carnegie-Mellon\n	   University \n	5. Shultz, T., Mareschal, D., & Schmidt, W. (1994).  Modeling\n	   Cognitive Development on Balance Scale Phenomena. Machine\n	   Learning, Vol. 16, pp. 59-88.\n\n4. Relevant Information: \n	This data set was generated to model psychological\n	experimental results.  Each example is classified as having the\n	balance scale tip to the right, tip to the left, or be\n	balanced.  The attributes are the left weight, the left\n	distance, the right weight, and the right distance.  The\n	correct way to find the class is the greater of \n	(left-distance * left-weight) and (right-distance *\n	right-weight).  If they are equal, it is balanced.\n\n5. Number of Instances: 625 (49 balanced, 288 left, 288 right)\n\n6. Number of Attributes: 4 (numeric) + class name = 5\n\n7. Attribute Information:\n	1. Class Name: 3 (L, B, R)\n	2. Left-Weight: 5 (1, 2, 3, 4, 5)\n	3. Left-Distance: 5 (1, 2, 3, 4, 5)\n	4. Right-Weight: 5 (1, 2, 3, 4, 5)\n	5. Right-Distance: 5 (1, 2, 3, 4, 5)\n\n8. Missing Attribute Values: \n	none\n\n9. Class Distribution: \n   1. 46.08 percent are L\n   2. 07.84 percent are B\n   3. 46.08 percent are R', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:19:55', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/11/dataset_11_balance-scale.arff', 'true', 4, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:19:55'),
(5, 1, 0, 'mfeat-factors', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe multi-feature digit dataset\n -------------------------------\n \n Oowned and donated by:\n ----------------------\n \n Robert P.W. Duin\n Department of Applied Physics \n Delft University of Technology \n P.O. Box 5046, 2600 GA Delft\n The Netherlands\n \n email: duin@ph.tn.tudelft.nl\n http : //www.ph.tn.tudelft.nl/~duin\n tel +31 15 2786143\n \n Usage\n -----\n A slightly different version of the database is used in\n \n M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog, Handwritten\n      digit recognition by combined classifiers, Kybernetika, vol. 34, no. 4,\n      1998, 381-386.\n \n M. van Breukelen and R.P.W. Duin, Neural Network Initialization by Combined\n      Classifiers, in: A.K. Jain, S. Venkatesh, B.C. Lovell (eds.), ICPR\'98,\n      Proc. 14th Int. Conference on Pattern Recognition (Brisbane, Aug. 16-20),\n \n The database as it is is used in:\n \n A.K. Jain, R.P.W. Duin, J. Mao, Statisitcal Pattern Recognition: A Review,\n      in preparation\n \n Description\n -----------\n \n This dataset consists of features of handwritten numerals (`0\'--`9\')\n extracted from a collection of Dutch utility maps. 200 patterns per\n class (for a total of 2,000 patterns) have been digitized in  binary\n images. These digits are represented in terms of the following six\n feature sets (files): \n \n 1. mfeat-fou: 76 Fourier coefficients of the character shapes; \n 2. mfeat-fac: 216 profile correlations; \n 3. mfeat-kar: 64 Karhunen-Love coefficients; \n 4. mfeat-pix: 240 pixel averages in 2 x 3 windows; \n 5. mfeat-zer: 47 Zernike moments; \n 6. mfeat-mor: 6 morphological features. \n \n In each file the 2000 patterns are stored in ASCI on 2000 lines. The\n first 200 patterns are of class `0\', followed by sets of 200 patterns\n for each of the classes `1\' - `9\'. Corresponding patterns in different\n feature sets (files) correspond to the same original character.\n \n The source image dataset is lost. Using the pixel-dataset (mfeat-pix)\n sampled versions of the original images may be obtained (15 x 16 pixels).\n \n Total number of instances:\n --------------------------\n 2000 (200 instances per class)\n \n Total number of attributes:\n ---------------------------\n 649 (distributed over 6 datasets,see above)\n \n no missing attributes\n \n Total number of classes:\n ------------------------\n 10\n \n Format:\n ------\n 6 files, see above.\n Each file contains 2000 lines, one for each instance.\n Attributes are SPACE separated and can be loaded by Matlab as\n > load filename\n No missing attributes. Some are integer, others are real.\n \n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:20:04', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/12/dataset_12_mfeat-factors.arff', 'true', 5, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:20:04'),
(6, 1, 0, 'mfeat-fourier', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe multi-feature digit dataset\n -------------------------------\n \n Oowned and donated by:\n ----------------------\n \n Robert P.W. Duin\n Department of Applied Physics \n Delft University of Technology \n P.O. Box 5046, 2600 GA Delft\n The Netherlands\n \n email: duin@ph.tn.tudelft.nl\n http : //www.ph.tn.tudelft.nl/~duin\n tel +31 15 2786143\n \n Usage\n -----\n A slightly different version of the database is used in\n \n M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog, Handwritten\n      digit recognition by combined classifiers, Kybernetika, vol. 34, no. 4,\n      1998, 381-386.\n \n M. van Breukelen and R.P.W. Duin, Neural Network Initialization by Combined\n      Classifiers, in: A.K. Jain, S. Venkatesh, B.C. Lovell (eds.), ICPR\'98,\n      Proc. 14th Int. Conference on Pattern Recognition (Brisbane, Aug. 16-20),\n \n The database as it is is used in:\n \n A.K. Jain, R.P.W. Duin, J. Mao, Statisitcal Pattern Recognition: A Review,\n      in preparation\n \n Description\n -----------\n \n This dataset consists of features of handwritten numerals (`0\'--`9\')\n extracted from a collection of Dutch utility maps. 200 patterns per\n class (for a total of 2,000 patterns) have been digitized in  binary\n images. These digits are represented in terms of the following six\n feature sets (files): \n \n 1. mfeat-fou: 76 Fourier coefficients of the character shapes; \n 2. mfeat-fac: 216 profile correlations; \n 3. mfeat-kar: 64 Karhunen-Love coefficients; \n 4. mfeat-pix: 240 pixel averages in 2 x 3 windows; \n 5. mfeat-zer: 47 Zernike moments; \n 6. mfeat-mor: 6 morphological features. \n \n In each file the 2000 patterns are stored in ASCI on 2000 lines. The\n first 200 patterns are of class `0\', followed by sets of 200 patterns\n for each of the classes `1\' - `9\'. Corresponding patterns in different\n feature sets (files) correspond to the same original character.\n \n The source image dataset is lost. Using the pixel-dataset (mfeat-pix)\n sampled versions of the original images may be obtained (15 x 16 pixels).\n \n Total number of instances:\n --------------------------\n 2000 (200 instances per class)\n \n Total number of attributes:\n ---------------------------\n 649 (distributed over 6 datasets,see above)\n \n no missing attributes\n \n Total number of classes:\n ------------------------\n 10\n \n Format:\n ------\n 6 files, see above.\n Each file contains 2000 lines, one for each instance.\n Attributes are SPACE separated and can be loaded by Matlab as\n > load filename\n No missing attributes. Some are integer, others are real.\n \n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:20:17', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/14/dataset_14_mfeat-fourier.arff', 'true', 6, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:20:17'),
(7, 1, 0, 'breast-w', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:20:20', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52350/openml_phpJNxH0q', 'true', 7, 'Class', NULL, NULL, NULL, 'public', NULL, NULL, 'added special attributes', '2014-09-21 23:04:47'),
(8, 1, 0, 'mfeat-karhunen', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe multi-feature digit dataset\n -------------------------------\n \n Oowned and donated by:\n ----------------------\n \n Robert P.W. Duin\n Department of Applied Physics \n Delft University of Technology \n P.O. Box 5046, 2600 GA Delft\n The Netherlands\n \n email: duin@ph.tn.tudelft.nl\n http : //www.ph.tn.tudelft.nl/~duin\n tel +31 15 2786143\n \n Usage\n -----\n A slightly different version of the database is used in\n \n M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog, Handwritten\n      digit recognition by combined classifiers, Kybernetika, vol. 34, no. 4,\n      1998, 381-386.\n \n M. van Breukelen and R.P.W. Duin, Neural Network Initialization by Combined\n      Classifiers, in: A.K. Jain, S. Venkatesh, B.C. Lovell (eds.), ICPR\'98,\n      Proc. 14th Int. Conference on Pattern Recognition (Brisbane, Aug. 16-20),\n \n The database as it is is used in:\n \n A.K. Jain, R.P.W. Duin, J. Mao, Statisitcal Pattern Recognition: A Review,\n      in preparation\n \n Description\n -----------\n \n This dataset consists of features of handwritten numerals (`0\'--`9\')\n extracted from a collection of Dutch utility maps. 200 patterns per\n class (for a total of 2,000 patterns) have been digitized in  binary\n images. These digits are represented in terms of the following six\n feature sets (files): \n \n 1. mfeat-fou: 76 Fourier coefficients of the character shapes; \n 2. mfeat-fac: 216 profile correlations; \n 3. mfeat-kar: 64 Karhunen-Love coefficients; \n 4. mfeat-pix: 240 pixel averages in 2 x 3 windows; \n 5. mfeat-zer: 47 Zernike moments; \n 6. mfeat-mor: 6 morphological features. \n \n In each file the 2000 patterns are stored in ASCI on 2000 lines. The\n first 200 patterns are of class `0\', followed by sets of 200 patterns\n for each of the classes `1\' - `9\'. Corresponding patterns in different\n feature sets (files) correspond to the same original character.\n \n The source image dataset is lost. Using the pixel-dataset (mfeat-pix)\n sampled versions of the original images may be obtained (15 x 16 pixels).\n \n Total number of instances:\n --------------------------\n 2000 (200 instances per class)\n \n Total number of attributes:\n ---------------------------\n 649 (distributed over 6 datasets,see above)\n \n no missing attributes\n \n Total number of classes:\n ------------------------\n 10\n \n Format:\n ------\n 6 files, see above.\n Each file contains 2000 lines, one for each instance.\n Attributes are SPACE separated and can be loaded by Matlab as\n > load filename\n No missing attributes. Some are integer, others are real.\n \n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:20:30', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/16/dataset_16_mfeat-karhunen.arff', 'true', 8, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:20:30'),
(9, 1, 0, 'mfeat-morphological', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe multi-feature digit dataset\n -------------------------------\n \n Oowned and donated by:\n ----------------------\n \n Robert P.W. Duin\n Department of Applied Physics \n Delft University of Technology \n P.O. Box 5046, 2600 GA Delft\n The Netherlands\n \n email: duin@ph.tn.tudelft.nl\n http : //www.ph.tn.tudelft.nl/~duin\n tel +31 15 2786143\n \n Usage\n -----\n A slightly different version of the database is used in\n \n M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog, Handwritten\n      digit recognition by combined classifiers, Kybernetika, vol. 34, no. 4,\n      1998, 381-386.\n \n M. van Breukelen and R.P.W. Duin, Neural Network Initialization by Combined\n      Classifiers, in: A.K. Jain, S. Venkatesh, B.C. Lovell (eds.), ICPR\'98,\n      Proc. 14th Int. Conference on Pattern Recognition (Brisbane, Aug. 16-20),\n \n The database as it is is used in:\n \n A.K. Jain, R.P.W. Duin, J. Mao, Statisitcal Pattern Recognition: A Review,\n      in preparation\n \n Description\n -----------\n \n This dataset consists of features of handwritten numerals (0 - 9)\n extracted from a collection of Dutch utility maps. 200 patterns per\n class (for a total of 2,000 patterns) have been digitized in  binary\n images. These digits are represented in terms of the following six\n feature sets (files): \n \n 1. mfeat-fou: 76 Fourier coefficients of the character shapes; \n 2. mfeat-fac: 216 profile correlations; \n 3. mfeat-kar: 64 Karhunen-Love coefficients; \n 4. mfeat-pix: 240 pixel averages in 2 x 3 windows; \n 5. mfeat-zer: 47 Zernike moments; \n 6. mfeat-mor: 6 morphological features. \n \n In each file the 2000 patterns are stored in ASCI on 2000 lines. The\n first 200 patterns are of class \'0\', followed by sets of 200 patterns\n for each of the classes \'1\' - \'9\'. Corresponding patterns in different\n feature sets (files) correspond to the same original character.\n \n The source image dataset is lost. Using the pixel-dataset (mfeat-pix)\n sampled versions of the original images may be obtained (15 x 16 pixels).\n \n Total number of instances:\n --------------------------\n 2000 (200 instances per class)\n \n Total number of attributes:\n ---------------------------\n 649 (distributed over 6 datasets,see above)\n \n no missing attributes\n \n Total number of classes:\n ------------------------\n 10\n \n Format:\n ------\n 6 files, see above.\n Each file contains 2000 lines, one for each instance.\n Attributes are SPACE separated and can be loaded by Matlab as\n > load filename\n No missing attributes. Some are integer, others are real.\n \n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:20:37', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/18/dataset_18_mfeat-morphological.arff', 'true', 9, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:20:37'),
(10, 1, 0, 'mfeat-pixel', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe multi-feature digit dataset\n -------------------------------\n \n Oowned and donated by:\n ----------------------\n \n Robert P.W. Duin\n Department of Applied Physics \n Delft University of Technology \n P.O. Box 5046, 2600 GA Delft\n The Netherlands\n \n email: duin@ph.tn.tudelft.nl\n http : //www.ph.tn.tudelft.nl/~duin\n tel +31 15 2786143\n \n Usage\n -----\n A slightly different version of the database is used in\n \n M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog, Handwritten\n      digit recognition by combined classifiers, Kybernetika, vol. 34, no. 4,\n      1998, 381-386.\n \n M. van Breukelen and R.P.W. Duin, Neural Network Initialization by Combined\n      Classifiers, in: A.K. Jain, S. Venkatesh, B.C. Lovell (eds.), ICPR\'98,\n      Proc. 14th Int. Conference on Pattern Recognition (Brisbane, Aug. 16-20),\n \n The database as it is is used in:\n \n A.K. Jain, R.P.W. Duin, J. Mao, Statisitcal Pattern Recognition: A Review,\n      in preparation\n \n Description\n -----------\n \n This dataset consists of features of handwritten numerals (`0\'--`9\')\n extracted from a collection of Dutch utility maps. 200 patterns per\n class (for a total of 2,000 patterns) have been digitized in  binary\n images. These digits are represented in terms of the following six\n feature sets (files): \n \n 1. mfeat-fou: 76 Fourier coefficients of the character shapes; \n 2. mfeat-fac: 216 profile correlations; \n 3. mfeat-kar: 64 Karhunen-Love coefficients; \n 4. mfeat-pix: 240 pixel averages in 2 x 3 windows; \n 5. mfeat-zer: 47 Zernike moments; \n 6. mfeat-mor: 6 morphological features. \n \n In each file the 2000 patterns are stored in ASCI on 2000 lines. The\n first 200 patterns are of class `0\', followed by sets of 200 patterns\n for each of the classes `1\' - `9\'. Corresponding patterns in different\n feature sets (files) correspond to the same original character.\n \n The source image dataset is lost. Using the pixel-dataset (mfeat-pix)\n sampled versions of the original images may be obtained (15 x 16 pixels).\n \n Total number of instances:\n --------------------------\n 2000 (200 instances per class)\n \n Total number of attributes:\n ---------------------------\n 649 (distributed over 6 datasets,see above)\n \n no missing attributes\n \n Total number of classes:\n ------------------------\n 10\n \n Format:\n ------\n 6 files, see above.\n Each file contains 2000 lines, one for each instance.\n Attributes are SPACE separated and can be loaded by Matlab as\n > load filename\n No missing attributes. Some are integer, others are real.\n \n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:20:48', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/20/dataset_20_mfeat-pixel.arff', 'true', 10, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:20:48'),
(11, 1, 0, 'car', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Car Evaluation Database\n \n 2. Sources:\n    (a) Creator: Marko Bohanec\n    (b) Donors: Marko Bohanec   (marko.bohanec@ijs.si)\n                Blaz Zupan      (blaz.zupan@ijs.si)\n    (c) Date: June, 1997\n \n 3. Past Usage:\n \n    The hierarchical decision model, from which this dataset is\n    derived, was first presented in \n \n    M. Bohanec and V. Rajkovic: Knowledge acquisition and explanation for\n    multi-attribute decision making. In 8th Intl Workshop on Expert\n    Systems and their Applications, Avignon, France. pages 59-78, 1988.\n \n    Within machine-learning, this dataset was used for the evaluation\n    of HINT (Hierarchy INduction Tool), which was proved to be able to\n    completely reconstruct the original hierarchical model. This,\n    together with a comparison with C4.5, is presented in\n \n    B. Zupan, M. Bohanec, I. Bratko, J. Demsar: Machine learning by\n    function decomposition. ICML-97, Nashville, TN. 1997 (to appear)\n \n 4. Relevant Information Paragraph:\n \n    Car Evaluation Database was derived from a simple hierarchical\n    decision model originally developed for the demonstration of DEX\n    (M. Bohanec, V. Rajkovic: Expert system for decision\n    making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates\n    cars according to the following concept structure:\n \n    CAR                      car acceptability\n    . PRICE                  overall price\n    . . buying               buying price\n    . . maint                price of the maintenance\n    . TECH                   technical characteristics\n    . . COMFORT              comfort\n    . . . doors              number of doors\n    . . . persons            capacity in terms of persons to carry\n    . . . lug_boot           the size of luggage boot\n    . . safety               estimated safety of the car\n \n    Input attributes are printed in lowercase. Besides the target\n    concept (CAR), the model includes three intermediate concepts:\n    PRICE, TECH, COMFORT. Every concept is in the original model\n    related to its lower level descendants by a set of examples (for\n    these examples sets see http://www-ai.ijs.si/BlazZupan/car.html).\n \n    The Car Evaluation Database contains examples with the structural\n    information removed, i.e., directly relates CAR to the six input\n    attributes: buying, maint, doors, persons, lug_boot, safety.\n \n    Because of known underlying concept structure, this database may be\n    particularly useful for testing constructive induction and\n    structure discovery methods.\n \n 5. Number of Instances: 1728\n    (instances completely cover the attribute space)\n \n 6. Number of Attributes: 6\n \n 7. Attribute Values:\n \n    buying       v-high, high, med, low\n    maint        v-high, high, med, low\n    doors        2, 3, 4, 5-more\n    persons      2, 4, more\n    lug_boot     small, med, big\n    safety       low, med, high\n \n 8. Missing Attribute Values: none\n \n 9. Class Distribution (number of instances per class)\n \n    class      N          N[%]\n    -----------------------------\n    unacc     1210     (70.023 %) \n    acc        384     (22.222 %) \n    good        69     ( 3.993 %) \n    v-good      65     ( 3.762 %) \n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:20:52', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/21/dataset_21_car.arff', 'true', 11, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:20:52'),
(12, 1, 0, 'mfeat-zernike', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe multi-feature digit dataset\n -------------------------------\n \n Oowned and donated by:\n ----------------------\n \n Robert P.W. Duin\n Department of Applied Physics \n Delft University of Technology \n P.O. Box 5046, 2600 GA Delft\n The Netherlands\n \n email: duin@ph.tn.tudelft.nl\n http : //www.ph.tn.tudelft.nl/~duin\n tel +31 15 2786143\n \n Usage\n -----\n A slightly different version of the database is used in\n \n M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog, Handwritten\n      digit recognition by combined classifiers, Kybernetika, vol. 34, no. 4,\n      1998, 381-386.\n \n M. van Breukelen and R.P.W. Duin, Neural Network Initialization by Combined\n      Classifiers, in: A.K. Jain, S. Venkatesh, B.C. Lovell (eds.), ICPR\'98,\n      Proc. 14th Int. Conference on Pattern Recognition (Brisbane, Aug. 16-20),\n \n The database as it is is used in:\n \n A.K. Jain, R.P.W. Duin, J. Mao, Statisitcal Pattern Recognition: A Review,\n      in preparation\n \n Description\n -----------\n \n This dataset consists of features of handwritten numerals (`0\'--`9\')\n extracted from a collection of Dutch utility maps. 200 patterns per\n class (for a total of 2,000 patterns) have been digitized in  binary\n images. These digits are represented in terms of the following six\n feature sets (files): \n \n 1. mfeat-fou: 76 Fourier coefficients of the character shapes; \n 2. mfeat-fac: 216 profile correlations; \n 3. mfeat-kar: 64 Karhunen-Love coefficients; \n 4. mfeat-pix: 240 pixel averages in 2 x 3 windows; \n 5. mfeat-zer: 47 Zernike moments; \n 6. mfeat-mor: 6 morphological features. \n \n In each file the 2000 patterns are stored in ASCI on 2000 lines. The\n first 200 patterns are of class `0\', followed by sets of 200 patterns\n for each of the classes `1\' - `9\'. Corresponding patterns in different\n feature sets (files) correspond to the same original character.\n \n The source image dataset is lost. Using the pixel-dataset (mfeat-pix)\n sampled versions of the original images may be obtained (15 x 16 pixels).\n \n Total number of instances:\n --------------------------\n 2000 (200 instances per class)\n \n Total number of attributes:\n ---------------------------\n 649 (distributed over 6 datasets,see above)\n \n no missing attributes\n \n Total number of classes:\n ------------------------\n 10\n \n Format:\n ------\n 6 files, see above.\n Each file contains 2000 lines, one for each instance.\n Attributes are SPACE separated and can be loaded by Matlab as\n > load filename\n No missing attributes. Some are integer, others are real.\n \n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:21:00', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/22/dataset_22_mfeat-zernike.arff', 'true', 12, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:21:00'),
(13, 1, 0, 'cmc', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Contraceptive Method Choice\n \n 2. Sources:\n    (a) Origin:  This dataset is a subset of the 1987 National Indonesia\n                 Contraceptive Prevalence Survey\n    (b) Creator: Tjen-Sien Lim (limt@stat.wisc.edu)\n    (c) Donor:   Tjen-Sien Lim (limt@stat.wisc.edu)\n    (c) Date:    June 7, 1997\n \n 3. Past Usage:\n    Lim, T.-S., Loh, W.-Y. & Shih, Y.-S. (1999). A Comparison of\n    Prediction Accuracy, Complexity, and Training Time of Thirty-three\n    Old and New Classification Algorithms. Machine Learning. Forthcoming.\n    (ftp://ftp.stat.wisc.edu/pub/loh/treeprogs/quest1.7/mach1317.pdf or\n    (http://www.stat.wisc.edu/~limt/mach1317.pdf)\n \n 4. Relevant Information:\n    This dataset is a subset of the 1987 National Indonesia Contraceptive\n    Prevalence Survey. The samples are married women who were either not \n    pregnant or do not know if they were at the time of interview. The \n    problem is to predict the current contraceptive method choice \n    (no use, long-term methods, or short-term methods) of a woman based \n    on her demographic and socio-economic characteristics.\n \n 5. Number of Instances: 1473\n \n 6. Number of Attributes: 10 (including the class attribute)\n \n 7. Attribute Information:\n \n    1. Wife\'s age                     (numerical)\n    2. Wife\'s education               (categorical)      1=low, 2, 3, 4=high\n    3. Husband\'s education            (categorical)      1=low, 2, 3, 4=high\n    4. Number of children ever born   (numerical)\n    5. Wife\'s religion                (binary)           0=Non-Islam, 1=Islam\n    6. Wife\'s now working?            (binary)           0=Yes, 1=No\n    7. Husband\'s occupation           (categorical)      1, 2, 3, 4\n    8. Standard-of-living index       (categorical)      1=low, 2, 3, 4=high\n    9. Media exposure                 (binary)           0=Good, 1=Not good\n    10. Contraceptive method used     (class attribute)  1=No-use \n                                                         2=Long-term\n                                                         3=Short-term\n \n 8. Missing Attribute Values: None\n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:21:03', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/23/dataset_23_cmc.arff', 'true', 13, 'Contraceptive_method_used', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:21:03'),
(14, 1, 0, 'mushroom', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Mushroom Database\n \n 2. Sources: \n     (a) Mushroom records drawn from The Audubon Society Field Guide to North\n         American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred\n         A. Knopf\n     (b) Donor: Jeff Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n     (c) Date: 27 April 1987\n \n 3. Past Usage:\n     1. Schlimmer,J.S. (1987). Concept Acquisition Through Representational\n        Adjustment (Technical Report 87-19).  Doctoral disseration, Department\n        of Information and Computer Science, University of California, Irvine.\n        --- STAGGER: asymptoted to 95% classification accuracy after reviewing\n            1000 instances.\n     2. Iba,W., Wogulis,J., & Langley,P. (1988).  Trading off Simplicity\n        and Coverage in Incremental Concept Learning. In Proceedings of \n        the 5th International Conference on Machine Learning, 73-79.\n        Ann Arbor, Michigan: Morgan Kaufmann.  \n        -- approximately the same results with their HILLARY algorithm    \n     3. In the following references a set of rules (given below) were\n         learned for this data set which may serve as a point of\n         comparison for other researchers.\n \n         Duch W, Adamczak R, Grabczewski K (1996) Extraction of logical rules\n         from training data using backpropagation networks, in: Proc. of the\n         The 1st Online Workshop on Soft Computing, 19-30.Aug.1996, pp. 25-30,\n         available on-line at: http://www.bioele.nuee.nagoya-u.ac.jp/wsc1/\n \n         Duch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H, Extraction of\n         crisp logical rules using constrained backpropagation networks -\n         comparison of two new approaches, in: Proc. of the European Symposium\n         on Artificial Neural Networks (ESANN\'97), Bruge, Belgium 16-18.4.1997,\n         pp. xx-xx\n \n         Wlodzislaw Duch, Department of Computer Methods, Nicholas Copernicus\n         University, 87-100 Torun, Grudziadzka 5, Poland\n         e-mail: duch@phys.uni.torun.pl\n         WWW     http://www.phys.uni.torun.pl/kmk/\n         \n         Date: Mon, 17 Feb 1997 13:47:40 +0100\n         From: Wlodzislaw Duch <duch@phys.uni.torun.pl>\n         Organization: Dept. of Computer Methods, UMK\n \n         I have attached a file containing logical rules for mushrooms.\n         It should be helpful for other people since only in the last year I\n         have seen about 10 papers analyzing this dataset and obtaining quite\n         complex rules. We will try to contribute other results later.\n \n         With best regards, Wlodek Duch\n         ________________________________________________________________\n \n         Logical rules for the mushroom data sets.\n \n         Logical rules given below seem to be the simplest possible for the\n         mushroom dataset and therefore should be treated as benchmark results.\n \n         Disjunctive rules for poisonous mushrooms, from most general\n         to most specific:\n \n         P_1) odor=NOT(almond.OR.anise.OR.none)\n              120 poisonous cases missed, 98.52% accuracy\n \n         P_2) spore-print-color=green\n              48 cases missed, 99.41% accuracy\n          \n         P_3) odor=none.AND.stalk-surface-below-ring=scaly.AND.\n                   (stalk-color-above-ring=NOT.brown) \n              8 cases missed, 99.90% accuracy\n          \n         P_4) habitat=leaves.AND.cap-color=white\n                  100% accuracy     \n \n         Rule P_4) may also be\n \n         P_4\') population=clustered.AND.cap_color=white\n \n         These rule involve 6 attributes (out of 22). Rules for edible\n         mushrooms are obtained as negation of the rules given above, for\n         example the rule:\n \n         odor=(almond.OR.anise.OR.none).AND.spore-print-color=NOT.green\n \n         gives 48 errors, or 99.41% accuracy on the whole dataset.\n \n         Several slightly more complex variations on these rules exist,\n         involving other attributes, such as gill_size, gill_spacing,\n         stalk_surface_above_ring, but the rules given above are the simplest\n         we have found.\n \n \n 4. Relevant Information:\n     This data set includes descriptions of hypothetical samples\n     corresponding to 23 species of gilled mushrooms in the Agaricus and\n     Lepiota Family (pp. 500-525).  Each species is identified as\n     definitely edible, definitely poisonous, or of unknown edibility and\n     not recommended.  This latter class was combined with the poisonous\n     one.  The Guide clearly states that there is no simple rule for\n     determining the edibility of a mushroom; no rule like ``leaflets\n     three, let it be\'\' for Poisonous Oak and Ivy.\n \n 5. Number of Instances: 8124\n \n 6. Number of Attributes: 22 (all nominally valued)\n \n 7. Attribute Information: (classes: edible=e, poisonous=p)\n      1. cap-shape:                bell=b,conical=c,convex=x,flat=f,\n                                   knobbed=k,sunken=s\n      2. cap-surface:              fibrous=f,grooves=g,scaly=y,smooth=s\n      3. cap-color:                brown=n,buff=b,cinnamon=c,gray=g,green=r,\n                                   pink=p,purple=u,red=e,white=w,yellow=y\n      4. bruises?:                 bruises=t,no=f\n      5. odor:                     almond=a,anise=l,creosote=c,fishy=y,foul=f,\n                                   musty=m,none=n,pungent=p,spicy=s\n      6. gill-attachment:          attached=a,descending=d,free=f,notched=n\n      7. gill-spacing:             close=c,crowded=w,distant=d\n      8. gill-size:                broad=b,narrow=n\n      9. gill-color:               black=k,brown=n,buff=b,chocolate=h,gray=g,\n                                   green=r,orange=o,pink=p,purple=u,red=e,\n                                   white=w,yellow=y\n     10. stalk-shape:              enlarging=e,tapering=t\n     11. stalk-root:               bulbous=b,club=c,cup=u,equal=e,\n                                   rhizomorphs=z,rooted=r,missing=?\n     12. stalk-surface-above-ring: ibrous=f,scaly=y,silky=k,smooth=s\n     13. stalk-surface-below-ring: ibrous=f,scaly=y,silky=k,smooth=s\n     14. stalk-color-above-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,\n                                   pink=p,red=e,white=w,yellow=y\n     15. stalk-color-below-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,\n                                   pink=p,red=e,white=w,yellow=y\n     16. veil-type:                partial=p,universal=u\n     17. veil-color:               brown=n,orange=o,white=w,yellow=y\n     18. ring-number:              none=n,one=o,two=t\n     19. ring-type:                cobwebby=c,evanescent=e,flaring=f,large=l,\n                                   none=n,pendant=p,sheathing=s,zone=z\n     20. spore-print-color:        black=k,brown=n,buff=b,chocolate=h,green=r,\n                                   orange=o,purple=u,white=w,yellow=y\n     21. population:               abundant=a,clustered=c,numerous=n,\n                                   scattered=s,several=v,solitary=y\n     22. habitat:                  grasses=g,leaves=l,meadows=m,paths=p,\n                                   urban=u,waste=w,woods=d\n \n 8. Missing Attribute Values: 2480 of them (denoted by \"?\"), all for\n    attribute #11.\n \n 9. Class Distribution: \n     --    edible: 4208 (51.8%)\n     -- poisonous: 3916 (48.2%)\n     --     total: 8124 instances', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:21:11', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/24/dataset_24_mushroom.arff', 'true', 14, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:21:11'),
(15, 1, 0, 'optdigits', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title of Database: Optical Recognition of Handwritten Digits\n \n 2. Source:\n 	E. Alpaydin, C. Kaynak\n 	Department of Computer Engineering\n 	Bogazici University, 80815 Istanbul Turkey\n 	alpaydin@boun.edu.tr\n 	July 1998\n \n 3. Past Usage:\n 	C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n 	Applications to Handwritten Digit Recognition, \n 	MSc Thesis, Institute of Graduate Studies in Science and \n 	Engineering, Bogazici University.\n \n 	E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika,\n 	to appear. ftp://ftp.icsi.berkeley.edu/pub/ai/ethem/kyb.ps.Z\n \n 4. Relevant Information:\n 	We used preprocessing programs made available by NIST to extract\n 	normalized bitmaps of handwritten digits from a preprinted form. From\n 	a total of 43 people, 30 contributed to the training set and different\n 	13 to the test set. 32x32 bitmaps are divided into nonoverlapping \n 	blocks of 4x4 and the number of on pixels are counted in each block.\n 	This generates an input matrix of 8x8 where each element is an \n 	integer in the range 0..16. This reduces dimensionality and gives \n 	invariance to small distortions.\n \n 	For info on NIST preprocessing routines, see \n 	M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, \n 	P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based \n 	Handprint Recognition System, NISTIR 5469, 1994.\n \n 5. Number of Instances\n 	optdigits.tra	Training	3823\n 	optdigits.tes	Testing		1797\n 	\n 	The way we used the dataset was to use half of training for \n 	actual training, one-fourth for validation and one-fourth\n 	for writer-dependent testing. The test set was used for \n 	writer-independent testing and is the actual quality measure.\n \n 6. Number of Attributes\n 	64 input+1 class attribute\n \n 7. For Each Attribute:\n 	All input attributes are integers in the range 0..16.\n 	The last attribute is the class code 0..9\n \n 8. Missing Attribute Values\n 	None\n \n 9. Class Distribution\n 	Class:	No of examples in training set\n 	0:  376\n 	1:  389\n 	2:  380\n 	3:  389\n 	4:  387\n 	5:  376\n 	6:  377\n 	7:  387\n 	8:  380\n 	9:  382\n \n 	Class: No of examples in testing set\n 	0:  178\n 	1:  182\n 	2:  177\n 	3:  183\n 	4:  181\n 	5:  182\n 	6:  181\n 	7:  179\n 	8:  174\n 	9:  180\n \n Accuracy on the testing set with k-nn \n using Euclidean distance as the metric\n \n  k =  1   : 98.00\n  k =  2   : 97.38\n  k =  3   : 97.83\n  k =  4   : 97.61\n  k =  5   : 97.89\n  k =  6   : 97.77\n  k =  7   : 97.66\n  k =  8   : 97.66\n  k =  9   : 97.72\n  k = 10   : 97.55\n  k = 11   : 97.89', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:21:34', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/28/dataset_28_optdigits.arff', 'true', 15, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:21:34'),
(16, 1, 0, 'credit-a', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Credit Approval\n \n 2. Sources: \n     (confidential)\n     Submitted by quinlan@cs.su.oz.au\n \n 3.  Past Usage:\n \n     See Quinlan,\n     * \"Simplifying decision trees\", Int J Man-Machine Studies 27,\n       Dec 1987, pp. 221-234.\n     * \"C4.5: Programs for Machine Learning\", Morgan Kaufmann, Oct 1992\n   \n 4.  Relevant Information:\n \n     This file concerns credit card applications.  All attribute names\n     and values have been changed to meaningless symbols to protect\n     confidentiality of the data.\n   \n     This dataset is interesting because there is a good mix of\n     attributes -- continuous, nominal with small numbers of\n     values, and nominal with larger numbers of values.  There\n     are also a few missing values.\n   \n 5.  Number of Instances: 690\n \n 6.  Number of Attributes: 15 + class attribute\n \n 7.  Attribute Information:\n \n     A1:	b, a.\n     A2:	continuous.\n     A3:	continuous.\n     A4:	u, y, l, t.\n     A5:	g, p, gg.\n     A6:	c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\n     A7:	v, h, bb, j, n, z, dd, ff, o.\n     A8:	continuous.\n     A9:	t, f.\n     A10:	t, f.\n     A11:	continuous.\n     A12:	t, f.\n     A13:	g, p, s.\n     A14:	continuous.\n     A15:	continuous.\n     A16: +,-         (class attribute)\n \n 8.  Missing Attribute Values:\n     37 cases (5%) have one or more missing values.  The missing\n     values from particular attributes are:\n \n     A1:  12\n     A2:  12\n     A4:   6\n     A5:   6\n     A6:   9\n     A7:   9\n     A14: 13\n \n 9.  Class Distribution\n   \n     +: 307 (44.5%)\n     -: 383 (55.5%)', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:21:38', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/29/dataset_29_credit-a.arff', 'true', 16, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:21:38'),
(17, 1, 0, 'credit-g', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nDescription of the German credit dataset.\n \n 1. Title: German Credit data\n \n 2. Source Information\n \n Professor Dr. Hans Hofmann  \n Institut f\"ur Statistik und \"Okonometrie  \n Universit\"at Hamburg  \n FB Wirtschaftswissenschaften  \n Von-Melle-Park 5    \n 2000 Hamburg 13 \n \n 3. Number of Instances:  1000\n \n Two datasets are provided.  the original dataset, in the form provided\n by Prof. Hofmann, contains categorical/symbolic attributes and\n is in the file \"german.data\".   \n  \n For algorithms that need numerical attributes, Strathclyde University \n produced the file \"german.data-numeric\".  This file has been edited \n and several indicator variables added to make it suitable for \n algorithms which cannot cope with categorical variables.   Several\n attributes that are ordered categorical (such as attribute 17) have\n been coded as integer.    This was the form used by StatLog.\n \n \n 6. Number of Attributes german: 20 (7 numerical, 13 categorical)\n    Number of Attributes german.numer: 24 (24 numerical)\n \n \n 7.  Attribute description for german\n \n Attribute 1:  (qualitative)\n 	       Status of existing checking account\n                A11 :      ... <    0 DM\n 	       A12 : 0 <= ... <  200 DM\n 	       A13 :      ... >= 200 DM /\n 		     salary assignments for at least 1 year\n                A14 : no checking account\n \n Attribute 2:  (numerical)\n 	      Duration in month\n \n Attribute 3:  (qualitative)\n 	      Credit history\n 	      A30 : no credits taken/\n 		    all credits paid back duly\n               A31 : all credits at this bank paid back duly\n 	      A32 : existing credits paid back duly till now\n               A33 : delay in paying off in the past\n 	      A34 : critical account/\n 		    other credits existing (not at this bank)\n \n Attribute 4:  (qualitative)\n 	      Purpose\n 	      A40 : car (new)\n 	      A41 : car (used)\n 	      A42 : furniture/equipment\n 	      A43 : radio/television\n 	      A44 : domestic appliances\n 	      A45 : repairs\n 	      A46 : education\n 	      A47 : (vacation - does not exist?)\n 	      A48 : retraining\n 	      A49 : business\n 	      A410 : others\n \n Attribute 5:  (numerical)\n 	      Credit amount\n \n Attibute 6:  (qualitative)\n 	      Savings account/bonds\n 	      A61 :          ... <  100 DM\n 	      A62 :   100 <= ... <  500 DM\n 	      A63 :   500 <= ... < 1000 DM\n 	      A64 :          .. >= 1000 DM\n               A65 :   unknown/ no savings account\n \n Attribute 7:  (qualitative)\n 	      Present employment since\n 	      A71 : unemployed\n 	      A72 :       ... < 1 year\n 	      A73 : 1  <= ... < 4 years  \n 	      A74 : 4  <= ... < 7 years\n 	      A75 :       .. >= 7 years\n \n Attribute 8:  (numerical)\n 	      Installment rate in percentage of disposable income\n \n Attribute 9:  (qualitative)\n 	      Personal status and sex\n 	      A91 : male   : divorced/separated\n 	      A92 : female : divorced/separated/married\n               A93 : male   : single\n 	      A94 : male   : married/widowed\n 	      A95 : female : single\n \n Attribute 10: (qualitative)\n 	      Other debtors / guarantors\n 	      A101 : none\n 	      A102 : co-applicant\n 	      A103 : guarantor\n \n Attribute 11: (numerical)\n 	      Present residence since\n \n Attribute 12: (qualitative)\n 	      Property\n 	      A121 : real estate\n 	      A122 : if not A121 : building society savings agreement/\n 				   life insurance\n               A123 : if not A121/A122 : car or other, not in attribute 6\n 	      A124 : unknown / no property\n \n Attribute 13: (numerical)\n 	      Age in years\n \n Attribute 14: (qualitative)\n 	      Other installment plans \n 	      A141 : bank\n 	      A142 : stores\n 	      A143 : none\n \n Attribute 15: (qualitative)\n 	      Housing\n 	      A151 : rent\n 	      A152 : own\n 	      A153 : for free\n \n Attribute 16: (numerical)\n               Number of existing credits at this bank\n \n Attribute 17: (qualitative)\n 	      Job\n 	      A171 : unemployed/ unskilled  - non-resident\n 	      A172 : unskilled - resident\n 	      A173 : skilled employee / official\n 	      A174 : management/ self-employed/\n 		     highly qualified employee/ officer\n \n Attribute 18: (numerical)\n 	      Number of people being liable to provide maintenance for\n \n Attribute 19: (qualitative)\n 	      Telephone\n 	      A191 : none\n 	      A192 : yes, registered under the customers name\n \n Attribute 20: (qualitative)\n 	      foreign worker\n 	      A201 : yes\n 	      A202 : no\n \n \n \n 8.  Cost Matrix\n \n This dataset requires use of a cost matrix (see below)\n \n \n       1        2\n ----------------------------\n   1   0        1\n -----------------------\n   2   5        0\n \n (1 = Good,  2 = Bad)\n \n the rows represent the actual classification and the columns\n the predicted classification.\n \n It is worse to class a customer as good when they are bad (5), \n than it is to class a customer as bad when they are good (1).\n \n\n\n\n\n Relabeled values in attribute checking_status\n    From: A11                     To: \'<0\'                \n    From: A12                     To: \'0<=X<200\'          \n    From: A13                     To: \'>=200\'             \n    From: A14                     To: \'no checking\'       \n\n\n Relabeled values in attribute credit_history\n    From: A30                     To: \'no credits/all paid\'\n    From: A31                     To: \'all paid\'          \n    From: A32                     To: \'existing paid\'     \n    From: A33                     To: \'delayed previously\'\n    From: A34                     To: \'critical/other existing credit\'\n\n\n Relabeled values in attribute purpose\n    From: A40                     To: \'new car\'           \n    From: A41                     To: \'used car\'          \n    From: A42                     To: furniture/equipment \n    From: A43                     To: radio/tv            \n    From: A44                     To: \'domestic appliance\'\n    From: A45                     To: repairs             \n    From: A46                     To: education           \n    From: A47                     To: vacation            \n    From: A48                     To: retraining          \n    From: A49                     To: business            \n    From: A410                    To: other               \n\n\n Relabeled values in attribute savings_status\n    From: A61                     To: \'<100\'              \n    From: A62                     To: \'100<=X<500\'        \n    From: A63                     To: \'500<=X<1000\'       \n    From: A64                     To: \'>=1000\'            \n    From: A65                     To: \'no known savings\'  \n\n\n Relabeled values in attribute employment\n    From: A71                     To: unemployed          \n    From: A72                     To: \'<1\'                \n    From: A73                     To: \'1<=X<4\'            \n    From: A74                     To: \'4<=X<7\'            \n    From: A75                     To: \'>=7\'               \n\n\n Relabeled values in attribute personal_status\n    From: A91                     To: \'male div/sep\'      \n    From: A92                     To: \'female div/dep/mar\'\n    From: A93                     To: \'male single\'       \n    From: A94                     To: \'male mar/wid\'      \n    From: A95                     To: \'female single\'     \n\n\n Relabeled values in attribute other_parties\n    From: A101                    To: none                \n    From: A102                    To: \'co applicant\'      \n    From: A103                    To: guarantor           \n\n\n Relabeled values in attribute property_magnitude\n    From: A121                    To: \'real estate\'       \n    From: A122                    To: \'life insurance\'    \n    From: A123                    To: car                 \n    From: A124                    To: \'no known property\' \n\n\n Relabeled values in attribute other_payment_plans\n    From: A141                    To: bank                \n    From: A142                    To: stores              \n    From: A143                    To: none                \n\n\n Relabeled values in attribute housing\n    From: A151                    To: rent                \n    From: A152                    To: own                 \n    From: A153                    To: \'for free\'          \n\n\n Relabeled values in attribute job\n    From: A171                    To: \'unemp/unskilled non res\'\n    From: A172                    To: \'unskilled resident\'\n    From: A173                    To: skilled             \n    From: A174                    To: \'high qualif/self emp/mgmt\'\n\n\n Relabeled values in attribute own_telephone\n    From: A191                    To: none                \n    From: A192                    To: yes                 \n\n\n Relabeled values in attribute foreign_worker\n    From: A201                    To: yes                 \n    From: A202                    To: no                  \n\n\n Relabeled values in attribute class\n    From: 1                       To: good                \n    From: 2                       To: bad', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:21:47', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/31/dataset_31_credit-g.arff', 'true', 17, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:21:47'),
(18, 1, 0, 'pendigits', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title of Database: Pen-Based Recognition of Handwritten Digits\n \n 2. Source:\n 	E. Alpaydin, F. Alimoglu\n 	Department of Computer Engineering\n 	Bogazici University, 80815 Istanbul Turkey\n 	alpaydin@boun.edu.tr\n 	July 1998\n \n 3. Past Usage:\n 	F. Alimoglu (1996) Combining Multiple Classifiers for Pen-Based\n 	Handwritten Digit Recognition, \n 	MSc Thesis, Institute of Graduate Studies in Science and \n 	Engineering, Bogazici University.\n 	http://www.cmpe.boun.edu.tr/~alimoglu/alimoglu.ps.gz\n \n 	F. Alimoglu, E. Alpaydin, \"Methods of Combining Multiple Classifiers \n 	Based on Different Representations for Pen-based Handwriting\n 	Recognition,\" Proceedings of the Fifth Turkish Artificial \n 	Intelligence and Artificial Neural Networks Symposium (TAINN 96), \n 	June 1996, Istanbul, Turkey.\n 	http://www.cmpe.boun.edu.tr/~alimoglu/tainn96.ps.gz\n \n 	\n 4. Relevant Information:\n \n 	We create a digit database by collecting 250 samples from 44 writers.\n 	The samples written by 30 writers are used for training,\n 	cross-validation and writer dependent testing, and the digits \n 	written by the other 14 are used for writer independent testing. This\n 	database is also available in the UNIPEN format.\n \n 	We use a WACOM PL-100V pressure sensitive tablet with an integrated \n 	LCD display and a cordless stylus. The input and display areas are\n 	located in the same place. Attached to the serial port of an Intel \n 	486 based PC, it allows us to collect handwriting samples. The tablet\n 	sends $x$ and $y$ tablet coordinates and pressure level values of the\n 	pen at fixed time intervals (sampling rate) of 100 miliseconds. \n \n 	These writers are asked to write 250 digits in random order inside \n 	boxes of 500 by 500 tablet pixel resolution.  Subject are monitored \n 	only during the first entry screens. Each screen contains five boxes\n 	with the digits to be written displayed above. Subjects are told to\n 	write only inside these boxes.  If they make a mistake or are unhappy\n 	with their writing, they are instructed to clear the content of a box \n 	by using an on-screen button. The first ten digits are ignored \n 	because most writers are not familiar with this type of input devices,\n 	but subjects are not aware of this. \n \n 	In our study, we use only ($x, y$) coordinate information. The stylus\n 	pressure level values are ignored. First we apply normalization to \n 	make our representation invariant to translations and scale \n 	distortions. The raw data that we capture from the tablet consist of\n 	integer values between 0 and 500 (tablet input box resolution). The \n 	new coordinates are such that the coordinate which has the maximum \n 	range varies between 0 and 100. Usually $x$ stays in this range, since\n 	most characters are taller than they are wide.  \n \n 	In order to train and test our classifiers, we need to represent \n 	digits as constant length feature vectors. A commonly used technique\n 	leading to good results is resampling the ( x_t, y_t) points. \n 	Temporal resampling (points regularly spaced in time) or spatial\n 	resampling (points regularly spaced in arc length) can be used here. \n 	Raw point data are already regularly spaced in time but the distance\n 	between them is variable. Previous research showed that spatial\n 	resampling to obtain a constant number of regularly spaced points \n 	on the trajectory yields much better performance, because it provides \n 	a better alignment between points. Our resampling algorithm uses \n 	simple linear interpolation between pairs of points. The resampled\n 	digits are represented as a sequence of T points ( x_t, y_t )_{t=1}^T,\n 	regularly spaced in arc length, as opposed to the input sequence, \n 	which is regularly spaced in time.\n \n 	So, the input vector size is 2*T, two times the number of points\n 	resampled. We considered spatial resampling to T=8,12,16 points in our\n 	experiments and found that T=8 gave the best trade-off between \n 	accuracy and complexity.\n \n \n 5. Number of Instances\n 	pendigits.tra	Training	7494\n 	pendigits.tes	Testing		3498\n 	\n 	The way we used the dataset was to use first half of training for \n 	actual training, one-fourth for validation and one-fourth\n 	for writer-dependent testing. The test set was used for \n 	writer-independent testing and is the actual quality measure.\n \n 6. Number of Attributes\n 	16 input+1 class attribute\n \n 7. For Each Attribute:\n 	All input attributes are integers in the range 0..100.\n 	The last attribute is the class code 0..9\n \n 8. Missing Attribute Values\n 	None\n \n 9. Class Distribution\n 	Class: No of examples in training set\n 	0:  780\n 	1:  779\n 	2:  780\n 	3:  719\n 	4:  780\n 	5:  720\n 	6:  720\n 	7:  778\n 	8:  719\n 	9:  719\n 	Class: No of examples in testing set\n 	0:  363\n 	1:  364\n 	2:  364\n 	3:  336\n 	4:  364\n 	5:  335\n 	6:  336\n 	7:  364\n 	8:  336\n 	9:  336\n \n Accuracy on the testing set with k-nn \n using Euclidean distance as the metric\n \n  k =  1 : 97.74\n  k =  2 : 97.37\n  k =  3 : 97.80\n  k =  4 : 97.66\n  k =  5 : 97.60\n  k =  6 : 97.57\n  k =  7 : 97.54\n  k =  8 : 97.54\n  k =  9 : 97.46\n  k = 10 : 97.48\n  k = 11 : 97.34', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:21:54', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/32/dataset_32_pendigits.arff', 'true', 18, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:21:54'),
(19, 1, 0, 'segment', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Image Segmentation data\n \n 2. Source Information\n    -- Creators: Vision Group, University of Massachusetts\n    -- Donor: Vision Group (Carla Brodley, brodley@cs.umass.edu)\n    -- Date: November, 1990\n  \n 3. Past Usage: None yet published\n \n 4. Relevant Information:\n \n    The instances were drawn randomly from a database of 7 outdoor \n    images.  The images were handsegmented to create a classification\n    for every pixel.  \n \n    Each instance is a 3x3 region.\n \n 5. Number of Instances: Training data: 210  Test data: 2100\n \n 6. Number of Attributes: 19 continuous attributes\n \n 7. Attribute Information:\n \n     1.  region-centroid-col:  the column of the center pixel of the region.\n     2.  region-centroid-row:  the row of the center pixel of the region.\n     3.  region-pixel-count:  the number of pixels in a region = 9.\n     4.  short-line-density-5:  the results of a line extractoin algorithm that \n          counts how many lines of length 5 (any orientation) with\n          low contrast, less than or equal to 5, go through the region.\n     5.  short-line-density-2:  same as short-line-density-5 but counts lines\n          of high contrast, greater than 5.\n     6.  vedge-mean:  measure the contrast of horizontally\n          adjacent pixels in the region.  There are 6, the mean and \n          standard deviation are given.  This attribute is used as\n         a vertical edge detector.\n     7.  vegde-sd:  (see 6)\n     8.  hedge-mean:  measures the contrast of vertically adjacent\n           pixels. Used for horizontal line detection. \n     9.  hedge-sd: (see 8).\n     10. intensity-mean:  the average over the region of (R + G + B)/3\n     11. rawred-mean: the average over the region of the R value.\n     12. rawblue-mean: the average over the region of the B value.\n     13. rawgreen-mean: the average over the region of the G value.\n     14. exred-mean: measure the excess red:  (2R - (G + B))\n     15. exblue-mean: measure the excess blue:  (2B - (G + R))\n     16. exgreen-mean: measure the excess green:  (2G - (R + B))\n     17. value-mean:  3-d nonlinear transformation\n          of RGB. (Algorithm can be found in Foley and VanDam, Fundamentals\n          of Interactive Computer Graphics)\n     18. saturatoin-mean:  (see 17)\n     19. hue-mean:  (see 17)\n \n 8. Missing Attribute Values: None\n \n 9. Class Distribution: \n \n    Classes:  brickface, sky, foliage, cement, window, path, grass.\n \n    30 instances per class for training data.\n    300 instances per class for test data.\n \n\n\n\n\n Relabeled values in attribute class\n    From: 1                       To: brickface           \n    From: 2                       To: sky                 \n    From: 3                       To: foliage             \n    From: 4                       To: cement              \n    From: 5                       To: window              \n    From: 6                       To: path                \n    From: 7                       To: grass', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:22:10', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/36/dataset_36_segment.arff', 'true', 19, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:22:10'),
(20, 1, 0, 'diabetes', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Pima Indians Diabetes Database\n \n 2. Sources:\n    (a) Original owners: National Institute of Diabetes and Digestive and\n                         Kidney Diseases\n    (b) Donor of database: Vincent Sigillito (vgs@aplcen.apl.jhu.edu)\n                           Research Center, RMI Group Leader\n                           Applied Physics Laboratory\n                           The Johns Hopkins University\n                           Johns Hopkins Road\n                           Laurel, MD 20707\n                           (301) 953-6231\n    (c) Date received: 9 May 1990\n \n 3. Past Usage:\n     1. Smith,~J.~W., Everhart,~J.~E., Dickson,~W.~C., Knowler,~W.~C., &\n        Johannes,~R.~S. (1988). Using the ADAP learning algorithm to forecast\n        the onset of diabetes mellitus.  In {it Proceedings of the Symposium\n        on Computer Applications and Medical Care} (pp. 261--265).  IEEE\n        Computer Society Press.\n \n        The diagnostic, binary-valued variable investigated is whether the\n        patient shows signs of diabetes according to World Health Organization\n        criteria (i.e., if the 2 hour post-load plasma glucose was at least \n        200 mg/dl at any survey  examination or if found during routine medical\n        care).   The population lives near Phoenix, Arizona, USA.\n \n        Results: Their ADAP algorithm makes a real-valued prediction between\n        0 and 1.  This was transformed into a binary decision using a cutoff of \n        0.448.  Using 576 training instances, the sensitivity and specificity\n        of their algorithm was 76% on the remaining 192 instances.\n \n 4. Relevant Information:\n       Several constraints were placed on the selection of these instances from\n       a larger database.  In particular, all patients here are females at\n       least 21 years old of Pima Indian heritage.  ADAP is an adaptive learning\n       routine that generates and executes digital analogs of perceptron-like\n       devices.  It is a unique algorithm; see the paper for details.\n \n 5. Number of Instances: 768\n \n 6. Number of Attributes: 8 plus class \n \n 7. For Each Attribute: (all numeric-valued)\n    1. Number of times pregnant\n    2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n    3. Diastolic blood pressure (mm Hg)\n    4. Triceps skin fold thickness (mm)\n    5. 2-Hour serum insulin (mu U/ml)\n    6. Body mass index (weight in kg/(height in m)^2)\n    7. Diabetes pedigree function\n    8. Age (years)\n    9. Class variable (0 or 1)\n \n 8. Missing Attribute Values: None\n \n 9. Class Distribution: (class value 1 is interpreted as \"tested positive for\n    diabetes\")\n \n    Class Value  Number of instances\n    0            500\n    1            268\n \n 10. Brief statistical analysis:\n \n     Attribute number:    Mean:   Standard Deviation:\n     1.                     3.8     3.4\n     2.                   120.9    32.0\n     3.                    69.1    19.4\n     4.                    20.5    16.0\n     5.                    79.8   115.2\n     6.                    32.0     7.9\n     7.                     0.5     0.3\n     8.                    33.2    11.8\n \n \n\n\n\n\n Relabeled values in attribute \'class\'\n    From: 0                       To: tested_negative     \n    From: 1                       To: tested_positive', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:22:13', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/37/dataset_37_diabetes.arff', 'true', 20, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:22:13'),
(21, 1, 0, 'sick', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n;\n ; Thyroid disease records supplied by the Garavan Institute and J. Ross\n ; Quinlan, New South Wales Institute, Syndney, Australia.\n ;\n ; 1987.\n ;\n \n sick, negative.			|  classes\n \n age:				continuous.\n sex:				M, F.\n on thyroxine:			f, t.\n query on thyroxine:		f, t.\n on antithyroid medication:	f, t.\n sick:				f, t.\n pregnant:			f, t.\n thyroid surgery:		f, t.\n I131 treatment:			f, t.\n query hypothyroid:		f, t.\n query hyperthyroid:		f, t.\n lithium:			f, t.\n goitre:				f, t.\n tumor:				f, t.\n hypopituitary:			f, t.\n psych:				f, t.\n TSH measured:			f, t.\n TSH:				continuous.\n T3 measured:			f, t.\n T3:				continuous.\n TT4 measured:			f, t.\n TT4:				continuous.\n T4U measured:			f, t.\n T4U:				continuous.\n FTI measured:			f, t.\n FTI:				continuous.\n TBG measured:			f, t.\n TBG:				continuous.\n referral source:		WEST, STMW, SVHC, SVI, SVHD, other.\n \n\n Num Instances:     3772\n Num Attributes:    30\n Num Continuous:    7 (Int 1 / Real 6)\n Num Discrete:      23\n Missing values:    6064 /  5.4%\n\n     name                      type enum ints real     missing    distinct  (1)\n   1 \'age\'                     Int    0% 100%   0%     1 /  0%    93 /  2%   0% \n   2 \'sex\'                     Enum  96%   0%   0%   150 /  4%     2 /  0%   0% \n   3 \'on thyroxine\'            Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   4 \'query on thyroxine\'      Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   5 \'on antithyroid medicati  Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   6 \'sick\'                    Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   7 \'pregnant\'                Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   8 \'thyroid surgery\'         Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n   9 \'I131 treatment\'          Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  10 \'query hypothyroid\'       Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  11 \'query hyperthyroid\'      Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  12 \'lithium\'                 Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  13 \'goitre\'                  Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  14 \'tumor\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  15 \'hypopituitary\'           Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  16 \'psych\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  17 \'TSH measured\'            Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  18 \'TSH\'                     Real   0%  11%  79%   369 / 10%   287 /  8%   2% \n  19 \'T3 measured\'             Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  20 \'T3\'                      Real   0%   9%  71%   769 / 20%    69 /  2%   0% \n  21 \'TT4 measured\'            Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  22 \'TT4\'                     Real   0%  94%   0%   231 /  6%   241 /  6%   1% \n  23 \'T4U measured\'            Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  24 \'T4U\'                     Real   0%   2%  87%   387 / 10%   146 /  4%   1% \n  25 \'FTI measured\'            Enum 100%   0%   0%     0 /  0%     2 /  0%   0% \n  26 \'FTI\'                     Real   0%  90%   0%   385 / 10%   234 /  6%   2% \n  27 \'TBG measured\'            Enum 100%   0%   0%     0 /  0%     1 /  0%   0% \n  28 \'TBG\'                     Real   0%   0%   0%  3772 /100%     0 /  0%   0% \n  29 \'referral source\'         Enum 100%   0%   0%     0 /  0%     5 /  0%   0% \n  30 \'Class\'                   Enum 100%   0%   0%     0 /  0%     2 /  0%   0%', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:22:19', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/38/dataset_38_sick.arff', 'true', 21, 'Class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:22:19'),
(22, 1, 0, 'soybean', '1', '1', '**Author**: R.S. Michalski and R.L. Chilausky (Donors: Ming Tan & Jeff Schlimmer)  \r\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Soybean+(Large)) - 1988  \r\n**Please cite**: R.S. Michalski and R.L. Chilausky \"Learning by Being Told and Learning from Examples: An Experimental Comparison of the Two Methods of Knowledge Acquisition in the Context of Developing an Expert System for Soybean Disease Diagnosis\", International Journal of Policy Analysis and Information Systems, Vol. 4, No. 2, 1980.  \r\n\r\n**Large Soybean Database**  \r\nThis is the large soybean database from the UCI repository, with its training and test database combined into a single file. There are 19 classes, only the first 15 of which have been used in prior work.  The folklore seems to be that the last four classes are unjustified by the data since they have so few examples. There are 35 categorical attributes, some nominal and some ordered.  The value \'dna\' means does not apply.  The values for attributes are encoded numerically, with the first value encoded as \'0\', the second as \'1\', and so forth.  An unknown values is encoded as \'?\'.', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:22:32', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/42/dataset_42_soybean.arff', 'true', 22, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:22:32'),
(23, 1, 0, 'spambase', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title:  SPAM E-mail Database\n \n 2. Sources:\n    (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n         Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n    (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n    (c) Generated: June-July 1999\n \n 3. Past Usage:\n    (a) Hewlett-Packard Internal-only Technical Report. External forthcoming.\n    (b) Determine whether a given email is spam or not.\n    (c) ~7% misclassification error.\n        False positives (marking good mail as spam) are very undesirable.\n        If we insist on zero false positives in the training/testing set,\n        20-25% of the spam passed through the filter.\n \n 4. Relevant Information:\n         The \"spam\" concept is diverse: advertisements for products/web\n         sites, make money fast schemes, chain letters, pornography...\n 	Our collection of spam e-mails came from our postmaster and \n 	individuals who had filed spam.  Our collection of non-spam \n 	e-mails came from filed work and personal e-mails, and hence\n 	the word \'george\' and the area code \'650\' are indicators of \n 	non-spam.  These are useful when constructing a personalized \n 	spam filter.  One would either have to blind such non-spam \n 	indicators or get a very wide collection of non-spam to \n 	generate a general purpose spam filter.\n \n         For background on spam:\n         Cranor, Lorrie F., LaMacchia, Brian A.  Spam! \n         Communications of the ACM, 41(8):74-83, 1998.\n \n 5. Number of Instances: 4601 (1813 Spam = 39.4%)\n \n 6. Number of Attributes: 58 (57 continuous, 1 nominal class label)\n \n 7. Attribute Information:\n The last column of \'spambase.data\' denotes whether the e-mail was \n considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n Most of the attributes indicate whether a particular word or\n character was frequently occuring in the e-mail.  The run-length\n attributes (55-57) measure the length of sequences of consecutive \n capital letters.  For the statistical measures of each attribute, \n see the end of this file.  Here are the definitions of the attributes:\n \n 48 continuous real [0,100] attributes of type word_freq_WORD \n = percentage of words in the e-mail that match WORD,\n i.e. 100 * (number of times the WORD appears in the e-mail) / \n total number of words in e-mail.  A \"word\" in this case is any \n string of alphanumeric characters bounded by non-alphanumeric \n characters or end-of-string.\n \n 6 continuous real [0,100] attributes of type char_freq_CHAR\n = percentage of characters in the e-mail that match CHAR,\n i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n \n 1 continuous real [1,...] attribute of type capital_run_length_average\n = average length of uninterrupted sequences of capital letters\n \n 1 continuous integer [1,...] attribute of type capital_run_length_longest\n = length of longest uninterrupted sequence of capital letters\n \n 1 continuous integer [1,...] attribute of type capital_run_length_total\n = sum of length of uninterrupted sequences of capital letters\n = total number of capital letters in the e-mail\n \n 1 nominal {0,1} class attribute of type spam\n = denotes whether the e-mail was considered spam (1) or not (0), \n i.e. unsolicited commercial e-mail.  \n \n \n 8. Missing Attribute Values: None\n \n 9. Class Distribution:\n 	Spam	  1813  (39.4%)\n 	Non-Spam  2788  (60.6%)\n \n \n Attribute Statistics:\n    Min: Max:   Average:  Std.Dev: Coeff.Var_%: \n 1  0    4.54   0.10455   0.30536  292          \n 2  0    14.28  0.21301   1.2906   606          \n 3  0    5.1    0.28066   0.50414  180          \n 4  0    42.81  0.065425  1.3952   2130         \n 5  0    10     0.31222   0.67251  215          \n 6  0    5.88   0.095901  0.27382  286          \n 7  0    7.27   0.11421   0.39144  343          \n 8  0    11.11  0.10529   0.40107  381          \n 9  0    5.26   0.090067  0.27862  309          \n 10 0    18.18  0.23941   0.64476  269          \n 11 0    2.61   0.059824  0.20154  337          \n 12 0    9.67   0.5417    0.8617   159          \n 13 0    5.55   0.09393   0.30104  320          \n 14 0    10     0.058626  0.33518  572          \n 15 0    4.41   0.049205  0.25884  526          \n 16 0    20     0.24885   0.82579  332          \n 17 0    7.14   0.14259   0.44406  311          \n 18 0    9.09   0.18474   0.53112  287          \n 19 0    18.75  1.6621    1.7755   107          \n 20 0    18.18  0.085577  0.50977  596          \n 21 0    11.11  0.80976   1.2008   148          \n 22 0    17.1   0.1212    1.0258   846          \n 23 0    5.45   0.10165   0.35029  345          \n 24 0    12.5   0.094269  0.44264  470          \n 25 0    20.83  0.5495    1.6713   304          \n 26 0    16.66  0.26538   0.88696  334          \n 27 0    33.33  0.7673    3.3673   439          \n 28 0    9.09   0.12484   0.53858  431          \n 29 0    14.28  0.098915  0.59333  600          \n 30 0    5.88   0.10285   0.45668  444          \n 31 0    12.5   0.064753  0.40339  623          \n 32 0    4.76   0.047048  0.32856  698          \n 33 0    18.18  0.097229  0.55591  572          \n 34 0    4.76   0.047835  0.32945  689          \n 35 0    20     0.10541   0.53226  505          \n 36 0    7.69   0.097477  0.40262  413          \n 37 0    6.89   0.13695   0.42345  309          \n 38 0    8.33   0.013201  0.22065  1670         \n 39 0    11.11  0.078629  0.43467  553          \n 40 0    4.76   0.064834  0.34992  540          \n 41 0    7.14   0.043667  0.3612   827          \n 42 0    14.28  0.13234   0.76682  579          \n 43 0    3.57   0.046099  0.22381  486          \n 44 0    20     0.079196  0.62198  785          \n 45 0    21.42  0.30122   1.0117   336          \n 46 0    22.05  0.17982   0.91112  507          \n 47 0    2.17   0.0054445 0.076274 1400         \n 48 0    10     0.031869  0.28573  897          \n 49 0    4.385  0.038575  0.24347  631          \n 50 0    9.752  0.13903   0.27036  194          \n 51 0    4.081  0.016976  0.10939  644          \n 52 0    32.478 0.26907   0.81567  303          \n 53 0    6.003  0.075811  0.24588  324          \n 54 0    19.829 0.044238  0.42934  971          \n 55 1    1102.5 5.1915    31.729   611          \n 56 1    9989   52.173    194.89   374          \n 57 1    15841  283.29    606.35   214          \n 58 0    1      0.39404   0.4887   124          \n \n \n This file: \'spambase.DOCUMENTATION\' at the UCI Machine Learning Repository\n http://www.ics.uci.edu/~mlearn/MLRepository.html\n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:22:41', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/44/dataset_44_spambase.arff', 'true', 23, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:22:41'),
(24, 1, 0, 'splice', '1', '1', '**Author**: Genbank  \nDonor: G. Towell, M. Noordewier, and J. Shavlik  \n**Source**: [Genbank 64.1](genbank.bio.net) - 1/1/92  \n**Please cite**:   \n\nPrimate splice-junction gene sequences (DNA) with associated imperfect domain theory. All examples taken from Genbank 64.1. Categories \"ei\" and \"ie\" include every \"split-gene\" for primates in Genbank 64.1. Non-splice examples taken from sequences known not to include a splicing site.\n \nProblem Description:  \nSplice junctions are points on a DNA sequence at which \'superfluous\' DNA is removed during the process of protein creation in higher organisms. The problem posed in this dataset is to recognize, given a sequence of DNA, the boundaries between exons (the parts of the DNA sequence retained after splicing) and introns (the parts of the DNA sequence that are spliced out). This problem consists of two subtasks: recognizing exon/intron boundaries (referred to as EI sites), and recognizing intron/exon boundaries (IE sites). (In the biological community, IE borders are referred to a \'\'acceptors\'\' while EI borders are referred to as \'\'donors\'\'.)\n \nThis dataset has been developed to help evaluate a \"hybrid\" learning algorithm (KBANN) that uses examples to inductively refine preexisting knowledge.\n        \nAttributes:  \n>\n              1   One of {n ei ie}, indicating the class.\n              2   The instance name.\n           3-62   The remaining 60 fields are the sequence, starting at \n                  position -30 and ending at position +30. Each of\n                  these fields is almost always filled by one of \n                  {a, g, t, c}. Other characters indicate ambiguity among\n                  the standard characters according to the following table:\n    character: meaning\n        D: A or G or T\n        N: A or G or C or T\n        S: C or G\n        R: A or G\n\nNotes:  \n* Instance_name is an identifier and should be ignored for modelling', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:22:49', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/46/dataset_46_splice.arff', 'true', 24, 'Class', NULL, 'Instance_name', NULL, 'public', NULL, NULL, 'Instance_name is an identifier and should be ignored for modelling', '2014-09-19 17:06:29'),
(25, 1, 0, 'tic-tac-toe', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Tic-Tac-Toe Endgame database\n \n 2. Source Information\n    -- Creator: David W. Aha (aha@cs.jhu.edu)\n    -- Donor: David W. Aha (aha@cs.jhu.edu)\n    -- Date: 19 August 1991\n  \n 3. Known Past Usage: \n    1. Matheus,~C.~J., & Rendell,~L.~A. (1989).  Constructive\n       induction on decision trees.  In {it Proceedings of the\n       Eleventh International Joint Conference on Artificial Intelligence} \n       (pp. 645--650).  Detroit, MI: Morgan Kaufmann.\n       -- CITRE was applied to 100-instance training and 200-instance test\n          sets.  In a study using various amounts of domain-specific\n          knowledge, its highest average accuracy was 76.7% (using the\n          final decision tree created for testing).\n \n    2. Matheus,~C.~J. (1990). Adding domain knowledge to SBL through\n       feature construction.  In {it Proceedings of the Eighth National\n       Conference on Artificial Intelligence} (pp. 803--808). \n       Boston, MA: AAAI Press.\n       -- Similar experiments with CITRE, includes learning curves up\n          to 500-instance training sets but used _all_ instances in the\n          database for testing.  Accuracies reached above 90%, but specific\n          values are not given (see Chris\'s dissertation for more details).\n \n    3. Aha,~D.~W. (1991). Incremental constructive induction: An instance-based\n       approach.  In {it Proceedings of the Eighth International Workshop\n       on Machine Learning} (pp. 117--121).  Evanston, ILL: Morgan Kaufmann.\n       -- Used 70% for training, 30% of the instances for testing, evaluated\n          over 10 trials.  Results reported for six algorithms:\n          -- NewID:   84.0%\n          -- CN2:     98.1%  \n          -- MBRtalk: 88.4%\n          -- IB1:     98.1% \n          -- IB3:     82.0%\n          -- IB3-CI:  99.1%\n       -- Results also reported when adding an additional 10 irrelevant \n          ternary-valued attributes; similar _relative_ results except that\n          IB1\'s performance degraded more quickly than the others.\n \n 4. Relevant Information:\n \n    This database encodes the complete set of possible board configurations\n    at the end of tic-tac-toe games, where \"x\" is assumed to have played\n    first.  The target concept is \"win for x\" (i.e., true when \"x\" has one\n    of 8 possible ways to create a \"three-in-a-row\").  \n \n    Interestingly, this raw database gives a stripped-down decision tree\n    algorithm (e.g., ID3) fits.  However, the rule-based CN2 algorithm, the\n    simple IB1 instance-based learning algorithm, and the CITRE \n    feature-constructing decision tree algorithm perform well on it.\n \n 5. Number of Instances: 958 (legal tic-tac-toe endgame boards)\n \n 6. Number of Attributes: 9, each corresponding to one tic-tac-toe square\n \n 7. Attribute Information: (x=player x has taken, o=player o has taken, b=blank)\n \n     1. top-left-square: {x,o,b}\n     2. top-middle-square: {x,o,b}\n     3. top-right-square: {x,o,b}\n     4. middle-left-square: {x,o,b}\n     5. middle-middle-square: {x,o,b}\n     6. middle-right-square: {x,o,b}\n     7. bottom-left-square: {x,o,b}\n     8. bottom-middle-square: {x,o,b}\n     9. bottom-right-square: {x,o,b}\n    10. Class: {positive,negative}\n \n 8. Missing Attribute Values: None\n \n 9. Class Distribution: About 65.3% are positive (i.e., wins for \"x\")\n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:22:59', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/50/dataset_50_tic-tac-toe.arff', 'true', 25, 'Class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:22:59'),
(26, 1, 0, 'vehicle', '1', '1', '**Author**:  Peter Mowforth  \n**Source**: UCI -   \n**Please cite**: Siebert,JP. Turing Institute Research Memorandum TIRM-87-018 \"Vehicle Recognition Using Rule Based Methods\" (March 1987)  \n\n NAME\n         vehicle silhouettes\n \n PURPOSE\n         to classify a given silhouette as one of four types of vehicle,\n         using  a set of features extracted from the silhouette. The\n         vehicle may be viewed from one of many different angles.  \n \n PROBLEM TYPE\n         classification\n         \n SOURCE\n         Drs.Pete Mowforth and Barry Shepherd\n         Turing Institute\n         George House\n         36 North Hanover St.\n         Glasgow\n         G1 2AD\n \n CONTACT\n         Alistair Sutherland\n         Statistics Dept.\n         Strathclyde University\n         Livingstone Tower\n         26 Richmond St.\n         GLASGOW G1 1XH\n         Great Britain\n         \n         Tel: 041 552 4400 x3033\n         \n         Fax: 041 552 4711 \n         \n         e-mail: alistair@uk.ac.strathclyde.stams\n \n HISTORY\n         This data was originally gathered at the TI in 1986-87 by\n         JP Siebert. It was partially financed by Barr and Stroud Ltd.\n         The original purpose was to find a method of distinguishing\n         3D objects within a 2D image by application of an ensemble of\n         shape feature extractors to the 2D silhouettes of the objects.\n         Measures of shape features extracted from example silhouettes\n         of objects to be discriminated were used to generate a class-\n         ification rule tree by means of computer induction.\n          This object recognition strategy was successfully used to \n         discriminate between silhouettes of model cars, vans and buses\n         viewed from constrained elevation but all angles of rotation.\n          The rule tree classification performance compared favourably\n         to MDC (Minimum Distance Classifier) and k-NN (k-Nearest Neigh-\n         bour) statistical classifiers in terms of both error rate and\n         computational efficiency. An investigation of these rule trees\n         generated by example indicated that the tree structure was \n         heavily influenced by the orientation of the objects, and grouped\n         similar object views into single decisions.\n \n DESCRIPTION\n          The features were extracted from the silhouettes by the HIPS\n         (Hierarchical Image Processing System) extension BINATTS, which \n         extracts a combination of scale independent features utilising\n         both classical moments based measures such as scaled variance,\n         skewness and kurtosis about the major/minor axes and heuristic\n         measures such as hollows, circularity, rectangularity and\n         compactness.\n          Four \"Corgie\" model vehicles were used for the experiment:\n         a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400.\n         This particular combination of vehicles was chosen with the \n         expectation that the bus, van and either one of the cars would\n         be readily distinguishable, but it would be more difficult to\n         distinguish between the cars.\n          The images were acquired by a camera looking downwards at the\n         model vehicle from a fixed angle of elevation (34.2 degrees\n         to the horizontal). The vehicles were placed on a diffuse\n         backlit surface (lightbox). The vehicles were painted matte black\n         to minimise highlights. The images were captured using a CRS4000\n         framestore connected to a vax 750. All images were captured with\n         a spatial resolution of 128x128 pixels quantised to 64 greylevels.\n         These images were thresholded to produce binary vehicle silhouettes,\n         negated (to comply with the processing requirements of BINATTS) and\n         thereafter subjected to shrink-expand-expand-shrink HIPS modules to\n         remove \"salt and pepper\" image noise.\n          The vehicles were rotated and their angle of orientation was measured\n         using a radial graticule beneath the vehicle. 0 and 180 degrees\n         corresponded to \"head on\" and \"rear\" views respectively while 90 and\n         270 corresponded to profiles in opposite directions. Two sets of\n         60 images, each set covering a full 360 degree rotation, were captured\n         for each vehicle. The vehicle was rotated by a fixed angle between \n         images. These datasets are known as e2 and e3 respectively.\n          A further two sets of images, e4 and e5, were captured with the camera \n         at elevations of 37.5 degs and 30.8 degs respectively. These sets\n         also contain 60 images per vehicle apart from e4.van which contains\n         only 46 owing to the difficulty of containing the van in the image\n         at some orientations.\n \n ATTRIBUTES\n         \n         COMPACTNESS     (average perim)**2/area\n         \n         CIRCULARITY     (average radius)**2/area\n         \n         DISTANCE CIRCULARITY    area/(av.distance from border)**2\n         \n         RADIUS RATIO    (max.rad-min.rad)/av.radius\n         \n         PR.AXIS ASPECT RATIO    (minor axis)/(major axis)\n         \n         MAX.LENGTH ASPECT RATIO (length perp. max length)/(max length)\n         \n         SCATTER RATIO   (inertia about minor axis)/(inertia about major axis)\n         \n         ELONGATEDNESS           area/(shrink width)**2\n         \n         PR.AXIS RECTANGULARITY  area/(pr.axis length*pr.axis width)\n         \n         MAX.LENGTH RECTANGULARITY area/(max.length*length perp. to this)\n         \n         SCALED VARIANCE         (2nd order moment about minor axis)/area\n         ALONG MAJOR AXIS\n         \n         SCALED VARIANCE         (2nd order moment about major axis)/area\n         ALONG MINOR AXIS \n         \n         SCALED RADIUS OF GYRATION       (mavar+mivar)/area\n         \n         SKEWNESS ABOUT  (3rd order moment about major axis)/sigma_min**3\n         MAJOR AXIS\n         \n         SKEWNESS ABOUT  (3rd order moment about minor axis)/sigma_maj**3\n         MINOR AXIS\n                 \n         KURTOSIS ABOUT  (4th order moment about major axis)/sigma_min**4\n         MINOR AXIS  \n                 \n         KURTOSIS ABOUT  (4th order moment about minor axis)/sigma_maj**4\n         MAJOR AXIS\n         \n         HOLLOWS RATIO   (area of hollows)/(area of bounding polygon)\n         \n          Where sigma_maj**2 is the variance along the major axis and\n         sigma_min**2 is the variance along the minor axis, and\n         \n         area of hollows= area of bounding poly-area of object \n         \n          The area of the bounding polygon is found as a side result of\n         the computation to find the maximum length. Each individual\n         length computation yields a pair of calipers to the object\n         orientated at every 5 degrees. The object is propagated into\n         an image containing the union of these calipers to obtain an\n         image of the bounding polygon. \n         \n NUMBER OF CLASSES\n \n         4       OPEL, SAAB, BUS, VAN\n \n NUMBER OF EXAMPLES\n \n                 Total no. = 946\n                 \n                 No. in each class\n                 \n                   opel 240\n                   saab 240\n                   bus  240\n                   van  226\n                 \n                 \n                 100 examples are being kept by Strathclyde for validation.\n                 So StatLog partners will receive 846 examples.\n \n NUMBER OF ATTRIBUTES\n \n                 No. of atts. = 18', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:23:10', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/54/dataset_54_vehicle.arff', 'true', 26, 'Class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:23:10'),
(27, 1, 0, 'waveform-5000', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Waveform Database Generator (written in C)\n  \n 2. Source:\n    (a) Breiman,L., Friedman,J.H., Olshen,R.A., & Stone,C.J. (1984). \n        Classification and Regression Trees.  Wadsworth International\n        Group: Belmont, California.  (see pages 43-49).\n    (b) Donor: David Aha \n    (c) Date: 11/10/1988\n \n 3. Past Usage:\n      1. CART book (above):\n         -- Optimal Bayes classification rate: 86% accuracy\n         -- CART decision tree algorithm: 72%\n         -- Nearest Neighbor Algorithm: 78%\n            --	300 training and 5000 test instances\n \n 4. Relevant Information:\n      -- 3 classes of waves\n      -- 21 attributes, all of which include noise\n      -- See the book for details (49-55, 169)\n      -- waveform.data.Z contains 5000 instances\n \n 5. Number of Instances: chosen by user\n \n 6. Number of Attributes:\n     -- 21 attributes with continuous values between 0 and 6\n \n 7. Attribute Information:\n     -- Each class is generated from a combination of 2 of 3 \"base\" waves\n     -- Each instance is generated f added noise (mean 0, variance 1) in \n        each attribute\n      -- See the book for details (49-55, 169)\n     \n 8. Missing Attribute Values: none\n \n 9. Class Distribution: 33% for each of 3 classes', 'ARFF', NULL, NULL, NULL, '2014-04-06 23:23:37', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/60/dataset_60_waveform-5000.arff', 'true', 27, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-06 23:23:37'),
(28, 1, 0, 'electricity', '1', '1', '**Author**: M. Harries, J. Gama, A. Bifet  \n**Source**: [Gama](http://www.inescporto.pt/~jgama/ales/ales_5.html) - 2009  \n**Please cite**:   \n\n**Electricity** is a widely used dataset described by M. Harries and analysed by Gama. This data was collected from the Australian New South Wales Electricity Market. In this market, prices are not fixed and are affected by demand and supply of the market. They are set every five minutes. The ELEC dataset contains 45, 312 instances. The class label identifies the change of the price relative to a moving average of the last 24 hours.', 'ARFF', NULL, NULL, NULL, '2014-04-10 02:42:23', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/2419/electricity-normalized.arff', 'true', 28, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-10 02:42:23'),
(29, 1, 0, 'satimage', '1', '1', '**Author**: Ashwin Srinivasan, Department of Statistics and Data Modeling, University of Strathclyde \r\n**Source**: https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)\r\n**Please cite**:   \r\n\r\nThe database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:14:59', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3619/dataset_186_satimage.arff', 'true', 29, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:14:59'),
(30, 1, 0, 'abalone', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title of Database: Abalone data\n \n 2. Sources:\n \n    (a) Original owners of database:\n 	Marine Resources Division\n 	Marine Research Laboratories - Taroona\n 	Department of Primary Industry and Fisheries, Tasmania\n 	GPO Box 619F, Hobart, Tasmania 7001, Australia\n 	(contact: Warwick Nash +61 02 277277, wnash@dpi.tas.gov.au)\n \n    (b) Donor of database:\n 	Sam Waugh (Sam.Waugh@cs.utas.edu.au)\n 	Department of Computer Science, University of Tasmania\n 	GPO Box 252C, Hobart, Tasmania 7001, Australia\n \n    (c) Date received: December 1995\n \n \n 3. Past Usage:\n \n    Sam Waugh (1995) \"Extending and benchmarking Cascade-Correlation\", PhD\n    thesis, Computer Science Department, University of Tasmania.\n \n    -- Test set performance (final 1044 examples, first 3133 used for training):\n 	24.86% Cascade-Correlation (no hidden nodes)\n 	26.25% Cascade-Correlation (5 hidden nodes)\n 	21.5%  C4.5\n 	 0.0%  Linear Discriminate Analysis\n 	 3.57% k=5 Nearest Neighbour\n       (Problem encoded as a classification task)\n \n    -- Data set samples are highly overlapped.  Further information is required\n 	to separate completely using affine combinations.  Other restrictions\n 	to data set examined.\n \n    David Clark, Zoltan Schreter, Anthony Adams \"A Quantitative Comparison of\n    Dystal and Backpropagation\", submitted to the Australian Conference on\n    Neural Networks (ACNN\'96). Data set treated as a 3-category classification\n    problem (grouping ring classes 1-8, 9 and 10, and 11 on).\n \n    -- Test set performance (3133 training, 1044 testing as above):\n 	64%    Backprop\n 	55%    Dystal\n    -- Previous work (Waugh, 1995) on same data set:\n 	61.40% Cascade-Correlation (no hidden nodes)\n 	65.61% Cascade-Correlation (5 hidden nodes)\n 	59.2%  C4.5\n 	32.57% Linear Discriminate Analysis\n 	62.46% k=5 Nearest Neighbour\n \n \n 4. Relevant Information Paragraph:\n \n    Predicting the age of abalone from physical measurements.  The age of\n    abalone is determined by cutting the shell through the cone, staining it,\n    and counting the number of rings through a microscope -- a boring and\n    time-consuming task.  Other measurements, which are easier to obtain, are\n    used to predict the age.  Further information, such as weather patterns\n    and location (hence food availability) may be required to solve the problem.\n \n    From the original data examples with missing values were removed (the\n    majority having the predicted value missing), and the ranges of the\n    continuous values have been scaled for use with an ANN (by dividing by 200).\n \n    Data comes from an original (non-machine-learning) study:\n \n 	Warwick J Nash, Tracy L Sellers, Simon R Talbot, Andrew J Cawthorn and\n 	Wes B Ford (1994) \"The Population Biology of Abalone (_Haliotis_\n 	species) in Tasmania. I. Blacklip Abalone (_H. rubra_) from the North\n 	Coast and Islands of Bass Strait\", Sea Fisheries Division, Technical\n 	Report No. 48 (ISSN 1034-3288)\n \n \n 5. Number of Instances: 4177\n \n \n 6. Number of Attributes: 8\n \n \n 7. Attribute information:\n \n    Given is the attribute name, attribute type, the measurement unit and a\n    brief description.  The number of rings is the value to predict: either\n    as a continuous value or as a classification problem.\n \n 	Name		Data Type	Meas.	Description\n 	----		---------	-----	-----------\n 	Sex		nominal			M, F, and I (infant)\n 	Length		continuous	mm	Longest shell measurement\n 	Diameter	continuous	mm	perpendicular to length\n 	Height		continuous	mm	with meat in shell\n 	Whole weight	continuous	grams	whole abalone\n 	Shucked weight	continuous	grams	weight of meat\n 	Viscera weight	continuous	grams	gut weight (after bleeding)\n 	Shell weight	continuous	grams	after being dried\n 	Rings		integer			+1.5 gives the age in years\n \n    Statistics for numeric domains:\n \n 		Length	Diam	Height	Whole	Shucked	Viscera	Shell	Rings\n 	Min	0.075	0.055	0.000	0.002	0.001	0.001	0.002	    1\n 	Max	0.815	0.650	1.130	2.826	1.488	0.760	1.005	   29\n 	Mean	0.524	0.408	0.140	0.829	0.359	0.181	0.239	9.934\n 	SD	0.120	0.099	0.042	0.490	0.222	0.110	0.139	3.224\n 	Correl	0.557	0.575	0.557	0.540	0.421	0.504	0.628	  1.0\n \n \n 8. Missing Attribute Values: None\n \n \n 9. Class Distribution:\n \n 	Class	Examples\n 	-----	--------\n 	1	1\n 	2	1\n 	3	15\n 	4	57\n 	5	115\n 	6	259\n 	7	391\n 	8	568\n 	9	689\n 	10	634\n 	11	487\n 	12	267\n 	13	203\n 	14	126\n 	15	103\n 	16	67\n 	17	58\n 	18	42\n 	19	32\n 	20	26\n 	21	14\n 	22	6\n 	23	9\n 	24	2\n 	25	1\n 	26	1\n 	27	2\n 	29	1\n 	-----	----\n 	Total	4177\n \n Num Instances:     4177\n Num Attributes:    9\n Num Continuous:    8 (Int 1 / Real 7)\n Num Discrete:      1\n Missing values:    0 /  0.0%\n\n     name                      type enum ints real     missing    distinct  (1)\n   1 \'Sex\'                     Enum 100%   0%   0%     0 /  0%     3 /  0%   0% \n   2 \'Length\'                  Real   0%   0% 100%     0 /  0%   134 /  3%   0% \n   3 \'Diameter\'                Real   0%   0% 100%     0 /  0%   111 /  3%   0% \n   4 \'Height\'                  Real   0%   0% 100%     0 /  0%    51 /  1%   0% \n   5 \'Whole weight\'            Real   0%   0% 100%     0 /  0%  2429 / 58%  31% \n   6 \'Shucked weight\'          Real   0%   0% 100%     0 /  0%  1515 / 36%  10% \n   7 \'Viscera weight\'          Real   0%   0% 100%     0 /  0%   880 / 21%   3% \n   8 \'Shell weight\'            Real   0%   0% 100%     0 /  0%   926 / 22%   8% \n   9 \'Class_Rings\'             Int    0% 100%   0%     0 /  0%    28 /  1%   0%', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:15:04', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3620/dataset_187_abalone.arff', 'true', 30, 'Class_number_of_rings', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:15:04'),
(31, 1, 0, 'eucalyptus', '1', '1', '**Author**: Bruce Bulloch  \n**Source**: [WEKA Dataset Collection](http://www.cs.waikato.ac.nz/ml/weka/datasets.html)   \n**Please cite**:   \n\n**Eucalyptus Soil Conservation**  \nThe objective was to determine which seedlots in a species are best for soil conservation in seasonally dry hill country. Determination is found by measurement of height, diameter by height, survival, and other contributing factors. \n \nIt is important to note that eucalypt trial methods changed over time; earlier trials included mostly 15 - 30cm tall seedling grown in peat plots and the later trials have included mostly three replications of eight trees grown. This change may contribute to less significant results.\n\nExperimental data recording procedures which require noting include:\n - instances with no data recorded due to experimental recording procedures\n   require that the absence of a species from one replicate at a site was\n   treated as a missing value, but if absent from two or more replicates at a\n   site the species was excluded from the site\'s analyses.\n - missing data for survival, vigour, insect resistance, stem form, crown form\n   and utility especially for the data recorded at the Morea Station; this \n   could indicate the death of species in these areas or a lack in collection\n   of data.  \n\n Attribute Information:  \n  1.  Abbrev - site abbreviation - enumerated\n  2.  Rep - site rep - integer\n  3.  Locality - site locality in the North Island - enumerated\n  4.  Map_Ref - map location in the North Island - enumerated\n  5.  Latitude - latitude approximation - enumerated\n  6.  Altitude - altitude approximation - integer\n  7.  Rainfall - rainfall (mm pa) - integer\n  8.  Frosts - frosts (deg. c) - integer\n  9.  Year - year of planting - integer\n  10. Sp - species code - enumerated\n  11. PMCno - seedlot number - integer\n  12. DBH - best diameter base height (cm) - real\n  13. Ht - height (m) - real\n  14. Surv - survival - integer\n  15. Vig - vigour - real\n  16. Ins_res - insect resistance - real\n  17. Stem_Fm - stem form - real\n  18. Crown_Fm - crown form - real\n  19. Brnch_Fm - branch form - real\n  Class:\n  20. Utility - utility rating - enumerated\n\n Class Distribution:\n  none    - 180\n  low     - 107\n  average - 130\n  good    - 214\n  best    - 105\n\n Contact: Bruce Bulloch, 128 Cook Street, Palmerston North, New Zealand', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:15:28', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3625/dataset_194_eucalyptus.arff', 'true', 31, 'Utility', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:15:28'),
(35, 2, 0, 'monks-problems-1', '1', NULL, '**Author**: Sebastian Thrun  \n**Source**: [original](https://archive.ics.uci.edu/ml/datasets/MONK\'s+Problems) - October 1992  \n**Please cite**:   \n\nThe Monk\'s Problems: Problem 1\n\nThis is a merged version of the separate train and test set which are usually distributed. On OpenML this train-test split can be found as one of the possible tasks. \n\nSources:  \n(a) Donor: Sebastian Thrun School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA  E-mail: thrun@cs.cmu.edu  \n(b) Date: October 1992\n\n4. Relevant Information:  The MONK\'s problem were the basis of a first international comparison of learning algorithms. The result of this comparison is summarized in \"The MONK\'s Problems - A Performance Comparison of Different Learning algorithms\" by S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang has been published as Technical Report CS-CMU-91-197, Carnegie Mellon University in Dec. 1991.  One significant characteristic of this comparison is that it was performed by a collection of researchers, each of whom was an advocate of the technique they tested (often they were the creators of the various methods). In this sense, the results are less biased than in comparisons performed by a single person advocating a specific learning method, and more accurately reflect the generalization behavior of the learning techniques as applied by knowledgeable users.  There are three MONK\'s problems. The domains for all MONK\'s problems are the same (described below). One of the MONK\'s problems has noise added. For each problem, the domain has been partitioned into a train and test set.\n\nAttribute information: \n1. class: 0, 1  \n2. a1: 1, 2, 3 \n3. a2: 1, 2, 3 \n4. a3: 1, 2 \n5. a4: 1, 2, 3 \n6. a5: 1, 2, 3, 4 \n7. a6: 1, 2 8. \n\nTarget Concepts associated to the MONK\'s problem:  \nMONK-1: (a1 = a2) or (a5 = 1)  \nMONK-2: EXACTLY TWO of {a1 = 1, a2 = 1, a3 = 1, a4 = 1, a5 = 1, a6 = 1}  \nMONK-3: (a5 = 3 and a4 = 1) or (a5 /= 4 and a2 /= 3) (5% class noise added to the training set)', 'ARFF', '\"Sebastian Thrun\"', NULL, 'October 1992', '2014-08-26 17:11:18', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52236/phpAyyBys', 'true', 35, 'class', NULL, NULL, NULL, 'public', NULL, 'https://archive.ics.uci.edu/ml/datasets/MONK\'s+Problems', NULL, '2014-08-26 17:11:18'),
(36, 2, 0, 'monks-problems-2', '1', NULL, '**Author**: Sebastian Thrun  \n**Source**: [original](https://archive.ics.uci.edu/ml/datasets/MONK\'s+Problems) - October 1992  \n**Please cite**:   \n\nThe Monk\'s Problems: Problem 2  \n\nThis is a merged version of the separate train and test set which are usually distributed. On OpenML this train-test split can be found as one of the possible tasks.   \n\nSources:  \n(a) Donor: Sebastian Thrun School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA E-mail: thrun@cs.cmu.edu  \n(b) Date: October 1992  \n\nRelevant Information: The MONK\'s problem were the basis of a first international comparison of learning algorithms. The result of this comparison is summarized in \"The MONK\'s Problems - A Performance Comparison of Different Learning algorithms\" by S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang has been published as Technical Report CS-CMU-91-197, Carnegie Mellon University in Dec. 1991. One significant characteristic of this comparison is that it was performed by a collection of researchers, each of whom was an advocate of the technique they tested (often they were the creators of the various methods). In this sense, the results are less biased than in comparisons performed by a single person advocating a specific learning method, and more accurately reflect the generalization behavior of the learning techniques as applied by knowledgeable users. There are three MONK\'s problems. The domains for all MONK\'s problems are the same (described below). One of the MONK\'s problems has noise added. For each problem, the domain has been partitioned into a train and test set.  \n\nAttribute information:  \n1. class: 0, 1  \n2. a1: 1, 2, 3  \n3. a2: 1, 2, 3  \n4. a3: 1, 2  \n5. a4: 1, 2, 3  \n6. a5: 1, 2, 3, 4  \n7. a6: 1, 2 \n\n8.   Target Concepts associated to the MONK\'s problem:  \nMONK-1: (a1 = a2) or (a5 = 1)  \nMONK-2: EXACTLY TWO of {a1 = 1, a2 = 1, a3 = 1, a4 = 1, a5 = 1, a6 = 1}  \nMONK-3: (a5 = 3 and a4 = 1) or (a5 /= 4 and a2 /= 3) (5% class noise added to the training set)', 'ARFF', '\"Sebastian Thrun\"', NULL, 'October 1992', '2014-08-26 17:29:02', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52237/php4fATLZ', 'true', 36, 'class', NULL, NULL, NULL, 'public', NULL, 'https://archive.ics.uci.edu/ml/datasets/MONK\'s+Problems', NULL, '2014-08-26 17:29:02'),
(37, 2, 0, 'monks-problems-3', '1', NULL, '**Author**: Sebastian Thrun  \n**Source**: [original](https://archive.ics.uci.edu/ml/datasets/MONK\'s+Problems) -   \n**Please cite**:   \n\nThe Monk\'s Problems: Problem 3   \n\nThis is a merged version of the separate train and test set which are usually distributed. On OpenML this train-test split can be found as one of the possible tasks.   \n\nSources:  \n(a) Donor: Sebastian Thrun School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA E-mail: thrun@cs.cmu.edu  \n(b) Date: October 1992   \n\nRelevant Information: The MONK\'s problem were the basis of a first international comparison of learning algorithms. The result of this comparison is summarized in \"The MONK\'s Problems - A Performance Comparison of Different Learning algorithms\" by S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang has been published as Technical Report CS-CMU-91-197, Carnegie Mellon University in Dec. 1991. One significant characteristic of this comparison is that it was performed by a collection of researchers, each of whom was an advocate of the technique they tested (often they were the creators of the various methods). In this sense, the results are less biased than in comparisons performed by a single person advocating a specific learning method, and more accurately reflect the generalization behavior of the learning techniques as applied by knowledgeable users. There are three MONK\'s problems. The domains for all MONK\'s problems are the same (described below). One of the MONK\'s problems has noise added. For each problem, the domain has been partitioned into a train and test set.   \n\nAttribute information:  \n1. class: 0, 1  \n2. a1: 1, 2, 3  \n3. a2: 1, 2, 3  \n4. a3: 1, 2  \n5. a4: 1, 2, 3  \n6. a5: 1, 2, 3, 4  \n7. a6: 1, 2  \n\nTarget Concepts associated to the MONK\'s problem:  \nMONK-1: (a1 = a2) or (a5 = 1)  \nMONK-2: EXACTLY TWO of {a1 = 1, a2 = 1, a3 = 1, a4 = 1, a5 = 1, a6 = 1}  \nMONK-3: (a5 = 3 and a4 = 1) or (a5 /= 4 and a2 /= 3) (5% class noise added to the training set)', 'ARFF', '\"Sebastian Thrun\"', NULL, NULL, '2014-08-26 17:41:07', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52238/phphZierv', 'true', 37, 'class', NULL, NULL, NULL, 'public', NULL, 'https://archive.ics.uci.edu/ml/datasets/MONK\'s+Problems', NULL, '2014-08-26 17:41:07'),
(38, 2, NULL, 'JapaneseVowels', '1', NULL, '**Author**: Mineichi Kudo, Jun Toyama, Masaru Shimbo ({mine,jun,shimbo}@main.eng.hokudai.ac.jp)  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Japanese+Vowels) - 2000  \n**Please cite**:   \n\n**Japanese vowels**  \nThis dataset records 640 time series of 12 LPC cepstrum coefficients taken from nine male speakers.\n\nThe data was collected for examining our newly developed classifier for multidimensional curves (multidimensional time series). Nine male speakers uttered two Japanese vowels /ae/ successively. For each utterance, with the analysis parameters described below, we applied 12-degree linear prediction analysis to it to obtain a discrete-time series with 12 LPC cepstrum coefficients. This means that one utterance by a speaker forms a time series whose length is in the range 7-29 and each point of a time series is of 12 features (12 coefficients).\n\nSimilar data are available for different utterances /ei/, /iu/, /uo/, /oa/ in addition to /ae/. Please contact the donor if you are interested in using this data.\n\nThe number of the time series is 640 in total. We used one set of 270 time series for training and the other set of 370 time series for testing.\n\nAnalysis parameters:  \n* Sampling rate : 10kHz\n* Frame length : 25.6 ms\n* Shift length : 6.4ms\n* Degree of LPC coefficients : 12\n\nEach line represents 12 LPC coefficients in the increasing order separated by spaces. This corresponds to one analysis\nframe. Lines are organized into blocks, which are a set of 7-29 lines separated by blank lines and corresponds to a single speech utterance of /ae/ with 7-29 frames.\n\nEach speaker is a set of consecutive blocks. In ae.train there are 30 blocks for each speaker. Blocks 1-30 represent speaker 1, blocks 31-60 represent speaker 2, and so on up to speaker 9. In ae.test, speakers 1 to 9 have the corresponding number of blocks: 31 35 88 44 29 24 40 50 29. Thus, blocks 1-31 represent speaker 1 (31 utterances of /ae/), blocks 32-66 represent speaker 2 (35 utterances of /ae/), and so on.\n\n**Past Usage**\n\nM. Kudo, J. Toyama and M. Shimbo. (1999). \"Multidimensional Curve Classification Using Passing-Through Regions\". Pattern Recognition Letters, Vol. 20, No. 11--13, pages 1103--1111.\n\nIf you publish any work using the dataset, please inform the donor. Use for commercial purposes requires donor permission.\n\nReferences  \n\n1. http://ips9.main.eng.hokudai.ac.jp/index_e.html\n2. mailto:mine@main.eng.hokudai.ac.jp\n3. mailto:jun@main.eng.hokudai.ac.jp\n4. mailto:shimbo@main.eng.hokudai.ac.jp\n5. http://kdd.ics.uci.edu/\n6. http://www.ics.uci.edu/\n7. http://www.uci.edu/', 'ARFF', NULL, NULL, NULL, '2014-09-27 10:56:03', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52415/JapaneseVowels.arff', 'true', 38, 'speaker', NULL, NULL, NULL, 'public', NULL, NULL, 'set target feature', '2014-09-27 11:19:36'),
(39, 2, NULL, 'synthetic_control', '1', NULL, '**Author**: Dr Robert Alcock (rob@skyblue.csd.auth.gr)  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Synthetic+Control+Chart+Time+Series) - 1999  \n**Please cite**:   \n\n**Synthetic Control Chart Time Series**  \nThis data consists of synthetically generated control charts. This dataset contains 600 examples of control charts synthetically generated by the process in Alcock and Manolopoulos (1999). There are six different classes of control charts:\n\n1. Normal\n2. Cyclic\n3. Increasing trend\n4. Decreasing trend\n5. Upward shift\n6. Downward shift\n\n**Past Usage**  \nAlcock R.J. and Manolopoulos Y. Time-Series Similarity Queries Employing a Feature-Based Approach. 7th Hellenic Conference on\nInformatics. August 27-29. Ioannina,Greece 1999.\n\n**References and Further Information**  \nD.T. Pham and A.B. Chan \"Control Chart Pattern Recognition using a New Type of Self Organizing Neural Network\" Proc. Instn, Mech, Engrs. Vol 212, No 1, pp 115-127 1998.\n\nReferences  \n\n1. http://skyblue.csd.auth.gr/~rob/\n2. mailto:rob@skyblue.csd.auth.gr\n3. http://kdd.ics.uci.edu/\n4. http://www.ics.uci.edu/\n5. http://www.uci.edu/', 'ARFF', NULL, NULL, NULL, '2014-09-27 10:56:07', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52417/synthetic_control.arff', 'true', 39, 'class', 'index', NULL, NULL, 'public', NULL, NULL, 'set index feature', '2014-09-27 11:35:53'),
(40, 2, NULL, 'irish', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nIrish Educational Transitions Data\n\nBelow are shown data on educational transitions for a sample of 500\nIrish schoolchildren aged 11 in 1967. The data were collected by\nGreaney and Kelleghan (1984), and reanalyzed by Raftery and Hout (1985, 1993).\n\nThe data were also used, in a simplified form, as an example to illustrate\nBayesian model selection methods by Raftery (1988) and Kass and Raftery (1993).\nIn that simplified form, primary terminal leavers and cases with any missing\ndata were removed, leaving 441 cases. The Leaving Certificate variable\nwas used as the dependent variable in a logistic regression analysis.\n\nThe variables shown are as follows:\n\n1. Sex: 1=male; 2=female.\n\n2. DVRT (Drumcondra Verbal Reasoning Test Score).\n\n3. Educational level attained:\n1		Primary terminal leaver\n2	 	Junior cycle incomplete: vocational school\n3		Junior cycle incomplete: secondary school\n4		Junior cycle terminal leaver: vocational school\n5		Junior cycle terminal leaver: secondary school\n6		Senior cycle incomplete: vocational school\n7		Senior cycle incomplete: secondary school\n8		Senior cycle terminal leaver: vocational school\n9		Senior cycle terminal leaver: secondary school\n10		3rd level incomplete\n11		3rd level complete\n\n4. Leaving Certificate. 1 if Leaving Certificate not taken; 2 if taken.\n\n5. Prestige score for father\'s occupation\n(calculated by Raftery and Hout, 1985). 0 if missing.\n\n6. Type of school: 1=secondary; 2=vocational; 9=primary terminal leaver.\n\nREFERENCES\n\nGreaney, V. and Kelleghan, T. (1984). Equality of Opportunity in Irish\nSchools. Dublin: Educational Company.\n\nKass, R.E. and Raftery, A.E. (1993). Bayes factors and model uncertainty.\nTechnical Report no. 254, Department of Statistics, University of Washington.\nRevised version to appear in Journal of the American Statistical\nAssociation.\n\nRaftery, A.E. (1988). Approximate Bayes factors for generalized linear models.\nTechnical Report no. 121, Department of Statistics, University of Washington.\n\nRaftery, A.E. and Hout, M. (1985). Does Irish education approach the\nmeritocratic ideal? A logistic analysis.\nEconomic and Social Review, 16, 115-140.\n\nRaftery, A.E. and Hout, M. (1993). Maximally maintained inequality:\nExpansion, reform and opportunity in Irish schools.\nSociology of Education, 66, 41-62.\n\n\nOWNERSHIP STATEMENT\n\nThis data belongs to Vincent Greaney and Thomas Kelleghan,\nEducational Research Centre, St. Patrick\'s College, Drumcondra,\nDublin 9, Ireland, who retain the copyright.\n\nIn the form given here, it may be used solely as an example for research\non the development of statistical methods. For any other use of the data,\npermission must be obtained from the owners.  Subject to this statement,\npermission is hereby given to StatLib to distribute this data freely.\n\nSubmitted by Adrian Raftery (raftery@stat.washington.edu).\n\nCopyright 1984 Vincent Greaney and Thomas Kelleghan.\n\n\n\n\nInformation about the dataset\nCLASSTYPE: nominal\nCLASSINDEX: 4', 'ARFF', NULL, NULL, NULL, '2014-09-28 23:50:51', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52563/irish.arff', 'true', 40, 'Leaving_Certificate', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-09-28 23:50:51'),
(41, 2, NULL, 'analcatdata_authorship', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nanalcatdata    A collection of data sets used in the book \"Analyzing Categorical Data,\"\nby Jeffrey S. Simonoff, Springer-Verlag, New York, 2003. The submission\nconsists of a zip file containing two versions of each of 84 data sets,\nplus this README file. Each data set is given in comma-delimited ASCII\n(.csv) form, and Microsoft Excel (.xls) form.\n\nNOTICE: These data sets may be used freely for scientific, educational and/or\nnoncommercial purposes, provided suitable acknowledgment is given (by citing\nthe above-named reference).\n\nFurther details concerning the book, including information on statistical software\n(including sample S-PLUS/R and SAS code), are available at the web site\n\nhttp://www.stern.nyu.edu/~jsimonof/AnalCatData\n\n\nInformation about the dataset\nCLASSTYPE: nominal\nCLASSINDEX: last\n\n\nNote: Quotes, Single-Quotes and Backslashes were removed, Blanks replaced\nwith Underscores', 'ARFF', NULL, NULL, NULL, '2014-09-28 23:51:06', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52570/analcatdata_authorship.arff', 'true', 41, 'Author', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-09-28 23:51:06'),
(42, 2, NULL, 'analcatdata_dmft', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nanalcatdata    A collection of data sets used in the book \"Analyzing Categorical Data,\"\nby Jeffrey S. Simonoff, Springer-Verlag, New York, 2003. The submission\nconsists of a zip file containing two versions of each of 84 data sets,\nplus this README file. Each data set is given in comma-delimited ASCII\n(.csv) form, and Microsoft Excel (.xls) form.\n\nNOTICE: These data sets may be used freely for scientific, educational and/or\nnoncommercial purposes, provided suitable acknowledgment is given (by citing\nthe above-named reference).\n\nFurther details concerning the book, including information on statistical software\n(including sample S-PLUS/R and SAS code), are available at the web site\n\nhttp://www.stern.nyu.edu/~jsimonof/AnalCatData\n\n\nInformation about the dataset\nCLASSTYPE: nominal\nCLASSINDEX: last\n\n\nNote: Quotes, Single-Quotes and Backslashes were removed, Blanks replaced\nwith Underscores', 'ARFF', NULL, NULL, NULL, '2014-09-28 23:51:25', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52581/analcatdata_dmft.arff', 'true', 42, 'Prevention', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-09-28 23:51:25'),
(43, 2, NULL, 'profb', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nPRO FOOTBALL SCORES  (raw data appears after the description below)\n\nHow well do the oddsmakers of Las Vegas predict the outcome of\nprofessional football games?  Is there really a home field advantage - if\nso how large is it?  Are teams that play the Monday Night game at a\ndisadvantage when they play again the following Sunday?  Do teams benefit\nfrom having a \"bye\" week off in the current schedule?  These questions and\na host of others can be investigated using this data set.\n\nHal Stern from the Statistics Department at Harvard University has\nmade available his compilation of scores for all National Football League\ngames from the 1989, 1990, and 1991 seasons.  Dr. Stern used these data as\npart of his presentation \"Who\'s Number One?\" in the special \"Best of\nBoston\" session at the 1992 Joint Statistics Meetings.\n\nSeveral variables in the data are keyed to the oddsmakers \"point\nspread\" for each game.  The point spread is a value assigned before each\ngame to serve as a handicap for whichever is perceived to be the better\nteam.  Thus, to win against the point spread, the \"favorite\" team must beat\nthe \"underdog\" team by more points than the spread.  The underdog \"wins\"\nagainst the spread if it wins the game outright or manages to lose by fewer\npoints than the spread.  In theory, the point spread should represent the\n\"expert\" prediction as to the game\'s outcome.  In practice, it more usually\ndenotes a point at which an equal amount of money will be wagered both for\nand against the favored team.\n\nRaw data below contains 672 cases (all 224 regular season games in\neach season and informatino on the following 9 varialbes:     .\n\nHome/Away       = Favored team is at home (1) or away (0)\nFavorite Points = Points scored by the favored team\nUnderdog Points = Points scored by the underdog team\nPointspread     = Oddsmaker\'s points to handicap the favored team\nFavorite Name   = Code for favored team\'s name\nUnderdog name   = Code for underdog\'s name\nYear            = 89, 90, or 91\nWeek            = 1, 2, ... 17\nSpecial         = Mon.night (M), Sat. (S), Thur. (H), Sun. night (N)\not - denotes an overtime game\n\n\nData were submitted by: Robin Lock  (rlock@stlawu.bitnet)\nMathematics Department, St. Lawrence University\n\nData were compiled by: Hal Stern, Dept. of Statistics, Harvard University\n\n\n\nInformation about the dataset\nCLASSTYPE: nominal\nCLASSINDEX: 1', 'ARFF', NULL, NULL, NULL, '2014-09-28 23:51:27', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52582/profb.arff', 'true', 43, 'Home/Away', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-09-28 23:51:27'),
(44, 2, NULL, 'collins', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nThe following are data used in an analysis of the Brown and Frown corpora for my doctoral dissertation titled ``Variations in Written English: Characterizing Authors\' Rhetorical Language Choices Across Corpora of Published Texts\" (Completed at Carnegie Mellon Univ, 2003).  The source of the corpora was the ICAME CD-ROM  (get info at <http://www.hit.uib.no/icame/cd>).\n\nThe data were generated from the texts using tagging and visualization software, Docuscope.\n\nThe first row is the variable names. The genre of each text (assigned by the Brown corpus compilers) is in \'Genre\' column and the corpus is listed in the \'corpus\' column with 1=Brown and 2=Frown corpus.\n\nThe dataset may be freely used and distributed for non-commercial purposes.\n\nJeff Collins <jeff.collins@acm.org> 11 July 2003\n\n\n\nInformation about the dataset\nCLASSTYPE: nominal\nCLASSINDEX: last', 'ARFF', NULL, NULL, NULL, '2014-09-28 23:51:43', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52590/collins.arff', 'true', 44, 'Corp.Genre', 'Counter', '\"Text\"', NULL, 'public', NULL, NULL, 'attribute counter is a row id', '2015-04-15 17:08:50'),
(45, 2, NULL, 'sylva_agnostic', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nDatasets from the Agnostic Learning vs. Prior Knowledge Challenge (http://www.agnostic.inf.ethz.ch)\n\nDataset from: http://www.agnostic.inf.ethz.ch/datasets.php\n\nModified by TunedIT (converted to ARFF format)\n\n\nSYLVA is the ecology database\n\n\nThe task of SYLVA is to classify forest cover types. The forest cover type for 30 x 30 meter cells is obtained from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. We brought it back to a two-class classification problem (classifying Ponderosa pine vs. everything else). The \"agnostic learning track\" data consists in 216 input variables. Each pattern is composed of 4 records: 2 true records matching the target and 2 records picked at random. Thus 1/2 of the features are distracters.\n\nData type: non-sparse\nNumber of features: 216\nNumber of examples and check-sums:\nPos_ex	Neg_ex	Tot_ex	Check_sum\nTrain	  805	12281	13086	238271607.00\nValid	   81	 1228	 1309	23817234.00\n\n\nThis dataset contains samples from both training and validation datasets.', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:55:56', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53919/sylva_agnostic.arff', 'true', 45, 'label', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:55:56'),
(46, 2, NULL, 'gina_agnostic', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nDatasets from the Agnostic Learning vs. Prior Knowledge Challenge (http://www.agnostic.inf.ethz.ch)\n\nDataset from: http://www.agnostic.inf.ethz.ch/datasets.php\n\n\nModified by TunedIT (converted to ARFF format)\n\n\nGINA is digit recognition database\n\nThe task of GINA is handwritten digit recognition. For the \"agnostic learning track\" we chose the problem of separating two-digit odd numbers from two-digit even numbers. Only the unit digit is informative for that task, therefore at least 1/2 of the features are distracters. Additionally, the pixels that are almost always blank were removed and the pixel order was randomized to hide the feature identity.  This is a two class classification problem with sparse continuous input variables, in which each class is composed of several clusters. It is a problem with heterogeneous classes.\n\nData type: non-sparse\nNumber of features: 970\nNumber of examples and check-sums:\nPos_ex	Neg_ex	Tot_ex	Check_sum\nTrain	 1550	 1603	 3153	164947945.00\nValid	  155	  160	  315	16688946.00\n\n\nThis dataset contains samples from both training and validation datasets.', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:56:01', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53921/gina_agnostic.arff', 'true', 46, 'label', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:56:01'),
(47, 2, NULL, 'ada_agnostic', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nDatasets from the Agnostic Learning vs. Prior Knowledge Challenge (http://www.agnostic.inf.ethz.ch)\n\nDataset from: http://www.agnostic.inf.ethz.ch/datasets.php\n\n\nModified by TunedIT (converted to ARFF format)\n\n\nADA is the marketing database\n\nThe task of ADA is to discover high revenue people from census data. This is a two-class classification problem. The raw data from the census bureau is known as the Adult database in the UCI machine-learning repository. The 14 original attributes (features) include age, workclass,  education,\nmarital status, occupation, native country, etc. It contains continuous, binary and categorical features. This dataset is from the \"agnostic learning track\", i.e. has access to a preprocessed numeric representation eliminating categorical variables, but the identity of the features is not revealed.\n\n\n\nData type: non-sparse\nNumber of features: 48\nNumber of examples and check-sums:\nPos_ex	Neg_ex	Tot_ex	Check_sum\nTrain	 1029	 3118	 4147	6798109.00\nValid	  103	  312	  415	681151.00\n\n\nThis dataset contains samples from both training and validation datasets.', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:56:15', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53926/ada_agnostic.arff', 'true', 47, 'label', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:56:15'),
(48, 2, NULL, 'mozilla4', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nThis is a PROMISE Software Engineering Repository data set made publicly\navailable in order to encourage repeatable, verifiable, refutable, and/or\nimprovable predictive models of software engineering.\n\nIf you publish material based on PROMISE data sets then, please\nfollow the acknowledgment guidelines posted on the PROMISE repository\nweb page http://promisedata.org/repository .\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n(c) 2007  A. Gunes Koru\nContact: gkoru AT umbc DOT edu Phone: +1 (410) 455 8843\nThis data set is distributed under the\nCreative Commons Attribution-Share Alike 3.0 License\nhttp://creativecommons.org/licenses/by-sa/3.0/\n\nYou are free:\n\n* to Share -- copy, distribute and transmit the work\n* to Remix -- to adapt the work\n\nUnder the following conditions:\n\nAttribution. You must attribute the work in the manner specified by\nthe author or licensor (but not in any way that suggests that they endorse\nyou or your use of the work).\n\nShare Alike. If you alter, transform, or build upon this work, you\nmay distribute the resulting work only under the same, similar or a\ncompatible license.\n\n* For any reuse or distribution, you must make clear to others the\nlicense terms of this work.\n* Any of the above conditions can be waived if you get permission from\nthe copyright holder.\n* Apart from the remix rights granted under this license, nothing in\nthis license impairs or restricts the author\'s moral rights.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\n1. Title: Recurrent event (defect fix) and size data for Mozilla Classes\nThis one includes a binary attribute (event) to show defect fix.\nThe data is at the \"observation\" level. Each modification made to\na C++ class was entered as an observation. A newly added class\ncreated an observation. The observation period was between\nMay 29, 2002 and Feb 22, 2006.\n\n2. Sources\n(a) Creator: A. Gunes Koru\n(b) Date: February 23, 2007\n(c) Contact: gkoru AT umbc DOT edu Phone: +1 (410) 455 8843\n\n3. Donor: A. Gunes Koru\n\n4. Past Usage: This data set was used for:\n\nA. Gunes Koru, Dongsong Zhang, and Hongfang Liu, \"Modeling the\nEffect of Size on Defect Proneness for Open-Source Software\",\nPredictive Models in Software Engineering Workshop, PROMISE 2007,\nMay 20th 2007, Minneapolis, Minnesota, US.\n\nAbstract:\nQuality is becoming increasingly important with the continuous\nadoption of open-source software.  Previous research has found that\nthere is generally a positive relationship between module size and\ndefect proneness. Therefore, in open-source software development, it\nis important to monitor module size and understand its impact on\ndefect proneness. However, traditional approaches to quality\nmodeling, which measure specific system snapshots and obtain future\ndefect counts, are not well suited because open-source modules\nusually evolve and their size changes over time. In this study, we\nused Cox proportional hazards modeling with recurrent events to\nstudy the effect of class size on defect-proneness in the Mozilla\nproduct. We found that the effect of size was significant, and we\nquantified this effect on defect proneness.\n\nThe full paper can be downloaded from A. Gunes Koru\'s Website\nhttp://umbc.edu/~gkoru\nby following the Publications link or from the Web site of PROMISE 2007.\n\n5. Features:\n\nThis data set is used to create a conditional Cox Proportional\nHazards Model\n\nid: A numeric identification assigned to each separate C++ class\n(Note that the id\'s do not increment from the first to the last\ndata row)\n\nstart: A time infinitesimally greater than the time of the modification\nthat created this observation (practically, modification time). When a\nclass is introduced to a system, a new observation is entered with start=0\n\nend: Either the time of the next modification, or the end of the\nobservation period, or the time of deletion, whichever comes first.\n\nevent: event is set to 1 if a defect fix takes place\nat the time represented by \'end\', or 0 otherwise.  A class deletion\nis handled easily by entering a final observation whose event is set\nto 1 if the class is deleted for corrective maintenance, or 0 otherwise.\n\nsize: It is a time-dependent covariate and its column carries the\nnumber of source Lines of Code of the C++ classes\nat time \'start\'. Blank and comment lines are not counted.\n\nstate: Initially set to 0, and it becomes 1 after the class\nexperiences an event, and remains at 1 thereafter.', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:57:07', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53929/mozilla4.arff', 'true', 48, 'state', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:57:07'),
(49, 2, NULL, 'pc4', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\n%-*- text -*-\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nThis is a PROMISE data set made publicly available in order to encourage\nrepeatable, verifiable, refutable, and/or improvable predictive models\nof software engineering.\n\nIf you publish material based on PROMISE data sets then, please\nfollow the acknowledgment guidelines posted on the PROMISE repository\nweb page http://promise.site.uottawa.ca/SERepository .\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n1. Title/Topic: PC4/ defect prediction\n\n(c) 2007 : Tim Menzies  : tim@menzies.us\nThis data set is distributed under the\nCreative Commons Attribution-Share Alike 3.0 License\nhttp://creativecommons.org/licenses/by-sa/3.0/\n\nYou are free:\n\n* to Share -- copy, distribute and transmit the work\n* to Remix -- to adapt the work\n\nUnder the following conditions:\n\nAttribution. You must attribute the work in the manner specified by\nthe author or licensor (but not in any way that suggests that they endorse\nyou or your use of the work).\n\nShare Alike. If you alter, transform, or build upon this work, you\nmay distribute the resulting work only under the same, similar or a\ncompatible license.\n\n* For any reuse or distribution, you must make clear to others the\nlicense terms of this work.\n* Any of the above conditions can be waived if you get permission from\nthe copyright holder.\n* Apart from the remix rights granted under this license, nothing in\nthis license impairs or restricts the author\'s moral rights.\nFor more deatils on this data set, see\nhttp://promisedata.org/repository/data/kc2/kc2.arff', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:57:12', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53932/pc4.arff', 'true', 49, 'c', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:57:12'),
(50, 2, NULL, 'pc3', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\n%-*- text -*-\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nThis is a PROMISE data set made publicly available in order to encourage\nrepeatable, verifiable, refutable, and/or improvable predictive models\nof software engineering.\n\nIf you publish material based on PROMISE data sets then, please\nfollow the acknowledgment guidelines posted on the PROMISE repository\nweb page http://promise.site.uottawa.ca/SERepository .\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n1. Title/Topic: PC3/software defect prediction\n\n(c) 2007 : Tim Menzies  : tim@menzies.us\nThis data set is distributed under the\nCreative Commons Attribution-Share Alike 3.0 License\nhttp://creativecommons.org/licenses/by-sa/3.0/\n\nYou are free:\n\n* to Share -- copy, distribute and transmit the work\n* to Remix -- to adapt the work\n\nUnder the following conditions:\n\nAttribution. You must attribute the work in the manner specified by\nthe author or licensor (but not in any way that suggests that they endorse\nyou or your use of the work).\n\nShare Alike. If you alter, transform, or build upon this work, you\nmay distribute the resulting work only under the same, similar or a\ncompatible license.\n\n* For any reuse or distribution, you must make clear to others the\nlicense terms of this work.\n* Any of the above conditions can be waived if you get permission from\nthe copyright holder.\n* Apart from the remix rights granted under this license, nothing in\nthis license impairs or restricts the author\'s moral rights.\nFor more deatils on this data set, see\nhttp://promisedata.org/repository/data/kc2/kc2.arff', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:57:13', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53933/pc3.arff', 'true', 50, 'c', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:57:13'),
(51, 2, NULL, 'jm1', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nThis is a PROMISE data set made publicly available in order to encourage repeatable, verifiable, refutable, and/or improvable predictive models of software engineering.\n\nIf you publish material based on PROMISE data sets then, please follow the acknowledgment guidelines posted on [the PROMISE repository web page](http://promise.site.uottawa.ca/SERepository).\n\n## Title/Topic\nJM1/software defect prediction\n\n\n## Sources\n* **Creators:**  NASA, then the NASA Metrics Data Program, http://mdp.ivv.nasa.gov. \n* **Contacts:** \n  * Mike Chapman, Galaxy Global Corporation (Robert.Chapman@ivv.nasa.gov) +1-304-367-8341\n  * Pat Callis, NASA, NASA project manager for MDP (Patrick.E.Callis@ivv.nasa.gov) +1-304-367-8309\n\n* **Donor:** Tim Menzies (tim@barmag.net)\n\n* **Date:**  December 2nd, 2004\n\n## Past usage\n**_How Good is Your Blind Spot Sampling Policy?_; 2003; Tim Menzies and Justin S. Di Stefano; 2004 IEEE Conference on High Assurance Software Engineering (http://menzies.us/pdf/03blind.pdf).**\n\n### Results:\n* Very simple learners (ROCKY) perform as well in this domain as more sophisticated methods (e.g. J48, model trees, model trees) for predicting detects\n* Many learners have very low false alarm rates.\n* Probability of detection (PD) rises with effort and rarely rises above it.\n* High PDs are associated with high PFs (probability of failure)\n* PD, PF, effort can change significantly while accuracy remains essentially stable\n* With two notable exceptions, detectors learned from one data set (e.g. KC2) have nearly they same properties when applied to another (e.g. PC2, KC2). Exceptions:\n* LinesOfCode measures generate wider inter-data-set variances;\n* Precision\'s inter-data-set variances vary wildly\n\n**_\"Assessing Predictors of Software Defects\"_, T. Menzies and J. DiStefano and A. Orrego and R. Chapman, 2004,**\n**Proceedings, workshop on Predictive Software Models, Chicago, Available from http://menzies.us/pdf/04psm.pdf.**\n\n### Results:\n* From JM1, Naive Bayes generated PDs of 25% with PF of 20%\n* Naive Bayes out-performs J48 for defect detection\n* When learning on more and more data, little improvement is seen after processing 300 examples.\n* PDs are much higher from data collected below the sub-sub-system level.\n* Accuracy is a surprisingly uninformative measure of success for a defect detector. Two detectors with the same accuracy can have widely varying PDs and PFs.\n\n## Relevant information\n* JM1 is written in \"C\" and is a real-time predictive ground system: Uses simulations to generate predictions\n* Data comes from McCabe and Halstead features extractors of source code.  These features were defined in the 70s in an attempt to objectively characterize code features that are associated with software quality. The nature of association is under dispute.\n\nNotes on McCabe and Halstead follow.\n\n* The McCabe and Halstead measures are \"module\"-based where a \"module\" is the smallest unit of functionality. In C or Smalltalk, \"modules\" would be called \"function\" or \"method\" respectively.\n\n* Defect detectors can be assessed according to the following measures:\n\n    module actually has defects\n    +-------------+------------+\n    |     no      |     yes    |\n    +-----+-------------+------------+\n    classifier predicts no defects |  no |      a      |      b     |\n    +-----+-------------+------------+\n    classifier predicts some defects | yes |      c      |      d     |\n    +-----+-------------+------------+\n\n\n    accuracy                   = acc          = (a+d)/(a+b+c+d\n    probability of detection   = pd  = recall = d/(b+d)\n    probability of false alarm = pf           = c/(a+c)\n    precision                  = prec         = d/(c+d)\n    effort                     = amount of code selected by detector = (c.LOC + d.LOC)/(Total LOC)\n\n\nIdeally, detectors have high PDs, low PFs, and low effort. This ideal state rarely happens:\n\n* PD and effort are linked. The more modules that trigger the detector, the higher the PD. However, effort also gets increases\n* High PD or low PF comes at the cost of high PF or low PD (respectively). This linkage can be seen in a standard receiver operator curve (ROC).  Suppose, for example, LOC> x is used as the detector (i.e. we assume large modules have more errors). LOC > x represents a family of detectors. At x=0, EVERY module is predicted to have errors. This detector has a high PD but also a high false alarm rate. At x=0, NO module is predicted to have errors. This detector has a low false alarm rate but won\'t detect anything at all. At 0<x<1, a set of detectors are generated as shown below:\n\n    pd\n    1 |           x  x  x                KEY:\n    |        x     .                   \".\"  denotes the line PD=PF\n    |     x      .                     \"x\"  denotes the roc curve\n    |   x      .                            for a set of detectors\n    |  x     .\n    | x    .\n    | x  .\n    |x .\n    |x\n    x------------------ pf\n    0                   1\n\nNote that:\n* The only way to make no mistakes (PF=0) is to do nothing (PD=0)\n* The only way to catch more detects is to make more mistakes (increasing PD means increasing PF).\n* Our detector bends towards the \"sweet spot\" of <PD=1,PF=0> but does not reach it.\n* The line pf=pd on the above graph represents the \"no information\" line. If pf=pd then the detector is pretty useless. The better the detector, the more it rises above PF=PD towards the \"sweet spot\".\n\nNOTES ON MCCABE/HALSTEAD\n========================\nMcCabe argued that code with complicated pathways are more error-prone.  His metrics therefore reflect the pathways within a code module.\n\n    @Article{mccabe76,\n    title  = \"A Complexity Measure\",\n    author  = \"T.J. McCabe\",\n    pages  = \"308--320\",\n    journal = \"IEEE Transactions on Software Engineering\",\n    year  = \"1976\",\n    volume  = \"2\",\n    month  = \"December\",\n    number  = \"4\"}\n\nHalstead argued that code that is hard to read is more likely to be fault prone. Halstead estimates reading complexity by counting the number of concepts in a module; e.g. number of unique operators.\n\n    @Book{halstead77,\n    Author    = \"M.H. Halstead\",\n    Title    = \"Elements of Software Science\",\n    Publisher = \"Elsevier \",\n    Year    = 1977}\n\nWe study these static code measures since they are useful, easy to use, and widely used:\n\n* USEFUL: E.g. this data set can generate highly accurate predictors for defects\n* EASY TO USE: Static code measures (e.g. lines of code, the McCabe/Halstead measures) can be automatically and cheaply collected.\n* WIDELY USED: Many researchers use static measures to guide software quality predictions (see the reference list in the above \"blind spot\" paper. Verification and validation (V\\&V) textbooks advise using static code complexity measures to decide which modules are worthy of manual inspections.  Further, we know of several large government software contractors that won\'t review software modules _unless_ tools like McCabe predict that they are fault prone.  Hence, defect detectors have a major economic impact when they may force programmers to rewrite code.\n\nNevertheless, the merits of these metrics has been widely criticized.  Static code measures are hardly a complete characterization of the internals of a function. Fenton offers an insightful example where the same functionality is achieved using different programming language constructs resulting in different static measurements for that module. Fenton uses this example to argue the uselessness of static code measures.\n\n    @Book{fenton97,\n    author    = \"N.E. Fenton and S.L. Pfleeger\",\n    title     = {Software metrics: a Rigorous \\& Practical Approach},\n    publisher = {International Thompson Press},\n    year      = {1997}}\n\nAn alternative interpretation of Fenton\'s example is that static measures can never be a definite and certain indicator of the presence of a fault.  Rather, defect detectors based on static measures are best viewed as probabilistic statements that the frequency of faults tends to increase in code modules that trigger the detector.  By definition, such probabilistic statements will are not categorical claims with some a non-zero false alarm rate. The research challenge for data miners is to ensure that these false alarms do not cripple their learned theories.\n\nThe McCabe metrics are a collection of four software metrics: essential complexity, cyclomatic complexity, design complexity and LOC, Lines of Code.\n\n* Cyclomatic Complexity, or \"v(G)\", measures the number of \"linearly independent paths\". A set of paths is said to be linearly independent if no path in the set is a linear combination of any other paths in the set through a program\'s \"flowgraph\". A flowgraph is a directed graph where each node corresponds to a program statement, and each arc indicates the flow of control from one statement to another. \"v(G)\" is calculated by \"v(G) = e - n + 2\" where \"G\" is a program\'s flowgraph, \"e\" is the number of arcs in the flowgraph, and \"n\" is the number of nodes in the flowgraph. The standard McCabes rules (\"v(G)\">10), are used to identify fault-prone module.\n* Essential Complexity, or \"ev(G)$\" is the extent to which a flowgraph can be \"reduced\" by decomposing all the subflowgraphs of $G$ that are \"D-structured primes\". Such \"D-structured primes\" are also sometimes referred to as \"proper one-entry one-exit subflowgraphs\" (for a more thorough discussion of D-primes, see the Fenton text referenced above). \"ev(G)\" is calculated using \"ev(G)= v(G) - m\" where $m$ is the number of subflowgraphs of \"G\" that are D-structured primes.\n* Design Complexity, or \"iv(G)\", is the cyclomatic complexity of a module\'s reduced flowgraph.  The flowgraph, \"G\", of a module is reduced to eliminate any complexity which does not influence the interrelationship between design modules.  According to McCabe, this complexity measurement reflects the modules calling patterns to its immediate subordinate modules.\n* Lines of code is measured according to McCabe\'s line counting conventions.\n\nThe Halstead falls into three groups: the base measures, the derived measures, and lines of code measures.\n\n* Base measures:\n  * mu1             = number of unique operators\n  * mu2             = number of unique operands\n  * N1              = total occurrences of operators\n  * N2              = total occurrences of operands\n  * length     = N  = N1 + N2\n  * vocabulary = mu = mu1 + mu2\n  * Constants set for each function:\n  * mu1\' =  2 = potential operator count (just the function name and the \"return\" operator)\n  * mu2\'      = potential operand count. (the number of arguments to the module)\n\nFor example, the expression \"return max(w+x,x+y)\" has \"N1=4\" operators \"return, max, +,+)\", \"N2=4\" operands (w,x,x,y), \"mu1=3\" unique operators (return, max,+), and \"mu2=3\" unique operands (w,x,y).\n\n* Derived measures:\n  * P = volume = V = N * log2(mu) (the number of mental comparisons needed to write\na program of length N)\n  * V* = volume on minimal implementation = (2 + mu2\')*log2(2 + mu2\')\n  * L  = program length = V*/N\n  * D  = difficulty = 1/L\n  * L\' = 1/D\n  * I  = intelligence = L\'*V\'\n  * E  = effort to write program = V/L\n  * T  = time to write program = E/18 seconds\n\n## Number of instances\n10885\n\n## Number of attributes\n22 (5 different lines of code measure, 3 McCabe metrics, 4 base Halstead measures, 8 derived Halstead measures, a branch-count, and 1 goal field)\n\n## Attribute Information\n1. loc             : numeric % McCabe\'s line count of code\n2. v(g)            : numeric % McCabe \"cyclomatic complexity\"\n3. ev(g)           : numeric % McCabe \"essential complexity\"\n4. iv(g)           : numeric % McCabe \"design complexity\"\n5. n               : numeric % Halstead total operators + operands\n6. v               : numeric % Halstead \"volume\"\n7. l               : numeric % Halstead \"program length\"\n8. d               : numeric % Halstead \"difficulty\"\n9. i               : numeric % Halstead \"intelligence\"\n10. e               : numeric % Halstead \"effort\"\n11. b               : numeric % Halstead\n12. t               : numeric % Halstead\'s time estimator\n13. lOCode          : numeric % Halstead\'s line count\n14. lOComment       : numeric % Halstead\'s count of lines of comments\n15. lOBlank         : numeric % Halstead\'s count of blank lines\n16. lOCodeAndComment: numeric\n17. uniq_Op         : numeric % unique operators\n18. uniq_Opnd       : numeric % unique operands\n19. total_Op        : numeric % total operators\n20. total_Opnd      : numeric % total operands\n21: branchCount     : numeric % of the flow graph\n22. defects         : {false,true} % module has/has not one or more reported defects\n\n## Missing attributes\nNone\n\n## Class Distribution\nThe class value (defects) is discrete\nfalse: 2106 = 19.35%\ntrue:  8779 = 80.65%', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:57:19', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53936/jm1.arff', 'true', 51, 'defects', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:57:19'),
(52, 2, NULL, 'kc2', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\n%-*- text -*-\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nThis is a PROMISE Software Engineering Repository data set made publicly\navailable in order to encourage repeatable, verifiable, refutable, and/or\nimprovable predictive models of software engineering.\n\nIf you publish material based on PROMISE data sets then, please\nfollow the acknowledgment guidelines posted on the PROMISE repository\nweb page http://promise.site.uottawa.ca/SERepository .\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n1. Title/Topic: KC2/software defect prediction\n2. Sources:\n\n-- Creators:  NASA, then the NASA Metrics Data Program,\n-- http://mdp.ivv.nasa.gov. Contacts: Mike Chapman,\nGalaxy Global Corporation (Robert.Chapman@ivv.nasa.gov)\n+1-304-367-8341; Pat Callis, NASA, NASA project manager\nfor MDP (Patrick.E.Callis@ivv.nasa.gov) +1-304-367-8309\n\n-- Donor: Tim Menzies (tim@barmag.net)\n\n-- Date:  December 2 2004\n3. Past usage:\n\n1. How Good is Your Blind  Spot Sampling Policy?; 2003; Tim Menzies\nand Justin S. Di Stefano; 2004 IEEE Conference on High Assurance\nSoftware Engineering (http://menzies.us/pdf/03blind.pdf).\n\n-- Results:\n\n-- Very simple learners (ROCKY) perform as well in this domain\nas more sophisticated methods (e.g. J48, model trees, model\ntrees) for predicting detects\n\n-- Many learners have very low false alarm rates.\n\n-- Probability of detection (PD) rises with effort and rarely\nrises above it.\n\n-- High PDs are associated with high PFs (probability of\nfailure)\n\n-- PD, PF, effort can change significantly while accuracy\nremains essentially stable\n\n-- With two notable exceptions, detectors learned from one\ndata set (e.g. KC2) have nearly they same properties when\napplied to another (e.g. PC2, KC2). Exceptions:\n-- LinesOfCode measures generate wider inter-data-set variances;\n-- Precision\'s inter-data-set variances vary wildly\n\n2. \"Assessing Predictors of Software Defects\", T. Menzies and\nJ. DiStefano and A. Orrego and R. Chapman, 2004,\nProceedings, workshop on Predictive Software Models, Chicago,\nAvailable from http://menzies.us/pdf/04psm.pdf.\n-- Results:\n\n-- From KC2, Naive Bayes generated PDs of 50% with PF of 10%\n\n-- Naive Bayes out-performs J48 for defect detection\n\n-- When learning on more and more data, little improvement is\nseen after processing 300 examples.\n\n-- PDs are much higher from data collected below the sub-sub-\nsystem level.\n\n-- Accuracy is a surprisingly uninformative measure of success\nfor a defect detector. Two detectors with the same accuracy\ncan have widely varying PDs and PFs.\n4. Relevant information:\n\n-- Data from C++ functions. Science data processing; another part\nof the same project as KC1; different personnel than KC1.  Shared\nsome third-party software libraries with KC1, but no other software\noverlap.\n\n-- Data comes from McCabe and Halstead features extractors of\nsource code.  These features were defined in the 70s in an attempt\nto objectively characterize code features that are associated with\nsoftware quality.  The nature of association is under dispute.\nNotes on McCabe and Halstead follow.\n\n-- The McCabe and Halstead measures are \"module\"-based where a\n\"module\" is the smallest unit of functionality. In C or Smalltalk,\n\"modules\" would be called \"function\" or \"method\" respectively.\n\n-- Defect detectors can be assessed according to the following measures:\n\nmodule actually has defects\n+-------------+------------+\n|     no      |     yes    |\n+-----+-------------+------------+\nclassifier predicts no defects |  no |      a      |      b     |\n+-----+-------------+------------+\nclassifier predicts some defects | yes |      c      |      d     |\n+-----+-------------+------------+\n\naccuracy                   = acc          = (a+d)/(a+b+c+d\nprobability of detection   = pd  = recall = d/(b+d)\nprobability of false alarm = pf           = c/(a+c)\nprecision                  = prec         = d/(c+d)\neffort                     = amount of code selected by detector\n= (c.LOC + d.LOC)/(Total LOC)\n\nIdeally, detectors have high PDs, low PFs, and low\neffort. This ideal state rarely happens:\n\n-- PD and effort are linked. The more modules that trigger\nthe detector, the higher the PD. However, effort also gets\nincreases\n\n-- High PD or low PF comes at the cost of high PF or low PD\n(respectively). This linkage can be seen in a standard\nreceiver operator curve (ROC).  Suppose, for example, LOC> x\nis used as the detector (i.e. we assume large modules have\nmore errors). LOC > x represents a family of detectors. At\nx=0, EVERY module is predicted to have errors. This detector\nhas a high PD but also a high false alarm rate. At x=0, NO\nmodule is predicted to have errors. This detector has a low\nfalse alarm rate but won\'t detect anything at all. At 0<x<1,\na set of detectors are generated as shown below:\n\npd\n1 |           x  x  x                KEY:\n|        x     .                   \".\"  denotes the line PD=PF\n|     x      .                     \"x\"  denotes the roc curve\n|   x      .                            for a set of detectors\n|  x     .\n| x    .\n| x  .\n|x .\n|x\nx------------------ pf\n0                   1\n\nNote that:\n\n-- The only way to make no mistakes (PF=0) is to do nothing\n(PD=0)\n\n-- The only way to catch more detects is to make more\nmistakes (increasing PD means increasing PF).\n\n-- Our detector bends towards the \"sweet spot\" of\n<PD=1,PF=0> but does not reach it.\n\n-- The line pf=pd on the above graph represents the \"no information\"\nline. If pf=pd then the detector is pretty useless. The better\nthe detector, the more it rises above PF=PD towards the \"sweet spot\".\n\nNOTES ON MCCABE/HALSTEAD\n========================\nMcCabe argued that code with complicated pathways are more\nerror-prone.  His metrics therefore reflect the pathways within a\ncode module.\n@Article{mccabe76,\ntitle 	= \"A Complexity Measure\",\nauthor 	= \"T.J. McCabe\",\npages 	= \"308--320\",\njournal = \"IEEE Transactions on Software Engineering\",\nyear 	= \"1976\",\nvolume 	= \"2\",\nmonth 	= \"December\",\nnumber 	= \"4\"}\n\nHalstead argued that code that is hard to read is more likely to be\nfault prone. Halstead estimates reading complexity by counting the\nnumber of concepts in a module; e.g. number of unique operators.\n@Book{halstead77,\nAuthor 	  = \"M.H. Halstead\",\nTitle 	  = \"Elements of Software Science\",\nPublisher = \"Elsevier \",\nYear 	  = 1977}\n\nWe study these static code measures since they are useful, easy to\nuse, and widely used:\n\n-- USEFUL: E.g. this data set can generate highly accurate\npredictors for defects\n\n-- EASY TO USE: Static code measures (e.g. lines of code, the\nMcCabe/Halstead measures) can be automatically and cheaply\ncollected.\n\n-- WIDELY USED: Many researchers use static measures to guide\nsoftware quality predictions (see the reference list in the above\n\"blind spot\" paper. Verification and validation (V\\&V) textbooks\nadvise using static code complexity measures to decide which\nmodules are worthy of manual inspections.  Further, we know of\nseveral large government software contractors that won\'t review\nsoftware modules _unless_ tools like McCabe predict that they are\nfault prone.  Hence, defect detectors have a major economic impact\nwhen they may force programmers to rewrite code.\n\nNevertheless, the merits of these metrics has been widely\ncriticized.  Static code measures are hardly a complete\ncharacterization of the internals of a function. Fenton offers an\ninsightful example where the same functionality is achieved using\ndifferent programming language constructs resulting in different\nstatic measurements for that module. Fenton uses this example to\nargue the uselessness of static code measures.\n@book{fenton97,\nauthor    = \"N.E. Fenton and S.L. Pfleeger\",\ntitle     = {Software metrics: a Rigorous \\& Practical Approach},\npublisher = {International Thompson Press},\nyear      = {1997}}\n\nAn alternative interpretation of Fenton\'s example is that static\nmeasures can never be a definite and certain indicator of the\npresence of a fault.  Rather, defect detectors based on static\nmeasures are best viewed as probabilistic statements that the\nfrequency of faults tends to increase in code modules that trigger\nthe detector.  By definition, such probabilistic statements will\nare not categorical claims with some a non-zero false alarm\nrate. The research challenge for data miners is to ensure that\nthese false alarms do not cripple their learned theories.\n\nThe McCabe metrics are a collection of four software metrics:\nessential complexity, cyclomatic complexity, design complexity and\nLOC, Lines of Code.\n\n-- Cyclomatic Complexity, or \"v(G)\", measures the number of\n\"linearly independent paths\". A set of paths is said to be\nlinearly independent if no path in the set is a linear combination\nof any other paths in the set through a program\'s \"flowgraph\". A\nflowgraph is a directed graph where each node corresponds to a\nprogram statement, and each arc indicates the flow of control from\none statement to another. \"v(G)\" is calculated by \"v(G) = e - n + 2\"\nwhere \"G\" is a program\'s flowgraph, \"e\" is the number of arcs in\nthe flowgraph, and \"n\" is the number of nodes in the\nflowgraph. The standard McCabes rules (\"v(G)\">10), are used to\nidentify fault-prone module.\n\n-- Essential Complexity, or \"ev(G)$\" is the extent to which a\nflowgraph can be \"reduced\" by decomposing all the subflowgraphs\nof $G$ that are \"D-structured primes\". Such \"D-structured\nprimes\" are also sometimes referred to as \"proper one-entry\none-exit subflowgraphs\" (for a more thorough discussion of\nD-primes, see the Fenton text referenced above). \"ev(G)\" is\ncalculated using \"ev(G)= v(G) - m\" where $m$ is the number of\nsubflowgraphs of \"G\" that are D-structured primes.\n\n-- Design Complexity, or \"iv(G)\", is the cyclomatic complexity of a\nmodule\'s reduced flowgraph.  The flowgraph, \"G\", of a module is\nreduced to eliminate any complexity which does not influence the\ninterrelationship between design modules.  According to McCabe,\nthis complexity measurement reflects the modules calling patterns\nto its immediate subordinate modules.\n\n-- Lines of code is measured according to McCabe\'s line counting\nconventions.\n\nThe Halstead falls into three groups: the base measures, the\nderived measures, and lines of code measures.\n\n-- Base measures:\n-- mu1             = number of unique operators\n-- mu2             = number of unique operands\n-- N1              = total occurrences of operators\n-- N2              = total occurrences of operands\n-- length     = N  = N1 + N2\n-- vocabulary = mu = mu1 + mu2\n-- Constants set for each function:\n-- mu1\' =  2 = potential operator count (just the function\nname and the \"return\" operator)\n-- mu2\'      = potential operand count. (the number\nof arguments to the module)\n\nFor example, the expression \"return max(w+x,x+y)\" has \"N1=4\"\noperators \"return, max, +,+)\", \"N2=4\" operands (w,x,x,y),\n\"mu1=3\" unique operators (return, max,+), and \"mu2=3\" unique\noperands (w,x,y).\n\n-- Derived measures:\n-- P = volume = V = N * log2(mu) (the number of mental\ncomparisons needed to write\na program of length N)\n-- V* = volume on minimal implementation\n= (2 + mu2\')*log2(2 + mu2\')\n-- L  = program length = V*/N\n-- D  = difficulty = 1/L\n-- L\' = 1/D\n-- I  = intelligence = L\'*V\'\n-- E  = effort to write program = V/L\n-- T  = time to write program = E/18 seconds\n5. Number of instances: 522\n6. Number of attributes: 22 (5 different lines of code measure,\n3 McCabe metrics, 4 base Halstead measures, 8 derived\nHalstead measures, a branch-count, and 1 goal field)\n7. Attribute Information:\n\n\n1. loc             : numeric % McCabe\'s line count of code\n2. v(g)            : numeric % McCabe \"cyclomatic complexity\"\n3. ev(g)           : numeric % McCabe \"essential complexity\"\n4. iv(g)           : numeric % McCabe \"design complexity\"\n5. n               : numeric % Halstead total operators + operands\n6. v               : numeric % Halstead \"volume\"\n7. l               : numeric % Halstead \"program length\"\n8. d               : numeric % Halstead \"difficulty\"\n9. i               : numeric % Halstead \"intelligence\"\n10. e               : numeric % Halstead \"effort\"\n11. b               : numeric % Halstead\n12. t               : numeric % Halstead\'s time estimator\n13. lOCode          : numeric % Halstead\'s line count\n14. lOComment       : numeric % Halstead\'s count of lines of comments\n15. lOBlank         : numeric % Halstead\'s count of blank lines\n16. lOCodeAndComment: numeric\n17. uniq_Op         : numeric % unique operators\n18. uniq_Opnd       : numeric % unique operands\n19. total_Op        : numeric % total operators\n20. total_Opnd      : numeric % total operands\n21: branchCount     : numeric % of the flow graph\n22. problems        : {no,yes}% module has/has not one or more\n% reported defects\n8. Missing attributes: none\n9. Class Distribution: the class value (problems) is discrete\nyes: 105 = 20.5%\nno:  415 = 79.5%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:57:36', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53946/kc2.arff', 'true', 52, 'problems', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:57:36'),
(53, 2, NULL, 'kc1', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\n%-*- text -*-\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nThis is a PROMISE Software Engineering Repository data set made publicly\navailable in order to encourage repeatable, verifiable, refutable, and/or\nimprovable predictive models of software engineering.\n\nIf you publish material based on PROMISE data sets then, please\nfollow the acknowledgment guidelines posted on the PROMISE repository\nweb page http://promise.site.uottawa.ca/SERepository .\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n1. Title/Topic: KC1/software defect prediction\n2. Sources:\n\n-- Creators:  NASA, then the NASA Metrics Data Program,\n-- http://mdp.ivv.nasa.gov. Contacts: Mike Chapman,\nGalaxy Global Corporation (Robert.Chapman@ivv.nasa.gov)\n+1-304-367-8341; Pat Callis, NASA, NASA project manager\nfor MDP (Patrick.E.Callis@ivv.nasa.gov) +1-304-367-8309\n\n-- Donor: Tim Menzies (tim@barmag.net)\n\n-- Date:  December 2 2004\n3. Past usage:\n\n1. How Good is Your Blind  Spot Sampling Policy?; 2003; Tim Menzies\nand Justin S. Di Stefano; 2004 IEEE Conference on High Assurance\nSoftware Engineering (http://menzies.us/pdf/03blind.pdf).\n\n-- Results:\n\n-- Very simple learners (ROCKY) perform as well in this domain\nas more sophisticated methods (e.g. J48, model trees, model\ntrees) for predicting detects\n\n-- Many learners have very low false alarm rates.\n\n-- Probability of detection (PD) rises with effort and rarely\nrises above it.\n\n-- High PDs are associated with high PFs (probability of\nfailure)\n\n-- PD, PF, effort can change significantly while accuracy\nremains essentially stable\n\n-- With two notable exceptions, detectors learned from one\ndata set (e.g. KC2) have nearly they same properties when\napplied to another (e.g. PC2, KC2). Exceptions:\n-- LinesOfCode measures generate wider inter-data-set variances;\n-- Precision\'s inter-data-set variances vary wildly\n\n2. \"Assessing Predictors of Software Defects\", T. Menzies and\nJ. DiStefano and A. Orrego and R. Chapman, 2004,\nProceedings, workshop on Predictive Software Models, Chicago,\nAvailable from http://menzies.us/pdf/04psm.pdf.\n-- Results:\n\n-- From KC2, Naive Bayes generated PDs of 45% with PF of 10%\n\n-- Naive Bayes out-performs J48 for defect detection\n\n-- When learning on more and more data, little improvement is\nseen after processing 300 examples.\n\n-- PDs are much higher from data collected below the sub-sub-\nsystem level.\n\n-- Accuracy is a surprisingly uninformative measure of success\nfor a defect detector. Two detectors with the same accuracy\ncan have widely varying PDs and PFs.\n4. Relevant information:\n\n\n-- KC1 is a \"C++\" system implementing storage management for\nreceiving and processing ground data\n\n-- Data comes from McCabe and Halstead features extractors of\nsource code.  These features were defined in the 70s in an attempt\nto objectively characterize code features that are associated with\nsoftware quality.  The nature of association is under dispute.\nNotes on McCabe and Halstead follow.\n\n-- The McCabe and Halstead measures are \"module\"-based where a\n\"module\" is the smallest unit of functionality. In C or Smalltalk,\n\"modules\" would be called \"function\" or \"method\" respectively.\n\n-- Defect detectors can be assessed according to the following measures:\n\nmodule actually has defects\n+-------------+------------+\n|     no      |     yes    |\n+-----+-------------+------------+\nclassifier predicts no defects |  no |      a      |      b     |\n+-----+-------------+------------+\nclassifier predicts some defects | yes |      c      |      d     |\n+-----+-------------+------------+\n\naccuracy                   = acc          = (a+d)/(a+b+c+d\nprobability of detection   = pd  = recall = d/(b+d)\nprobability of false alarm = pf           = c/(a+c)\nprecision                  = prec         = d/(c+d)\neffort                     = amount of code selected by detector\n= (c.LOC + d.LOC)/(Total LOC)\n\nIdeally, detectors have high PDs, low PFs, and low\neffort. This ideal state rarely happens:\n\n-- PD and effort are linked. The more modules that trigger\nthe detector, the higher the PD. However, effort also gets\nincreases\n\n-- High PD or low PF comes at the cost of high PF or low PD\n(respectively). This linkage can be seen in a standard\nreceiver operator curve (ROC).  Suppose, for example, LOC> x\nis used as the detector (i.e. we assume large modules have\nmore errors). LOC > x represents a family of detectors. At\nx=0, EVERY module is predicted to have errors. This detector\nhas a high PD but also a high false alarm rate. At x=0, NO\nmodule is predicted to have errors. This detector has a low\nfalse alarm rate but won\'t detect anything at all. At 0<x<1,\na set of detectors are generated as shown below:\n\npd\n1 |           x  x  x                KEY:\n|        x     .                   \".\"  denotes the line PD=PF\n|     x      .                     \"x\"  denotes the roc curve\n|   x      .                            for a set of detectors\n|  x     .\n| x    .\n| x  .\n|x .\n|x\nx------------------ pf\n0                   1\n\nNote that:\n\n-- The only way to make no mistakes (PF=0) is to do nothing\n(PD=0)\n\n-- The only way to catch more detects is to make more\nmistakes (increasing PD means increasing PF).\n\n-- Our detector bends towards the \"sweet spot\" of\n<PD=1,PF=0> but does not reach it.\n\n-- The line pf=pd on the above graph represents the \"no information\"\nline. If pf=pd then the detector is pretty useless. The better\nthe detector, the more it rises above PF=PD towards the \"sweet spot\".\n\nNOTES ON MCCABE/HALSTEAD\n========================\nMcCabe argued that code with complicated pathways are more\nerror-prone.  His metrics therefore reflect the pathways within a\ncode module.\n@Article{mccabe76,\ntitle 	= \"A Complexity Measure\",\nauthor 	= \"T.J. McCabe\",\npages 	= \"308--320\",\njournal = \"IEEE Transactions on Software Engineering\",\nyear 	= \"1976\",\nvolume 	= \"2\",\nmonth 	= \"December\",\nnumber 	= \"4\"}\n\nHalstead argued that code that is hard to read is more likely to be\nfault prone. Halstead estimates reading complexity by counting the\nnumber of concepts in a module; e.g. number of unique operators.\n@Book{halstead77,\nAuthor 	  = \"M.H. Halstead\",\nTitle 	  = \"Elements of Software Science\",\nPublisher = \"Elsevier \",\nYear 	  = 1977}\n\nWe study these static code measures since they are useful, easy to\nuse, and widely used:\n\n-- USEFUL: E.g. this data set can generate highly accurate\npredictors for defects\n\n-- EASY TO USE: Static code measures (e.g. lines of code, the\nMcCabe/Halstead measures) can be automatically and cheaply\ncollected.\n\n-- WIDELY USED: Many researchers use static measures to guide\nsoftware quality predictions (see the reference list in the above\n\"blind spot\" paper. Verification and validation (V\\&V) textbooks\nadvise using static code complexity measures to decide which\nmodules are worthy of manual inspections.  Further, we know of\nseveral large government software contractors that won\'t review\nsoftware modules _unless_ tools like McCabe predict that they are\nfault prone.  Hence, defect detectors have a major economic impact\nwhen they may force programmers to rewrite code.\n\nNevertheless, the merits of these metrics has been widely\ncriticized.  Static code measures are hardly a complete\ncharacterization of the internals of a function. Fenton offers an\ninsightful example where the same functionality is achieved using\ndifferent programming language constructs resulting in different\nstatic measurements for that module. Fenton uses this example to\nargue the uselessness of static code measures.\n@book{fenton97,\nauthor    = \"N.E. Fenton and S.L. Pfleeger\",\ntitle     = {Software metrics: a Rigorous \\& Practical Approach},\npublisher = {International Thompson Press},\nyear      = {1997}}\n\nAn alternative interpretation of Fenton\'s example is that static\nmeasures can never be a definite and certain indicator of the\npresence of a fault.  Rather, defect detectors based on static\nmeasures are best viewed as probabilistic statements that the\nfrequency of faults tends to increase in code modules that trigger\nthe detector.  By definition, such probabilistic statements will\nare not categorical claims with some a non-zero false alarm\nrate. The research challenge for data miners is to ensure that\nthese false alarms do not cripple their learned theories.\n\nThe McCabe metrics are a collection of four software metrics:\nessential complexity, cyclomatic complexity, design complexity and\nLOC, Lines of Code.\n\n-- Cyclomatic Complexity, or \"v(G)\", measures the number of\n\"linearly independent paths\". A set of paths is said to be\nlinearly independent if no path in the set is a linear combination\nof any other paths in the set through a program\'s \"flowgraph\". A\nflowgraph is a directed graph where each node corresponds to a\nprogram statement, and each arc indicates the flow of control from\none statement to another. \"v(G)\" is calculated by \"v(G) = e - n + 2\"\nwhere \"G\" is a program\'s flowgraph, \"e\" is the number of arcs in\nthe flowgraph, and \"n\" is the number of nodes in the\nflowgraph. The standard McCabes rules (\"v(G)\">10), are used to\nidentify fault-prone module.\n\n-- Essential Complexity, or \"ev(G)$\" is the extent to which a\nflowgraph can be \"reduced\" by decomposing all the subflowgraphs\nof $G$ that are \"D-structured primes\". Such \"D-structured\nprimes\" are also sometimes referred to as \"proper one-entry\none-exit subflowgraphs\" (for a more thorough discussion of\nD-primes, see the Fenton text referenced above). \"ev(G)\" is\ncalculated using \"ev(G)= v(G) - m\" where $m$ is the number of\nsubflowgraphs of \"G\" that are D-structured primes.\n\n-- Design Complexity, or \"iv(G)\", is the cyclomatic complexity of a\nmodule\'s reduced flowgraph.  The flowgraph, \"G\", of a module is\nreduced to eliminate any complexity which does not influence the\ninterrelationship between design modules.  According to McCabe,\nthis complexity measurement reflects the modules calling patterns\nto its immediate subordinate modules.\n\n-- Lines of code is measured according to McCabe\'s line counting\nconventions.\n\nThe Halstead falls into three groups: the base measures, the\nderived measures, and lines of code measures.\n\n-- Base measures:\n-- mu1             = number of unique operators\n-- mu2             = number of unique operands\n-- N1              = total occurrences of operators\n-- N2              = total occurrences of operands\n-- length     = N  = N1 + N2\n-- vocabulary = mu = mu1 + mu2\n-- Constants set for each function:\n-- mu1\' =  2 = potential operator count (just the function\nname and the \"return\" operator)\n-- mu2\'      = potential operand count. (the number\nof arguments to the module)\n\nFor example, the expression \"return max(w+x,x+y)\" has \"N1=4\"\noperators \"return, max, +,+)\", \"N2=4\" operands (w,x,x,y),\n\"mu1=3\" unique operators (return, max,+), and \"mu2=3\" unique\noperands (w,x,y).\n\n-- Derived measures:\n-- P = volume = V = N * log2(mu) (the number of mental\ncomparisons needed to write\na program of length N)\n-- V* = volume on minimal implementation\n= (2 + mu2\')*log2(2 + mu2\')\n-- L  = program length = V*/N\n-- D  = difficulty = 1/L\n-- L\' = 1/D\n-- I  = intelligence = L\'*V\'\n-- E  = effort to write program = V/L\n-- T  = time to write program = E/18 seconds\n5. Number of instances: 2109\n6. Number of attributes: 22 (5 different lines of code measure,\n3 McCabe metrics, 4 base Halstead measures, 8 derived\nHalstead measures, a branch-count, and 1 goal field)\n7. Attribute Information:\n\n1. loc             : numeric % McCabe\'s line count of code\n2. v(g)            : numeric % McCabe \"cyclomatic complexity\"\n3. ev(g)           : numeric % McCabe \"essential complexity\"\n4. iv(g)           : numeric % McCabe \"design complexity\"\n5. n               : numeric % Halstead total operators + operands\n6. v               : numeric % Halstead \"volume\"\n7. l               : numeric % Halstead \"program length\"\n8. d               : numeric % Halstead \"difficulty\"\n9. i               : numeric % Halstead \"intelligence\"\n10. e               : numeric % Halstead \"effort\"\n11. b               : numeric % Halstead\n12. t               : numeric % Halstead\'s time estimator\n13. lOCode          : numeric % Halstead\'s line count\n14. lOComment       : numeric % Halstead\'s count of lines of comments\n15. lOBlank         : numeric % Halstead\'s count of blank lines\n16. lOCodeAndComment: numeric\n17. uniq_Op         : numeric % unique operators\n18. uniq_Opnd       : numeric % unique operands\n19. total_Op        : numeric % total operators\n20. total_Opnd      : numeric % total operands\n21: branchCount     : numeric % of the flow graph\n22. problems        : {false,true}% module has/has not one or more\n% reported defects\n8. Missing attributes: none\n9. Class Distribution: the class value (problems) is discrete\nyes:  326 = 15.45%\nno:  1783 = 84.54%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:57:43', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53950/kc1.arff', 'true', 53, 'defects', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:57:43'),
(54, 2, NULL, 'pc1', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\n%-*- text -*-\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nThis is a PROMISE Software Engineering Repository data set made publicly\navailable in order to encourage repeatable, verifiable, refutable, and/or\nimprovable predictive models of software engineering.\n\nIf you publish material based on PROMISE data sets then, please\nfollow the acknowledgment guidelines posted on the PROMISE repository\nweb page http://promise.site.uottawa.ca/SERepository .\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n1. Title/Topic: PC1/software defect prediction\n2. Sources:\n\n-- Creators:  NASA, then the NASA Metrics Data Program,\n-- http://mdp.ivv.nasa.gov. Contacts: Mike Chapman,\nGalaxy Global Corporation (Robert.Chapman@ivv.nasa.gov)\n+1-304-367-8341; Pat Callis, NASA, NASA project manager\nfor MDP (Patrick.E.Callis@ivv.nasa.gov) +1-304-367-8309\n\n-- Donor: Tim Menzies (tim@barmag.net)\n\n-- Date:  December 2 2004\n3. Past usage:\n\n1. How Good is Your Blind  Spot Sampling Policy?; 2003; Tim Menzies\nand Justin S. Di Stefano; 2004 IEEE Conference on High Assurance\nSoftware Engineering (http://menzies.us/pdf/03blind.pdf).\n\n-- Results:\n\n-- Very simple learners (ROCKY) perform as well in this domain\nas more sophisticated methods (e.g. J48, model trees, model\ntrees) for predicting detects\n\n-- Many learners have very low false alarm rates.\n\n-- Probability of detection (PD) rises with effort and rarely\nrises above it.\n\n-- High PDs are associated with high PFs (probability of\nfailure)\n\n-- PD, PF, effort can change significantly while accuracy\nremains essentially stable\n\n-- With two notable exceptions, detectors learned from one\ndata set (e.g. KC2) have nearly they same properties when\napplied to another (e.g. PC2, KC2). Exceptions:\n-- LinesOfCode measures generate wider inter-data-set variances;\n-- Precision\'s inter-data-set variances vary wildly\n\n2. \"Assessing Predictors of Software Defects\", T. Menzies and\nJ. DiStefano and A. Orrego and R. Chapman, 2004,\nProceedings, workshop on Predictive Software Models, Chicago,\nAvailable from http://menzies.us/pdf/04psm.pdf.\n-- Results:\n\n-- From JM1, Naive Bayes generated PDs of 20% with PF of 25%\n\n-- Naive Bayes out-performs J48 for defect detection\n\n-- When learning on more and more data, little improvement is\nseen after processing 300 examples.\n\n-- PDs are much higher from data collected below the sub-sub-\nsystem level.\n\n-- Accuracy is a surprisingly uninformative measure of success\nfor a defect detector. Two detectors with the same accuracy\ncan have widely varying PDs and PFs.\n4. Relevant information:\n\n-- Data from C  functions. flight software\nfor earth orbiting satellite.\n\n-- Data comes from McCabe and Halstead features extractors of\nsource code.  These features were defined in the 70s in an attempt\nto objectively characterize code features that are associated with\nsoftware quality.  The nature of association is under dispute.\nNotes on McCabe and Halstead follow.\n\n-- The McCabe and Halstead measures are \"module\"-based where a\n\"module\" is the smallest unit of functionality. In C or Smalltalk,\n\"modules\" would be called \"function\" or \"method\" respectively.\n\n-- Defect detectors can be assessed according to the following measures:\n\nmodule actually has defects\n+-------------+------------+\n|     no      |     yes    |\n+-----+-------------+------------+\nclassifier predicts no defects |  no |      a      |      b     |\n+-----+-------------+------------+\nclassifier predicts some defects | yes |      c      |      d     |\n+-----+-------------+------------+\n\naccuracy                   = acc          = (a+d)/(a+b+c+d\nprobability of detection   = pd  = recall = d/(b+d)\nprobability of false alarm = pf           = c/(a+c)\nprecision                  = prec         = d/(c+d)\neffort                     = amount of code selected by detector\n= (c.LOC + d.LOC)/(Total LOC)\n\nIdeally, detectors have high PDs, low PFs, and low\neffort. This ideal state rarely happens:\n\n-- PD and effort are linked. The more modules that trigger\nthe detector, the higher the PD. However, effort also gets\nincreases\n\n-- High PD or low PF comes at the cost of high PF or low PD\n(respectively). This linkage can be seen in a standard\nreceiver operator curve (ROC).  Suppose, for example, LOC> x\nis used as the detector (i.e. we assume large modules have\nmore errors). LOC > x represents a family of detectors. At\nx=0, EVERY module is predicted to have errors. This detector\nhas a high PD but also a high false alarm rate. At x=0, NO\nmodule is predicted to have errors. This detector has a low\nfalse alarm rate but won\'t detect anything at all. At 0<x<1,\na set of detectors are generated as shown below:\n\npd\n1 |           x  x  x                KEY:\n|        x     .                   \".\"  denotes the line PD=PF\n|     x      .                     \"x\"  denotes the roc curve\n|   x      .                            for a set of detectors\n|  x     .\n| x    .\n| x  .\n|x .\n|x\nx------------------ pf\n0                   1\n\nNote that:\n\n-- The only way to make no mistakes (PF=0) is to do nothing\n(PD=0)\n\n-- The only way to catch more detects is to make more\nmistakes (increasing PD means increasing PF).\n\n-- Our detector bends towards the \"sweet spot\" of\n<PD=1,PF=0> but does not reach it.\n\n-- The line pf=pd on the above graph represents the \"no information\"\nline. If pf=pd then the detector is pretty useless. The better\nthe detector, the more it rises above PF=PD towards the \"sweet spot\".\n\nNOTES ON MCCABE/HALSTEAD\n========================\nMcCabe argued that code with complicated pathways are more\nerror-prone.  His metrics therefore reflect the pathways within a\ncode module.\n@Article{mccabe76,\ntitle 	= \"A Complexity Measure\",\nauthor 	= \"T.J. McCabe\",\npages 	= \"308--320\",\njournal = \"IEEE Transactions on Software Engineering\",\nyear 	= \"1976\",\nvolume 	= \"2\",\nmonth 	= \"December\",\nnumber 	= \"4\"}\n\nHalstead argued that code that is hard to read is more likely to be\nfault prone. Halstead estimates reading complexity by counting the\nnumber of concepts in a module; e.g. number of unique operators.\n@Book{halstead77,\nAuthor 	  = \"M.H. Halstead\",\nTitle 	  = \"Elements of Software Science\",\nPublisher = \"Elsevier \",\nYear 	  = 1977}\n\nWe study these static code measures since they are useful, easy to\nuse, and widely used:\n\n-- USEFUL: E.g. this data set can generate highly accurate\npredictors for defects\n\n-- EASY TO USE: Static code measures (e.g. lines of code, the\nMcCabe/Halstead measures) can be automatically and cheaply\ncollected.\n\n-- WIDELY USED: Many researchers use static measures to guide\nsoftware quality predictions (see the reference list in the above\n\"blind spot\" paper. Verification and validation (V\\&V) textbooks\nadvise using static code complexity measures to decide which\nmodules are worthy of manual inspections.  Further, we know of\nseveral large government software contractors that won\'t review\nsoftware modules _unless_ tools like McCabe predict that they are\nfault prone.  Hence, defect detectors have a major economic impact\nwhen they may force programmers to rewrite code.\n\nNevertheless, the merits of these metrics has been widely\ncriticized.  Static code measures are hardly a complete\ncharacterization of the internals of a function. Fenton offers an\ninsightful example where the same functionality is achieved using\ndifferent programming language constructs resulting in different\nstatic measurements for that module. Fenton uses this example to\nargue the uselessness of static code measures.\n@book{fenton97,\nauthor    = \"N.E. Fenton and S.L. Pfleeger\",\ntitle     = {Software metrics: a Rigorous \\& Practical Approach},\npublisher = {International Thompson Press},\nyear      = {1997}}\n\nAn alternative interpretation of Fenton\'s example is that static\nmeasures can never be a definite and certain indicator of the\npresence of a fault.  Rather, defect detectors based on static\nmeasures are best viewed as probabilistic statements that the\nfrequency of faults tends to increase in code modules that trigger\nthe detector.  By definition, such probabilistic statements will\nare not categorical claims with some a non-zero false alarm\nrate. The research challenge for data miners is to ensure that\nthese false alarms do not cripple their learned theories.\n\nThe McCabe metrics are a collection of four software metrics:\nessential complexity, cyclomatic complexity, design complexity and\nLOC, Lines of Code.\n\n-- Cyclomatic Complexity, or \"v(G)\", measures the number of\n\"linearly independent paths\". A set of paths is said to be\nlinearly independent if no path in the set is a linear combination\nof any other paths in the set through a program\'s \"flowgraph\". A\nflowgraph is a directed graph where each node corresponds to a\nprogram statement, and each arc indicates the flow of control from\none statement to another. \"v(G)\" is calculated by \"v(G) = e - n + 2\"\nwhere \"G\" is a program\'s flowgraph, \"e\" is the number of arcs in\nthe flowgraph, and \"n\" is the number of nodes in the\nflowgraph. The standard McCabes rules (\"v(G)\">10), are used to\nidentify fault-prone module.\n\n-- Essential Complexity, or \"ev(G)$\" is the extent to which a\nflowgraph can be \"reduced\" by decomposing all the subflowgraphs\nof $G$ that are \"D-structured primes\". Such \"D-structured\nprimes\" are also sometimes referred to as \"proper one-entry\none-exit subflowgraphs\" (for a more thorough discussion of\nD-primes, see the Fenton text referenced above). \"ev(G)\" is\ncalculated using \"ev(G)= v(G) - m\" where $m$ is the number of\nsubflowgraphs of \"G\" that are D-structured primes.\n\n-- Design Complexity, or \"iv(G)\", is the cyclomatic complexity of a\nmodule\'s reduced flowgraph.  The flowgraph, \"G\", of a module is\nreduced to eliminate any complexity which does not influence the\ninterrelationship between design modules.  According to McCabe,\nthis complexity measurement reflects the modules calling patterns\nto its immediate subordinate modules.\n\n-- Lines of code is measured according to McCabe\'s line counting\nconventions.\n\nThe Halstead falls into three groups: the base measures, the\nderived measures, and lines of code measures.\n\n-- Base measures:\n-- mu1             = number of unique operators\n-- mu2             = number of unique operands\n-- N1              = total occurrences of operators\n-- N2              = total occurrences of operands\n-- length     = N  = N1 + N2\n-- vocabulary = mu = mu1 + mu2\n-- Constants set for each function:\n-- mu1\' =  2 = potential operator count (just the function\nname and the \"return\" operator)\n-- mu2\'      = potential operand count. (the number\nof arguments to the module)\n\nFor example, the expression \"return max(w+x,x+y)\" has \"N1=4\"\noperators \"return, max, +,+)\", \"N2=4\" operands (w,x,x,y),\n\"mu1=3\" unique operators (return, max,+), and \"mu2=3\" unique\noperands (w,x,y).\n\n-- Derived measures:\n-- P = volume = V = N * log2(mu) (the number of mental\ncomparisons needed to write\na program of length N)\n-- V* = volume on minimal implementation\n= (2 + mu2\')*log2(2 + mu2\')\n-- L  = program length = V*/N\n-- D  = difficulty = 1/L\n-- L\' = 1/D\n-- I  = intelligence = L\'*V\'\n-- E  = effort to write program = V/L\n-- T  = time to write program = E/18 seconds\n5. Number of instances: 1109\n6. Number of attributes: 22 (5 different lines of code measure,\n3 McCabe metrics, 4 base Halstead measures, 8 derived\nHalstead measures, a branch-count, and 1 goal field)\n7. Attribute Information:\n\n1. loc             : numeric % McCabe\'s line count of code\n2. v(g)            : numeric % McCabe \"cyclomatic complexity\"\n3. ev(g)           : numeric % McCabe \"essential complexity\"\n4. iv(g)           : numeric % McCabe \"design complexity\"\n5. n               : numeric % Halstead total operators + operands\n6. v               : numeric % Halstead \"volume\"\n7. l               : numeric % Halstead \"program length\"\n8. d               : numeric % Halstead \"difficulty\"\n9. i               : numeric % Halstead \"intelligence\"\n10. e               : numeric % Halstead \"effort\"\n11. b               : numeric % Halstead\n12. t               : numeric % Halstead\'s time estimator\n13. lOCode          : numeric % Halstead\'s line count\n14. lOComment       : numeric % Halstead\'s count of lines of comments\n15. lOBlank         : numeric % Halstead\'s count of blank lines\n16. lOCodeAndComment: numeric\n17. uniq_Op         : numeric % unique operators\n18. uniq_Opnd       : numeric % unique operands\n19. total_Op        : numeric % total operators\n20. total_Opnd      : numeric % total operands\n21: branchCount     : numeric % of the flow graph\n22. defects         : {false,true} % module has/has not one or more\n% reported defects\n8. Missing attributes: none\n9. Class Distribution: the class value (defects) is discrete\nfalse:   77 =  6.94%\ntrue:  1032 = 93.05%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%', 'ARFF', NULL, NULL, NULL, '2014-10-06 23:57:45', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53951/pc1.arff', 'true', 54, 'defects', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-06 23:57:45'),
(55, 2, NULL, 'KDDCup09_churn', '1', NULL, '**Author**: Orange Telecom  \n**Source**: [ACM KDD Cup](http://www.sigkdd.org/kddcup/index.php) - 2009  \n**Please cite**: \n\nThe KDD Cup 2009 offers the opportunity to work on large marketing databases from the French Telecom company Orange to predict the propensity of customers to switch provider (churn). \n\nChurn (wikipedia definition): Churn rate is also sometimes called attrition rate. It is one of two primary factors that determine\nthe steady-state level of customers a business will support. In its broadest sense, churn rate is a measure of the number\nof individuals or items moving into or out of a collection over a specific period of time.\n\nThe term is used in many contexts, but is most widely applied in business with respect to a contractual customer base. For instance, it is an important factor for any business with a subscriber-based service model, including mobile telephone networks and pay TV operators. The term is also used to refer to participant turnover in peer-to-peer networks.\n\nThe training set contains 50,000 examples.\nThe first predictive 190 variables are numerical and the last 40 predictive variables are categorical.\nThe last target variable is binary {-1,1}.', 'ARFF', NULL, NULL, NULL, '2014-10-07 00:08:02', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53995/KDDCup09_churn.arff', 'true', 55, 'CHURN', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-07 00:08:02'),
(56, 2, NULL, 'KDDCup09_upselling', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nDatasets from ACM KDD Cup (http://www.sigkdd.org/kddcup/index.php)\n\nKDD Cup 2009\nhttp://www.kddcup-orange.com\n\nConverted to ARFF format by TunedIT\nCustomer Relationship Management (CRM) is a key element of modern marketing strategies. The KDD Cup 2009 offers the opportunity to work on large marketing databases from the French Telecom company Orange to predict the propensity of customers to switch provider (churn), buy new products or services (appetency), or buy upgrades or add-ons proposed to them to make the sale more profitable (up-selling).\nThe most practical way, in a CRM system, to build knowledge on customer is to produce scores. A score (the output of a model) is an evaluation for all instances of a target variable to explain (i.e. churn, appetency or up-selling). Tools which produce scores allow to project, on a given population, quantifiable information. The score is computed using input variables which describe instances. Scores are then used by the information system (IS), for example, to personalize the customer relationship. An industrial customer analysis platform able to build prediction models with a very large number of input variables has been developed by Orange Labs. This platform implements several processing methods for instances and variables selection, prediction and indexation based on an efficient model combined with variable selection regularization and model averaging method. The main characteristic of this platform is its ability to scale on very large datasets with hundreds of thousands of instances and thousands of variables. The rapid and robust detection of the variables that have most contributed to the output prediction can be a key factor in a marketing application.\nUp-selling (wikipedia definition): Up-selling is a sales technique whereby a salesman attempts to have the customer purchase more expensive\nitems, upgrades, or other add-ons in an attempt to make a more profitable sale.\nUp-selling usually involves marketing more profitable services or products, but up-selling can also be simply exposing the customer\nto other options he or she may not have considered previously.\nUp-selling can imply selling something additional, or selling something that is more profitable or otherwise preferable for the seller instead of the original sale.\nThe training set contains 50,000 examples.\nThe first predictive 190 variables are numerical and the last 40 predictive variables are categorical.\nThe last target variable is binary {-1,1}.', 'ARFF', NULL, NULL, NULL, '2014-10-07 00:08:27', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53997/KDDCup09_upselling.arff', 'true', 56, 'UPSELLING', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-07 00:08:27'),
(57, 2, NULL, 'musk', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nDataset from the MLRR repository: http://axon.cs.byu.edu:5000/', 'ARFF', NULL, NULL, NULL, '2014-10-07 00:41:54', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/53999/musk.arff', 'true', 57, 'class', 'ID', '\"conformation_name\"', NULL, 'public', NULL, NULL, 'ID is a row id', '2015-04-15 17:37:23'),
(58, 2, NULL, 'MagicTelescope', '1', NULL, '**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nDataset from the MLRR repository: http://axon.cs.byu.edu:5000/', 'ARFF', NULL, NULL, NULL, '2014-10-07 00:42:01', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/54003/MagicTelescope.arff', 'true', 58, 'class:', 'ID', NULL, NULL, 'public', NULL, NULL, 'ID is a row id', '2015-04-15 17:41:20'),
(59, 2, NULL, 'Internet-Advertisements', '1', NULL, '**Author**: Nicholas Kushmerick  \n**Source**: [UCI](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements) - 1998  \n**Please cite**:   \n\nThis dataset represents a set of possible advertisements on Internet pages. The features encode the geometry of the image (if available) as well as phrases occurring in the URL, the image\'s URL and alt text, the anchor text, and words occurring near the anchor text. The task is to predict whether an image is an advertisement (\"ad\") or not (\"nonad\").\n\nRelevant Papers: N. Kushmerick (1999). \"Learning to remove Internet advertisements\", 3rd Int Conf Autonomous Agents.  \nAvailable at: http://rexa.info/paper/2fdc1cee89b7f4f2c9227d6f5d9b05d22c5ab3e9', 'ARFF', '\"Nicholas Kushmerick\"', NULL, NULL, '2014-10-30 11:15:44', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/116567/phpCzcrGG', 'true', 59, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-10-30 11:15:44'),
(60, 2, NULL, 'Click_prediction_small', '5', NULL, '**Author**: Tencent Inc.  \n**Source**: [KDD Cup](https://www.kddcup2012.org/) - 2012  \n**Please cite**:   \n\n**0.1% balanced subsample of the original KDD dataset**  \n\nThis data is derived from the 2012 KDD Cup. The data is subsampled to 0.1% of the original number of instances, downsampling the majority class (click=0) so that the target feature is reasonably balanced (5 to 1).\n\nThe data is about advertisements shown alongside search results in a search engine, and whether or not people clicked on these ads. \nThe task is to build the best possible model to predict whether a user will click on a given ad.\n\nA search session contains information on user id, the query issued by the user, ads displayed to the user, and target feature indicating whether a user clicked at least one of the ads in this session. The number of ads displayed to a user in a session is called ‘depth’. The order of an ad in the displayed list is called ‘position’.  An ad is displayed as a short text called ‘title’, followed by a slightly longer text called ’description’, and a URL  called ‘display URL’.   \nTo construct this dataset each session was split into multiple instances. Each instance describes an ad displayed under a certain setting  (‘depth’, ‘position’).  Instances with the same user id, ad id, query, and setting are merged. Each ad and each user have some additional properties located in separate data files that can be looked up using ids in the instances.\n\nThe dataset has the following features:  \n* Click – binary variable indicating whether a user clicked on at least one ad. \n* Impression - the number of search sessions in which AdID was impressed by UserID who issued Query.\n* Url_hash - URL is hashed for anonymity\n* AdID \n* AdvertiserID - some advertisers consistently optimize their ads, so the title and description of their ads are more attractive than those of others’ ads.\n* Depth - number of ads displayed to a user in a session\n* Position - order of an ad in the displayed list\n* QueryID - is the key of the data file \'queryid_tokensid.txt\'. (follow the link to the original KDD Cup page, track 2)\n* KeywordID - is the key of  \'purchasedkeyword_tokensid.txt\' (follow the link to the original KDD Cup page, track 2)\n* TitleID - is the key of \'titleid_tokensid.txt\'\n* DescriptionID - is the key of \'descriptionid_tokensid.txt\' (follow the link to the original KDD Cup page, track 2)\n* UserID – is also the key of \'userid_profile.txt\' (follow the link to the original KDD Cup page, track 2). 0 is a special value denoting that the user could be identified.', 'ARFF', NULL, NULL, NULL, '2014-11-27 01:18:40', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/184157/phpfGCaQC', 'true', 60, 'click', NULL, '\"url_hash\",\"query_id\"', NULL, 'public', NULL, NULL, NULL, '2014-11-27 01:26:36'),
(103, 1, 0, 'kin8nm', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThis is data set is concerned with the forward kinematics of an 8 link\n robot arm. Among the existing variants of this data set we have used\n the variant 8nm, which is known to be highly non-linear and medium\n noisy.\n\n Original source: DELVE repository of data. \n Source: collection of regression datasets by Luis Torgo (ltorgo@ncc.up.pt) at\n http://www.ncc.up.pt/~ltorgo/Regression/DataSets.html\n Characteristics: 8192 cases, 9 attributes (0 nominal, 9 continuous).', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:04', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3626/dataset_2175_kin8nm.arff', 'true', 103, 'y', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:04'),
(104, 1, 0, 'mbagrade', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nDataset from Smoothing Methods in Statistics \n (ftp stat.cmu.edu/datasets)\n\n Simonoff, J.S. (1996). Smoothing Methods in Statistics. New York: Springer-Verlag.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:07', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3627/dataset_2176_mbagrade.arff', 'true', 104, 'grade_point_average', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:07'),
(105, 1, 0, 'wisconsin', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Wisconsin Prognostic Breast Cancer (WPBC)\n \n 2. Source Information\n \n a) Creators: \n \n 	Dr. William H. Wolberg, General Surgery Dept., University of\n 	Wisconsin,  Clinical Sciences Center, Madison, WI 53792\n 	wolberg@eagle.surgery.wisc.edu\n \n 	W. Nick Street, Computer Sciences Dept., University of\n 	Wisconsin, 1210 West Dayton St., Madison, WI 53706\n 	street@cs.wisc.edu  608-262-6619\n \n 	Olvi L. Mangasarian, Computer Sciences Dept., University of\n 	Wisconsin, 1210 West Dayton St., Madison, WI 53706\n 	olvi@cs.wisc.edu \n \n b) Donor: Nick Street\n \n c) Date: December 1995\n \n 3. Past Usage:\n \n 	Various versions of this data have been used in the following\n 	publications: \n \n 	(i) W. N. Street, O. L. Mangasarian, and W.H. Wolberg. \n 	An inductive learning approach to prognostic prediction. \n 	In A. Prieditis and S. Russell, editors, Proceedings of the\n 	Twelfth International Conference on Machine Learning, pages\n 	522--530, San Francisco, 1995. Morgan Kaufmann.\n \n 	(ii) O.L. Mangasarian, W.N. Street and W.H. Wolberg. \n 	Breast cancer diagnosis and prognosis via linear programming. \n 	Operations Research, 43(4), pages 570-577, July-August 1995. \n \n 	(iii) W.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. \n 	Computerized breast cancer diagnosis and prognosis from fine\n 	needle aspirates.  Archives of Surgery 1995;130:511-516. \n \n 	(iv) W.H. Wolberg, W.N. Street, and O.L. Mangasarian. \n 	Image analysis and machine learning applied to breast cancer\n 	diagnosis and prognosis. Analytical and Quantitative Cytology\n 	and Histology, Vol. 17 No. 2, pages 77-87, April 1995.\n \n 	(v) W.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. \n 	Computer-derived nuclear ``grade\'\' and breast cancer prognosis. \n 	Analytical and Quantitative Cytology and Histology, Vol. 17,\n 	pages 257-264, 1995. \n \n See also:\n 	http://www.cs.wisc.edu/~olvi/uwmp/mpml.html\n 	http://www.cs.wisc.edu/~olvi/uwmp/cancer.html\n \n Results:\n \n 	Two possible learning problems:\n \n 	1) Predicting field 2, outcome: R = recurrent, N = nonrecurrent\n 	- Dataset should first be filtered to reflect a particular\n 	endpoint; e.g., recurrences before 24 months = positive,\n 	nonrecurrence beyond 24 months = negative.\n 	- 86.3% accuracy estimated accuracy on 2-year recurrence using\n 	previous version of this data.  Learning method: MSM-T (see\n 	below) in the 4-dimensional space of Mean Texture, Worst Area,\n 	Worst Concavity, Worst Fractal Dimension.\n \n 	2) Predicting Time To Recur (field 3 in recurrent records)\n 	- Estimated mean error 13.9 months using Recurrence Surface\n 	Approximation. (See references (i) and (ii) above)\n \n 4. Relevant information\n \n 	Each record represents follow-up data for one breast cancer\n 	case.  These are consecutive patients seen by Dr. Wolberg\n 	since 1984, and include only those cases exhibiting invasive\n 	breast cancer and no evidence of distant metastases at the\n 	time of diagnosis. \n \n 	The first 30 features are computed from a digitized image of a\n 	fine needle aspirate (FNA) of a breast mass.  They describe\n 	characteristics of the cell nuclei present in the image.\n 	A few of the images can be found at\n 	http://www.cs.wisc.edu/~street/images/\n \n 	The separation described above was obtained using\n 	Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n 	Construction Via Linear Programming.\" Proceedings of the 4th\n 	Midwest Artificial Intelligence and Cognitive Science Society,\n 	pp. 97-101, 1992], a classification method which uses linear\n 	programming to construct a decision tree.  Relevant features\n 	were selected using an exhaustive search in the space of 1-4\n 	features and 1-3 separating planes.\n \n 	The actual linear program used to obtain the separating plane\n 	in the 3-dimensional space is that described in:\n 	[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n 	Programming Discrimination of Two Linearly Inseparable Sets\",\n 	Optimization Methods and Software 1, 1992, 23-34].\n \n 	The Recurrence Surface Approximation (RSA) method is a linear\n 	programming model which predicts Time To Recur using both\n 	recurrent and nonrecurrent cases.  See references (i) and (ii)\n 	above for details of the RSA method. \n \n 	This database is also available through the UW CS ftp server:\n \n 	ftp ftp.cs.wisc.edu\n 	cd math-prog/cpo-dataset/machine-learn/WPBC/\n \n 5. Number of instances: 198\n \n 6. Number of attributes: 34 (ID, outcome, 32 real-valued input features)\n \n 7. Attribute information\n \n 1) ID number\n 2) Outcome (R = recur, N = nonrecur)\n 3) Time (recurrence time if field 2 = R, disease-free time if \n 	field 2	= N)\n 4-33) Ten real-valued features are computed for each cell nucleus:\n \n 	a) radius (mean of distances from center to points on the perimeter)\n 	b) texture (standard deviation of gray-scale values)\n 	c) perimeter\n 	d) area\n 	e) smoothness (local variation in radius lengths)\n 	f) compactness (perimeter^2 / area - 1.0)\n 	g) concavity (severity of concave portions of the contour)\n 	h) concave points (number of concave portions of the contour)\n 	i) symmetry \n 	j) fractal dimension (\"coastline approximation\" - 1)\n \n Several of the papers listed above contain detailed descriptions of\n how these features are computed. \n \n The mean, standard error, and \"worst\" or largest (mean of the three\n largest values) of these features were computed for each image,\n resulting in 30 features.  For instance, field 4 is Mean Radius, field\n 14 is Radius SE, field 24 is Worst Radius.\n \n Values for features 4-33 are recoded with four significant digits.\n \n 34) Tumor size - diameter of the excised tumor in centimeters\n 35) Lymph node status - number of positive axillary lymph nodes\n observed at time of surgery\n \n 8. Missing attribute values: \n 	Lymph node status is missing in 4 cases.\n \n 9. Class distribution: 151 nonrecur, 47 recur\n\n-----------------------------------------------------------------------------------------------------------\n Luis Torgo\'s version: (reconstructed)\n - removed the four instances with unknown values of the last attribute\n - exchanged the attribute position of attributes n.3 (Time) and n.35\n   (Lymph node).\n - removed the attribute outcome as it is the class attribute if the\n   problem is treated as a classification one\n-----------------------------------------------------------------------------------------------------------', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:10', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3628/dataset_2177_wisconsin.arff', 'true', 105, 'time', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:10'),
(106, 1, 0, 'vineyard', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nDataset from Smoothing Methods in Statistics \n (ftp stat.cmu.edu/datasets)\n\n Simonoff, J.S. (1996). Smoothing Methods in Statistics. New York: Springer-Verlag.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:12', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3629/dataset_2178_vineyard.arff', 'true', 106, 'lugs_1991', 'row_number', NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:12'),
(107, 1, 0, 'bolts', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nData from StatLib (ftp stat.cmu.edu/datasets)\n \n SUMMARY:\n \n Data from an experiment on the affects of machine adjustments on\n the time to count bolts.  Data appear as the STATS (Issue 10) Challenge.\n \n DATA:\n \n Submitted by W. Robert Stephenson, Iowa State University\n                             email: wrstephe@iastate.edu\n \n A manufacturer of automotive accessories provides hardware, e.g. nuts,\n bolts, washers and screws, to fasten the accessory to the car or truck.\n Hardware is counted and packaged automatically.  Specifically, bolts\n are dumped into a large metal dish.  A plate that forms the bottom of\n the dish rotates counterclockwise.  This rotation forces bolts to the\n outside of the dish and up along a narrow ledge.  Due to the vibration\n of the dish caused by the spinning bottom plate, some bolts fall off \n the ledge and back into the dish.  The ledge spirals up to a point \n where the bolts are allowed to drop into a pan on a conveyor belt.  \n As a bolt drops, it passes by an electronic eye that counts it.  When \n the electronic counter reaches the preset number of bolts, the\n rotation is stopped and the conveyor belt is moved forward.  \n \n There are several adjustments on the machine that affect its operation.  \n These include; a speed setting that controls the speed of rotation\n (SPEED1) of the plate at the bottom of the dish, a total number of \n bolts (TOTAL) to be counted, a second speed setting (SPEED2) that is \n used to change the speed of rotation (usually slowing it down) for the\n last few bolts, the number of bolts to be counted at this second speed\n (NUMBER2), and the sensitivity of the electronic eye (SENS).  The \n sensitivity setting is to insure that the correct number of bolts are \n counted.  Too few bolts packaged causes customer complaints.  Too many\n bolts packaged increases costs.  For each run conducted in this \n experiment the correct number of bolts was counted.  From an\n engineering standpoint if the correct number of bolts is counted, the \n sensitivity should not affect the time to count bolts.  The measured \n response is the time (TIME), in seconds, it takes to count the desired\n number of bolts.  In order to put times on a equal footing the\n response to be analyzed is the time to count 20 bolts (T20BOLT).\n Below are the data for 40 combinations of settings.  RUN is the order \n in which the data were collected.\n \n Analyze the data.  What adjustments have the greatest effect on the \n time to count 20 bolts?  How would you adjust the machine to get\n the shortest time to count 20 bolts?  Are there any unusual features\n to the data?\n \n The data description and data may be freely used for non-commercial\n purposes and can be freely distributed.  Copyright remains with the\n author and STATS Magazine.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:14', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3630/dataset_2179_bolts.arff', 'true', 107, 'T20BOLT', 'RUN', NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:14'),
(108, 1, 0, 'cleveland', '1', '1', '**Author**: Andras Janosi, M.D.  \nDonor: David W. Aha (aha@ics.uci.edu)  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) - July 1988  \n**Please cite**: The author request that any publications resulting from the use of the data include the name of the author.\n\n**Heart Disease Databases: Cleveland**  \nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to this date.  The \"goal\" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).  \n    \nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\nOne file has been \"processed\", that one containing the Cleveland database. \n     \nAttribute documentation:  \n>\n       1 id: patient identification number\n       2 ccf: social security number (I replaced this with a dummy value of 0)\n       3 age: age in years\n       4 sex: sex (1 = male; 0 = female)\n       5 painloc: chest pain location (1 = substernal; 0 = otherwise)\n       6 painexer (1 = provoked by exertion; 0 = otherwise)\n       7 relrest (1 = relieved after rest; 0 = otherwise)\n       8 pncaden (sum of 5, 6, and 7)\n       9 cp: chest pain type\n         -- Value 1: typical angina\n         -- Value 2: atypical angina\n         -- Value 3: non-anginal pain\n         -- Value 4: asymptomatic\n      10 trestbps: resting blood pressure (in mm Hg on admission to the \n         hospital)\n      11 htn\n      12 chol: serum cholestoral in mg/dl\n      13 smoke: I believe this is 1 = yes; 0 = no (is or is not a smoker)\n      14 cigs (cigarettes per day)\n      15 years (number of years as a smoker)\n      16 fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n      17 dm (1 = history of diabetes; 0 = no such history)\n      18 famhist: family history of coronary artery disease (1 = yes; 0 = no)\n      19 restecg: resting electrocardiographic results\n         -- Value 0: normal\n         -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST \n                     elevation or depression of > 0.05 mV)\n         -- Value 2: showing probable or definite left ventricular hypertrophy\n                     by Estes\' criteria\n      20 ekgmo (month of exercise ECG reading)\n      21 ekgday(day of exercise ECG reading)\n      22 ekgyr (year of exercise ECG reading)\n      23 dig (digitalis used furing exercise ECG: 1 = yes; 0 = no)\n      24 prop (Beta blocker used during exercise ECG: 1 = yes; 0 = no)\n      25 nitr (nitrates used during exercise ECG: 1 = yes; 0 = no)\n      26 pro (calcium channel blocker used during exercise ECG: 1 = yes; 0 = no)\n      27 diuretic (diuretic used used during exercise ECG: 1 = yes; 0 = no)\n      28 proto: exercise protocol\n           1 = Bruce     \n           2 = Kottus\n           3 = McHenry\n           4 = fast Balke\n           5 = Balke\n           6 = Noughton \n           7 = bike 150 kpa min/min  (Not sure if \"kpa min/min\" is what was \n               written!)\n           8 = bike 125 kpa min/min  \n           9 = bike 100 kpa min/min\n          10 = bike 75 kpa min/min\n          11 = bike 50 kpa min/min\n          12 = arm ergometer\n      29 thaldur: duration of exercise test in minutes\n      30 thaltime: time when ST measure depression was noted\n      31 met: mets achieved\n      32 thalach: maximum heart rate achieved\n      33 thalrest: resting heart rate\n      34 tpeakbps: peak exercise blood pressure (first of 2 parts)\n      35 tpeakbpd: peak exercise blood pressure (second of 2 parts)\n      36 dummy\n      37 trestbpd: resting blood pressure\n      38 exang: exercise induced angina (1 = yes; 0 = no)\n      39 xhypo: (1 = yes; 0 = no)\n      40 oldpeak = ST depression induced by exercise relative to rest\n      41 slope: the slope of the peak exercise ST segment\n         -- Value 1: upsloping\n         -- Value 2: flat\n         -- Value 3: downsloping\n      42 rldv5: height at rest\n      43 rldv5e: height at peak exercise\n      44 ca: number of major vessels (0-3) colored by flourosopy\n      45 restckm: irrelevant\n      46 exerckm: irrelevant\n      47 restef: rest raidonuclid (sp?) ejection fraction\n      48 restwm: rest wall (sp?) motion abnormality\n         0 = none\n         1 = mild or moderate\n         2 = moderate or severe\n         3 = akinesis or dyskmem (sp?)\n      49 exeref: exercise radinalid (sp?) ejection fraction\n      50 exerwm: exercise wall (sp?) motion \n      51 thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n      52 thalsev: not used\n      53 thalpul: not used\n      54 earlobe: not used\n      55 cmo: month of cardiac cath (sp?)  (perhaps \"call\")\n      56 cday: day of cardiac cath (sp?)\n      57 cyr: year of cardiac cath (sp?)\n      58 num: diagnosis of heart disease (angiographic disease status)\n         -- Value 0: < 50% diameter narrowing\n         -- Value 1: > 50% diameter narrowing\n         (in any major vessel: attributes 59 through 68 are vessels)\n      59 lmt\n      60 ladprox\n      61 laddist\n      62 diag\n      63 cxmain\n      64 ramus\n      65 om1\n      66 om2\n      67 rcaprox\n      68 rcadist\n      69 lvx1: not used\n      70 lvx2: not used\n      71 lvx3: not used\n      72 lvx4: not used\n      73 lvf: not used\n      74 cathef: not used\n      75 junk: not used\n      76 name: last name of patient \n         (I replaced this with the dummy string \"name\")', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:17', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3631/dataset_2180_cleveland.arff', 'true', 108, 'num', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:17'),
(109, 1, 0, 'auto_price', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThis data set consists of three types of entities:\n (a) the specification of an auto in terms of various characteristics;\n (b) its assigned insurance risk rating,;\n (c) its normalized losses in use as compared to other cars. \n The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially\n assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by\n moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is\n risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year.\n This value is normalized for all autos within a particular size classification (two-door small, station wagons,\n sports/speciality, etc...), and represents the average loss per car per year.\n - Note: Several of the attributes in the database could be used as a \"class\" attribute.\n The original data (from the UCI repository) (http://www.ics.uci.edu/~mlearn/MLSummary.html) has 205 instances\n described by 26 attributes :\n - 15 continuous\n - 1 integer\n - 10 nominal\n The following provides more information on these attributes:\n \n   1. symboling:                 -3, -2, -1, 0, 1, 2, 3.\n   2. normalized-losses:        continuous from 65 to 256.\n   3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\n                                isuzu, jaguar, mazda, mercedes-benz, mercury,\n                                mitsubishi, nissan, peugot, plymouth, porsche,\n                                renault, saab, subaru, toyota, volkswagen, volvo\n   4. fuel-type:                diesel, gas.\n   5. aspiration:               std, turbo.\n   6. num-of-doors:             four, two.\n   7. body-style:               hardtop, wagon, sedan, hatchback,convertible.\n   8. drive-wheels:             4wd, fwd, rwd.\n   9. engine-location:          front, rear.\n  10. wheel-base:               continuous from 86.6 120.9.\n  11. length:                   continuous from 141.1 to 208.1.\n  12. width:                    continuous from 60.3 to 72.3.\n  13. height:                   continuous from 47.8 to 59.8.\n  14. curb-weight:              continuous from 1488 to 4066.\n  15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n  16. num-of-cylinders:         eight, five, four, six, three, twelve, two.\n  17. engine-size:              continuous from 61 to 326.\n  18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n  19. bore:                     continuous from 2.54 to 3.94.\n  20. stroke:                   continuous from 2.07 to 4.17.\n  21. compression-ratio:        continuous from 7 to 23.\n  22. horsepower:               continuous from 48 to 288.\n  23. peak-rpm:                 continuous from 4150 to 6600.\n  24. city-mpg:                 continuous from 13 to 49.\n  25. highway-mpg:              continuous from 16 to 54.\n  26. price:                    continuous from 5118 to 45400.\n \n The original data also has some missing attribute values denoted by \"?\" : \n \n    Attribute #:   Number of instances missing a value:\n    2.             41\n    6.             2\n    19.            4\n    20.            4\n    22.            2\n    23.            2\n    26.            4\n \n I\'ve changed the original data in the following way :\n - All instances with unknowns were removed giving 159 instances.\n - The goal variable is \"price\"\n - All nominal attributes (10) were removed.\n \n Original source: UCI machine learning repository. (http://www.ics.uci.edu/~mlearn/MLSummary.html). \n Source: collection of regression datasets by Luis Torgo (ltorgo@ncc.up.pt) at\n http://www.ncc.up.pt/~ltorgo/Regression/DataSets.html\n Characteristics: 159 cases; 14 continuous variables; 1 nominal vars..', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:20', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3632/dataset_2181_auto_price.arff', 'true', 109, 'price', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:20'),
(110, 1, 0, 'autoMpg', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Identifier attribute deleted.\n\n As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction\n using instance-based learning with encoding length selection. In Progress\n in Connectionist-Based Information Systems. Singapore: Springer-Verlag.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\n 1. Title: Auto-Mpg Data\n \n 2. Sources:\n    (a) Origin:  This dataset was taken from the StatLib library which is\n                 maintained at Carnegie Mellon University. The dataset was \n                 used in the 1983 American Statistical Association Exposition.\n    (c) Date: July 7, 1993\n \n 3. Past Usage:\n     -  See 2b (above)\n     -  Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning.\n        In Proceedings on the Tenth International Conference of Machine \n        Learning, 236-243, University of Massachusetts, Amherst. Morgan\n        Kaufmann.\n \n 4. Relevant Information:\n \n    This dataset is a slightly modified version of the dataset provided in\n    the StatLib library.  In line with the use by Ross Quinlan (1993) in\n    predicting the attribute \"mpg\", 8 of the original instances were removed \n    because they had unknown values for the \"mpg\" attribute.  The original \n    dataset is available in the file \"auto-mpg.data-original\".\n \n    \"The data concerns city-cycle fuel consumption in miles per gallon,\n     to be predicted in terms of 3 multivalued discrete and 5 continuous\n     attributes.\" (Quinlan, 1993)\n \n 5. Number of Instances: 398\n \n 6. Number of Attributes: 9 including the class attribute\n \n 7. Attribute Information:\n \n     1. mpg:           continuous\n     2. cylinders:     multi-valued discrete\n     3. displacement:  continuous\n     4. horsepower:    continuous\n     5. weight:        continuous\n     6. acceleration:  continuous\n     7. model year:    multi-valued discrete\n     8. origin:        multi-valued discrete\n     9. car name:      string (unique for each instance)\n \n 8. Missing Attribute Values:  horsepower has 6 missing values', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:22', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3633/dataset_2182_autoMpg.arff', 'true', 110, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:22'),
(111, 1, 0, 'cpu_act', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe Computer Activity databases are a collection of computer systems\n activity measures. The data was collected from a Sun Sparcstation\n 20/712 with 128 Mbytes of memory running in a multi-user university\n department. Users would typically be doing a large variety of tasks\n ranging from accessing the internet, editing files or running very\n cpu-bound programs.  The data was collected continuously on two\n separate occasions. On both occassions, system activity was gathered\n every 5 seconds. The final dataset is taken from both occasions with\n equal numbers of observations coming from each collection epoch.\n \n System measures used:\n 1. lread - Reads (transfers per second ) between system memory and user memory.\n 2. lwrite - writes (transfers per second) between system memory and user memory.\n 3. scall - Number of system calls of all types per second.\n 4. sread - Number of system read calls per second.\n 5. swrite - Number of system write calls per second . \n 6. fork - Number of system fork calls per second. \n 7. exec - Number of system exec calls per second. \n 8. rchar - Number of characters transferred per second by system read calls.\n 9. wchar - Number of characters transfreed per second by system write calls. \n 10. pgout - Number of page out requests per second.\n 11. ppgout - Number of pages, paged out per second. \n 12. pgfree - Number of pages per second placed on the free list. \n 13. pgscan - Number of pages checked if they can be freed per second.\n 14. atch - Number of page attaches (satisfying a page fault by reclaiming a page in memory) per second.\n 15. pgin - Number of page-in requests per second.\n 16. ppgin - Number of pages paged in per second.\n 17. pflt - Number of page faults caused by protection errors (copy-on-writes). \n 18. vflt - Number of page faults caused by address translation. \n 19. runqsz - Process run queue size.\n 20. freemem - Number of memory pages available to user processes.\n 21. freeswap - Number of disk blocks available for page swapping. \n 22. usr - Portion of time (%) that cpus run in user mode.\n 23. sys - Portion of time (%) that cpus run in system mode.\n 24. wio - Portion of time (%) that cpus are idle waiting for block IO.\n 25. idle - Portion of time (%) that cpus are otherwise idle.\n \n The two different regression tasks obtained from these databases are:\n \n CompAct \n Predict usr, the portion of time that cpus run in user mode from all attributes 1-21.\n \n CompAct(s) \n Predict usr using a restricted number (excluding the paging information (10-18)\n \n Source: collection of regression datasets by Luis Torgo (ltorgo@ncc.up.pt) at\n http://www.ncc.up.pt/~ltorgo/Regression/DataSets.html\n Original source: DELVE repository of data. \n Characteristics: 8192 cases, 22 continuous attributes', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:32', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3634/dataset_2183_cpu_act.arff', 'true', 111, 'usr', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:32'),
(112, 1, 0, 'delta_elevators', '1', '1', '**Author**: Rui Camacho (rcamacho@garfield.fe.up.pt)  \n**Source**: [Regression datasets collection Luis Torgo](http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html)  \n**Please cite**:   \n\nThis data set is also obtained from the task of controlling the ailerons of a F16 aircraft, although the target variable and attributes are different from the ailerons domain. The target variable here is a variation instead of an absolute value, and there was some pre-selection of the attributes.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:39', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/52348/openml_phpmlDSTj', 'true', 112, 'Se', NULL, NULL, NULL, 'public', NULL, NULL, 'set target feature', '2014-09-22 16:13:44'),
(113, 1, 0, 'fruitfly', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Identifier attribute deleted.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n NAME:  Sexual activity and the lifespan of male fruitflies\n TYPE:  Designed (almost factorial) experiment\n SIZE:  125 observations, 5 variables\n \n DESCRIPTIVE ABSTRACT:\n A cost of increased reproduction in terms of reduced longevity has been\n shown for female fruitflies, but not for males.  The flies used were an\n outbred stock.  Sexual activity was manipulated by supplying individual\n males with one or eight receptive virgin females per day.  The\n longevity of these males was compared with that of two control types.\n The first control consisted of two sets of individual males kept with\n one or eight newly inseminated females.  Newly inseminated females will\n not usually remate for at least two days, and thus served as a control\n for any effect of competition with the male for food or space.  The\n second control was a set of individual males kept with no females.\n There were 25 males in each of the five groups, which were treated\n identically in number of anaesthetizations (using CO2) and provision of\n fresh food medium.\n \n SOURCE:\n Figure 2 in the article \"Sexual Activity and the Lifespan of Male\n Fruitflies\" by Linda Partridge and Marion Farquhar.  _Nature_, 294,\n 580-581, 1981.\n \n VARIABLE DESCRIPTIONS:\n Columns  Variable    Description\n -------  --------    -----------\n  1- 2    ID          Serial No. (1-25) within each group of 25\n                      (the order in which data points were abstracted)\n \n  4       PARTNERS    Number of companions (0, 1 or 8)\n \n  6       TYPE        Type of companion\n                        0: newly pregnant female\n                        1: virgin female\n                        9: not applicable (when PARTNERS=0)\n \n  8- 9    LONGEVITY   Lifespan, in days\n \n 11-14    THORAX      Length of thorax, in mm (x.xx)\n \n 16-17    SLEEP       Percentage of each day spent sleeping\n \n \n SPECIAL NOTES:\n `Compliance\' of the males in the two experimental groups was documented\n as follows:  On two days per week throughout the life of each\n experimental male, the females that had been supplied as virgins to\n that male were kept and examined for fertile eggs.  The insemination\n rate declined from approximately 7 females/day at age one week to just\n under 2/day at age eight weeks in the males supplied with eight virgin\n females per day, and from just under 1/day at age one week to\n approximately 0.6/day at age eight weeks in the males supplied with one\n virgin female per day.  These `compliance\' data were not supplied for\n individual males, but the authors say that \"There were no significant\n differences between the individual males within each experimental\n group.\"\n \n STORY BEHIND THE DATA:\n James Hanley found this dataset in _Nature_ and was attracted by the\n way the raw data were presented in classical analysis of covariance\n style in Figure 2.  He read the data points from the graphs and brought\n them to the attention of a colleague with whom he was teaching the\n applied statistics course.  Dr. Liddell thought that with only three\n explanatory variables (THORAX, plus PARTNERS and TYPE to describe the\n five groups), it would not be challenging enough as a data-analysis\n project.  He suggested adding another variable.  James Hanley added\n SLEEP, a variable not mentioned in the published article.  Teachers can\n contact us about the construction of this variable.  (We prefer to\n divulge the details at the end of the data-analysis project.)\n \n Further discussion of the background and pedagogical use of this\n dataset can be found in Hanley (1983) and in Hanley and Shapiro\n (1994).  To obtain the Hanley and Shapiro article, send the one-line\n e-mail message:\n send jse/v2n1/datasets.hanley\n to the address archive@jse.stat.ncsu.edu\n \n PEDAGOGICAL NOTES:\n This has been the most successful and the most memorable dataset we\n have used in an \"applications of statistics\" course, which we have\n taught for ten years.  The most common analysis techniques have been\n analysis of variance, classical analysis of covariance, and multiple\n regression.  Because the variable THORAX is so strong (it explains\n about 1/3 of the variance in LONGEVITY), it is important to consider it\n to increase the precision of between-group contrasts.  When students\n first check and find that the distributions of thorax length, and in\n particular, the mean thorax length, are very similar in the different\n groups, many of them are willing to say (in epidemiological\n terminology) that THORAX is not a confounding variable, and that it can\n be omitted from the analysis.\n \n There is usually lively discussion about the primary contrast.  The\n five groups and their special structure allow opportunities for\n students to understand and verbalize what we mean by the term\n \"statistical interaction.\"\n \n There is also much debate as to whether one should take the SLEEP\n variable into account.  Some students say that it is an `intermediate\'\n variable.  Some students formally test the mean level of SLEEP across\n groups, find one pair where there is a statistically significant\n difference, and want to treat it as a confounding variable.  A few\n students muse about how it was measured.\n \n There is heteroscedasticity in the LONGEVITY variable.\n \n One very observant student (now a professor) argued that THORAX cannot\n be used as a predictor or explanatory variable for the LONGEVITY\n outcome since fruitflies who die young may not be fully grown, i.e., it\n is also an intermediate variable.  One Ph.D. student who had studied\n entomology assured us that fruitflies do not grow longer after birth;\n therefore, the THORAX length is not time-dependent!\n \n Curiously, the dataset has seldom been analyzed using techniques from\n survival analysis.  The fact that there are no censored observations is\n not really an excuse, and one could easily devise a way to introduce\n censoring of LONGEVITY.\n \n REFERENCES:\n Hanley, J. A. (1983), \"Appropriate Uses of Multivariate Analysis,\"\n _Annual Review of Public Health_, 4, 155-180.\n \n Hanley, J. A., and Shapiro, S. H. (1994), \"Sexual Activity and the\n Lifespan of Male Fruitflies:  A Dataset That Gets Attention,\" _Journal\n of Statistics Education_, Volume 2, Number 1.\n \n SUBMITTED BY:\n James A. Hanley and Stanley H. Shapiro\n Department of Epidemiology and Biostatistics\n McGill University\n 1020 Pine Avenue West\n Montreal, Quebec, H3A 1A2\n Canada\n tel: +1 (514) 398-6270 (JH) \n      +1 (514) 398-6272 (SS)\n fax: +1 (514) 398-4503\n INJH@musicb.mcgill.ca, StanS@epid.lan.mcgill.ca', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:42', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3636/dataset_2185_fruitfly.arff', 'true', 113, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:42'),
(114, 1, 0, 'pbc', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Case number deleted. X treated as the class attribute.\n\n As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction\n using instance-based learning with encoding length selection. In Progress\n in Connectionist-Based Information Systems. Singapore: Springer-Verlag.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n NAME:  PBC Data\n SIZE:  418 observations, 20 variables\n \n \n \n DESCRIPTIVE ABSTRACT:\n \n Below is a description of the variables recorded from the Mayo Clinic trial \n in primary biliary cirrhosis (PBC) of the liver conducted between 1974 and \n 1984.  A total of 424 PBC patients, referred to Mayo Clinic during\n that ten-year interval, met eligibility criteria for the randomized placebo \n controlled trial of the drug D-penicillamine. The first 312 cases in the data \n set participated in the randomized trial, and contain largely complete data. \n The additional 112 cases did not participate in the clinical trial, but \n consented to have basic measurements recorded and to be followed for survival.\n Six of those cases were lost to follow-up shortly after diagnosis, so there \n are data here on an additional 106 cases as well as the 312 randomized \n participants. Missing data items are denoted by \".\".  At least one space \n separates each variable in the .DAT file.  Censoring was due to liver \n transplantation for twenty-five subjects with the following case numbers: \n 5, 105, 111, 120, 125, 158, 183, 241, 246, 247, 254, 263, 264, 265, 274, \n 288, 291, 295, 297, 345, 361, 362, 375, 380, 383.\n \n \n \n SOURCE:  Counting Processes and Survival Analysis by T. Fleming & \n          D. Harrington, (1991),  published by John Wiley & Sons.\n \n \n \n VARIABLE DESCRIPTIONS:\n \n The data are in free format.  That is, at least one blank space separates\n each variable.  The variables contained in the .DAT are:\n \n \n N:   Case number.\n X:   The number of days between registration and the earlier of\n      death, liver transplantation, or study analysis time in July, 1986.\n D:   1 if X is time to death, 0 if time to censoring\n Z1:  Treatment Code, 1 = D-penicillamine, 2 = placebo.\n Z2:  Age in years. For the first 312 cases, age was calculated by\n      dividing the number of days between birth and study registration by 365.\n Z3:  Sex, 0 = male, 1 = female.\n Z4:  Presence of ascites, 0 = no, 1 = yes.\n Z5:  Presence of hepatomegaly, 0 = no, 1 = yes.\n Z6:  Presence of spiders 0 = no, 1 = Yes.\n Z7:  Presence of edema, 0 = no edema and no diuretic therapy for\n      edema; 0.5 = edema present for which no diuretic therapy was given, or \n      edema resolved with diuretic therapy; 1 = edema despite diuretic therapy\n Z8:  Serum bilirubin, in mg/dl.\n Z9:  Serum cholesterol, in mg/dl.\n Z10: Albumin, in gm/dl.\n Z11: Urine copper, in mg/day.\n Z12: Alkaline phosphatase, in U/liter.\n Z13: SGOT, in U/ml.\n Z14: Triglycerides, in mg/dl.\n Z15: Platelet count; coded value is number of platelets\n      per-cubic-milliliter of blood divided by 1000.\n Z16: Prothrombin time, in seconds.\n Z17: Histologic stage of disease, graded 1, 2, 3, or 4.\n \n \n \n \n STORY BEHIND THE DATA:\n \n Between January, 1974 and May, 1984, the Mayo Clinic conducted a\n double-blinded randomized trial in primary biliary cirrhosis of the liver\n (PBC), comparing the drug D-penicillamine (DPCA) with a placebo. There\n were 424 patients who met the eligibility criteria seen at the Clinic while\n the trial was open for patient registration. Both the treating physician and\n the patient agreed to participate in the randomized trial in 312 of the 424\n cases. The date of randomization and a large number of clinical, biochemical,\n serologic, and histologic parameters were recorded for each of the 312\n clinical trial patients. The data from the trial were analyzed in 1986 for\n presentation in the clinical literature. For that analysis, disease and \n survival status as of July, 1986, were recorded for as many patients as \n possible.  By that date, 125 of the 312 patients had died, with only 11 \n not attributable to PBC.  Eight patients had been lost to follow up, and 19 \n had undergone liver transplantation. \n \n PBC is a rare but fatal chronic liver disease of unknown cause,\n with a prevalence of about 50-cases-per-million population. The primary\n pathologic event appears to be the destruction of interlobular bile ducts,\n which may be mediated by immunologic mechanisms. The data discussed here are\n important in two respects. First, controlled clinical trials are difficult to\n complete in rare diseases, and this case series of patients uniformly\n diagnosed, treated, and followed is the largest existing for PBC. The\n treatment comparison in this trial is more precise than in similar trials\n having fewer participants and avoids the bias that may arise in comparing\n a case series to historical controls. Second, the data present an\n opportunity to study the natural history of the disease. We will see that, \n despite the immunosuppressive properties of DPCA, there are no detectable\n differences between the distributions of survival times for the DPCA and\n placebo treatment groups. This suggests that these groups can be combined\n in studying the association between survival time from randomization and\n clinical and other measurements. In the early to mid 1980s, the rate of \n successful liver transplant increased substantially, and transplant has \n become an effective therapy for PBC. The Mayo Clinic data set is therefore \n one of the last allowing a study of the natural history of PBC in patients \n who were treated with only supportive care or its equivalent. The PBC data \n can be used to: estimate a survival distribution; test for differences \n between two groups; and estimate covariate effects via a regression\n model.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:16:46', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3637/dataset_2186_pbc.arff', 'true', 114, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:16:46'),
(115, 1, 0, 'pol', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThis is a commercial application described in Weiss & Indurkhya (1995). \n The data describes a telecommunication problem. No further information\n is available.\n \n Characteristics: (10000+5000) cases, 49 continuous attributes \n Source: collection of regression datasets by Luis Torgo (ltorgo@ncc.up.pt) at\n http://www.ncc.up.pt/~ltorgo/Regression/DataSets.html\n Original Source: The data in the original format can be obtained \n from http://www.cs.su.oz.au/~nitin', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:01', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3638/dataset_2187_pol.arff', 'true', 115, 'foo', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:01'),
(116, 1, 0, 'autoHorse', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Horsepower treated as the class attribute.\n\n As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction\n using instance-based learning with encoding length selection. In Progress\n in Connectionist-Based Information Systems. Singapore: Springer-Verlag.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n 1. Title: 1985 Auto Imports Database\n \n 2. Source Information:\n    -- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n    -- Date: 19 May 1987\n    -- Sources:\n      1) 1985 Model Import Car and Truck Specifications, 1985 Ward\'s\n         Automotive Yearbook.\n      2) Personal Auto Manuals, Insurance Services Office, 160 Water\n         Street, New York, NY 10038 \n      3) Insurance Collision Report, Insurance Institute for Highway\n         Safety, Watergate 600, Washington, DC 20037\n\n 3. Past Usage:\n    -- Kibler,~D., Aha,~D.~W., & Albert,~M. (1989).  Instance-based prediction\n       of real-valued attributes.  {it Computational Intelligence}, {it 5},\n       51--57.\n          -- Predicted price of car using all numeric and Boolean attributes\n          -- Method: an instance-based learning (IBL) algorithm derived from a\n             localized k-nearest neighbor algorithm.  Compared with a\n             linear regression prediction...so all instances\n             with missing attribute values were discarded.  This resulted with\n             a training set of 159 instances, which was also used as a test\n             set (minus the actual instance during testing).\n          -- Results: Percent Average Deviation Error of Prediction from Actual\n             -- 11.84% for the IBL algorithm\n             -- 14.12% for the resulting linear regression equation\n \n 4. Relevant Information:\n    -- Description\n       This data set consists of three types of entities: (a) the\n       specification of an auto in terms of various characteristics, (b)\n       its assigned insurance risk rating, (c) its normalized losses in use\n       as compared to other cars.  The second rating corresponds to the\n       degree to which the auto is more risky than its price indicates.\n       Cars are initially assigned a risk factor symbol associated with its\n       price.   Then, if it is more risky (or less), this symbol is\n       adjusted by moving it up (or down) the scale.  Actuarians call this\n       process \"symboling\".  A value of +3 indicates that the auto is\n       risky, -3 that it is probably pretty safe.\n \n       The third factor is the relative average loss payment per insured\n       vehicle year.  This value is normalized for all autos within a\n       particular size classification (two-door small, station wagons,\n       sports/speciality, etc...), and represents the average loss per car\n       per year.\n \n    -- Note: Several of the attributes in the database could be used as a\n             \"class\" attribute.\n \n 5. Number of Instances: 205\n \n 6. Number of Attributes: 26 total\n    -- 15 continuous\n    -- 1 integer\n    -- 10 nominal\n \n 7. Attribute Information:     \n      Attribute:                Attribute Range:\n      ------------------        -----------------------------------------------\n   1. symboling:                -3, -2, -1, 0, 1, 2, 3.\n   2. normalized-losses:        continuous from 65 to 256.\n   3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\n                                isuzu, jaguar, mazda, mercedes-benz, mercury,\n                                mitsubishi, nissan, peugot, plymouth, porsche,\n                                renault, saab, subaru, toyota, volkswagen, volvo\n   4. fuel-type:                diesel, gas.\n   5. aspiration:               std, turbo.\n   6. num-of-doors:             four, two.\n   7. body-style:               hardtop, wagon, sedan, hatchback, convertible.\n   8. drive-wheels:             4wd, fwd, rwd.\n   9. engine-location:          front, rear.\n  10. wheel-base:               continuous from 86.6 120.9.\n  11. length:                   continuous from 141.1 to 208.1.\n  12. width:                    continuous from 60.3 to 72.3.\n  13. height:                   continuous from 47.8 to 59.8.\n  14. curb-weight:              continuous from 1488 to 4066.\n  15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n  16. num-of-cylinders:         eight, five, four, six, three, twelve, two.\n  17. engine-size:              continuous from 61 to 326.\n  18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n  19. bore:                     continuous from 2.54 to 3.94.\n  20. stroke:                   continuous from 2.07 to 4.17.\n  21. compression-ratio:        continuous from 7 to 23.\n  22. horsepower:               continuous from 48 to 288.\n  23. peak-rpm:                 continuous from 4150 to 6600.\n  24. city-mpg:                 continuous from 13 to 49.\n  25. highway-mpg:              continuous from 16 to 54.\n  26. price:                    continuous from 5118 to 45400.\n \n 8. Missing Attribute Values: (denoted by \"?\")\n    Attribute #:   Number of instances missing a value:\n    2.             41\n    6.             2\n    19.            4\n    20.            4\n    22.            2\n    23.            2\n    26.            4%', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:04', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3639/dataset_2188_autoHorse.arff', 'true', 116, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:04'),
(117, 1, 0, 'lowbwt', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Identification code deleted. \n\n As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction\n using instance-based learning with encoding length selection. In Progress\n in Connectionist-Based Information Systems. Singapore: Springer-Verlag.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n NAME:  LOW BIRTH WEIGHT DATA\n KEYWORDS:  Logistic Regression\n SIZE:  189 observations, 11 variables\n \n NOTE:\n         These data come from Appendix 1 of Hosmer and Lemeshow (1989).\n These data are copyrighted and must be acknowledged and used accordingly.\n \n DESCRIPTIVE ABSTRACT:\n         The goal of this study was to identify risk factors associated with\n giving birth to a low birth weight baby (weighing less than 2500 grams).\n Data were collected on 189 women, 59 of which had low birth weight babies\n and 130 of which had normal birth weight babies.  Four variables which were\n thought to be of importance were age, weight of the subject at her last\n menstrual period, race, and the number of physician visits during the first\n trimester of pregnancy.\n \n \n SOURCE:\n          Data were collected at Baystate Medical Center, Springfield,\n Massachusetts, during 1986.\n \n \n NOTE:\n           This data set consists of the complete data.  A paired data set\n created from this low birth weight data may be found in plowbwt.dat and\n a 3 to 1 matched data set created from the low birth weight data may be\n found in mlowbwt.dat.\n \n \n \n Table:  Code Sheet for the Variables in the Low Birth Weight Data Set.\n \n Columns   Variable                                              Abbreviation\n -----------------------------------------------------------------------------\n 2-4     Identification Code                                     ID\n    \n 10      Low Birth Weight (0 = Birth Weight ge 2500g,            LOW\n                           l = Birth Weight < 2500g)\n   \n 17-18   Age of the Mother in Years                              AGE\n      \n 23-25   Weight in Pounds at the Last Menstrual Period           LWT\n      \n 32      Race (1 = White, 2 = Black, 3 = Other)                  RACE\n      \n 40      Smoking Status During Pregnancy (1 = Yes, 0 = No)       SMOKE\n      \n 48      History of Premature Labor (0 = None, 1 = One, etc.)    PTL\n      \n 55      History of Hypertension (1 = Yes, 0 = No)               HT\n      \n 61      Presence of Uterine Irritability (1 = Yes, 0 = No)      UI\n      \n 67      Number of Physician Visits During the First Trimester   FTV\n                 (0 = None, 1 = One, 2 = Two, etc.)\n      \n 73-76   Birth Weight in Grams                                   BWT\n -----------------------------------------------------------------------------\n \n PEDAGOGICAL NOTES:\n         These data have been used as an example of fitting a multiple\n logistic regression model.\n \n STORY BEHIND THE DATA:\n         Low birth weight is an outcome that has been of concern to physicians\n for years. This is due to the fact that infant mortality rates and birth\n defect rates are very high for low birth weight babies. A woman\'s behavior\n during pregnancy (including diet, smoking habits, and receiving prenatal care)\n can greatly alter the chances of carrying the baby to term and, consequently,\n of delivering a baby of normal birth weight.\n         The variables identified in the code sheet given in the table have been\n shown to be associated with low birth weight in the obstetrical literature. The\n goal of the current study was to ascertain if these variables were important\n in the population being served by the medical center where the data were\n collected.\n \n \n References:\n \n 1. Hosmer and Lemeshow, Applied Logistic Regression, Wiley, (1989).', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:07', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3640/dataset_2189_lowbwt.arff', 'true', 117, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:07'),
(118, 1, 0, 'cholesterol', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Cholesterol treated as the class attribute.\n\n As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction\n using instance-based learning with encoding length selection. In Progress\n in Connectionist-Based Information Systems. Singapore: Springer-Verlag.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Publication Request: \n    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n    This file describes the contents of the heart-disease directory.\n \n    This directory contains 4 databases concerning heart disease diagnosis.\n    All attributes are numeric-valued.  The data was collected from the\n    four following locations:\n \n      1. Cleveland Clinic Foundation (cleveland.data)\n      2. Hungarian Institute of Cardiology, Budapest (hungarian.data)\n      3. V.A. Medical Center, Long Beach, CA (long-beach-va.data)\n      4. University Hospital, Zurich, Switzerland (switzerland.data)\n \n    Each database has the same instance format.  While the databases have 76\n    raw attributes, only 14 of them are actually used.  Thus I\'ve taken the\n    liberty of making 2 copies of each database: one with all the attributes\n    and 1 with the 14 attributes actually used in past experiments.\n \n    The authors of the databases have requested:\n \n       ...that any publications resulting from the use of the data include the \n       names of the principal investigator responsible for the data collection\n       at each institution.  They would be:\n \n        1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n        2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n        3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n        4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:\n           Robert Detrano, M.D., Ph.D.\n \n    Thanks in advance for abiding by this request.\n \n    David Aha\n    July 22, 1988\n    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n \n 1. Title: Heart Disease Databases\n \n 2. Source Information:\n    (a) Creators: \n        -- 1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n        -- 2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n        -- 3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n        -- 4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:\n              Robert Detrano, M.D., Ph.D.\n    (b) Donor: David W. Aha (aha@ics.uci.edu) (714) 856-8779   \n    (c) Date: July, 1988\n \n 3. Past Usage:\n     1. Detrano,~R., Janosi,~A., Steinbrunn,~W., Pfisterer,~M., Schmid,~J.,\n        Sandhu,~S., Guppy,~K., Lee,~S., & Froelicher,~V. (1989).  {it \n        International application of a new probability algorithm for the \n        diagnosis of coronary artery disease.}  {it American Journal of \n        Cardiology}, {it 64},304--310.\n        -- International Probability Analysis \n        -- Address: Robert Detrano, M.D.\n                    Cardiology 111-C\n                    V.A. Medical Center\n                    5901 E. 7th Street\n                    Long Beach, CA 90028\n        -- Results in percent accuracy: (for 0.5 probability threshold)\n              Data Name:  CDF    CADENZA\n           -- Hungarian   77     74\n              Long beach  79     77\n              Swiss       81     81\n           -- Approximately a 77% correct classification accuracy with a\n              logistic-regression-derived discriminant function\n     2. David W. Aha & Dennis Kibler\n        -- \n           \n           \n           -- Instance-based prediction of heart-disease presence with the \n              Cleveland database\n              -- NTgrowth: 77.0% accuracy\n              --       C4: 74.8% accuracy\n     3. John Gennari\n        -- Gennari, J.~H., Langley, P, & Fisher, D. (1989). Models of\n           incremental concept formation. {it Artificial Intelligence, 40},\n           11--61.\n        -- Results: \n           -- The CLASSIT conceptual clustering system achieved a 78.9% accuracy\n              on the Cleveland database.\n \n 4. Relevant Information:\n      This database contains 76 attributes, but all published experiments\n      refer to using a subset of 14 of them.  In particular, the Cleveland\n      database is the only one that has been used by ML researchers to \n      this date.  The \"goal\" field refers to the presence of heart disease\n      in the patient.  It is integer valued from 0 (no presence) to 4.\n      Experiments with the Cleveland database have concentrated on simply\n      attempting to distinguish presence (values 1,2,3,4) from absence (value\n      0).  \n    \n      The names and social security numbers of the patients were recently \n      removed from the database, replaced with dummy values.\n \n      One file has been \"processed\", that one containing the Cleveland \n      database.  All four unprocessed files also exist in this directory.\n     \n 5. Number of Instances: \n         Database:    # of instances:\n           Cleveland: 303\n           Hungarian: 294\n         Switzerland: 123\n       Long Beach VA: 200\n \n 6. Number of Attributes: 76 (including the predicted attribute)\n \n 7. Attribute Information:\n    -- Only 14 used\n       -- 1. #3  (age)       \n       -- 2. #4  (sex)       \n       -- 3. #9  (cp)        \n       -- 4. #10 (trestbps)  \n       -- 5. #12 (chol)      \n       -- 6. #16 (fbs)       \n       -- 7. #19 (restecg)   \n       -- 8. #32 (thalach)   \n       -- 9. #38 (exang)     \n       -- 10. #40 (oldpeak)   \n       -- 11. #41 (slope)     \n       -- 12. #44 (ca)        \n       -- 13. #51 (thal)      \n       -- 14. #58 (num)       (the predicted attribute)\n \n    -- Complete attribute documentation:\n       1 id: patient identification number\n       2 ccf: social security number (I replaced this with a dummy value of 0)\n       3 age: age in years\n       4 sex: sex (1 = male; 0 = female)\n       5 painloc: chest pain location (1 = substernal; 0 = otherwise)\n       6 painexer (1 = provoked by exertion; 0 = otherwise)\n       7 relrest (1 = relieved after rest; 0 = otherwise)\n       8 pncaden (sum of 5, 6, and 7)\n       9 cp: chest pain type\n         -- Value 1: typical angina\n         -- Value 2: atypical angina\n         -- Value 3: non-anginal pain\n         -- Value 4: asymptomatic\n      10 trestbps: resting blood pressure (in mm Hg on admission to the \n         hospital)\n      11 htn\n      12 chol: serum cholestoral in mg/dl\n      13 smoke: I believe this is 1 = yes; 0 = no (is or is not a smoker)\n      14 cigs (cigarettes per day)\n      15 years (number of years as a smoker)\n      16 fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n      17 dm (1 = history of diabetes; 0 = no such history)\n      18 famhist: family history of coronary artery disease (1 = yes; 0 = no)\n      19 restecg: resting electrocardiographic results\n         -- Value 0: normal\n         -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST \n                     elevation or depression of > 0.05 mV)\n         -- Value 2: showing probable or definite left ventricular hypertrophy\n                     by Estes\' criteria\n      20 ekgmo (month of exercise ECG reading)\n      21 ekgday(day of exercise ECG reading)\n      22 ekgyr (year of exercise ECG reading)\n      23 dig (digitalis used furing exercise ECG: 1 = yes; 0 = no)\n      24 prop (Beta blocker used during exercise ECG: 1 = yes; 0 = no)\n      25 nitr (nitrates used during exercise ECG: 1 = yes; 0 = no)\n      26 pro (calcium channel blocker used during exercise ECG: 1 = yes; 0 = no)\n      27 diuretic (diuretic used used during exercise ECG: 1 = yes; 0 = no)\n      28 proto: exercise protocol\n           1 = Bruce     \n           2 = Kottus\n           3 = McHenry\n           4 = fast Balke\n           5 = Balke\n           6 = Noughton \n           7 = bike 150 kpa min/min  (Not sure if \"kpa min/min\" is what was \n               written!)\n           8 = bike 125 kpa min/min  \n           9 = bike 100 kpa min/min\n          10 = bike 75 kpa min/min\n          11 = bike 50 kpa min/min\n          12 = arm ergometer\n      29 thaldur: duration of exercise test in minutes\n      30 thaltime: time when ST measure depression was noted\n      31 met: mets achieved\n      32 thalach: maximum heart rate achieved\n      33 thalrest: resting heart rate\n      34 tpeakbps: peak exercise blood pressure (first of 2 parts)\n      35 tpeakbpd: peak exercise blood pressure (second of 2 parts)\n      36 dummy\n      37 trestbpd: resting blood pressure\n      38 exang: exercise induced angina (1 = yes; 0 = no)\n      39 xhypo: (1 = yes; 0 = no)\n      40 oldpeak = ST depression induced by exercise relative to rest\n      41 slope: the slope of the peak exercise ST segment\n         -- Value 1: upsloping\n         -- Value 2: flat\n         -- Value 3: downsloping\n      42 rldv5: height at rest\n      43 rldv5e: height at peak exercise\n      44 ca: number of major vessels (0-3) colored by flourosopy\n      45 restckm: irrelevant\n      46 exerckm: irrelevant\n      47 restef: rest raidonuclid (sp?) ejection fraction\n      48 restwm: rest wall (sp?) motion abnormality\n         0 = none\n         1 = mild or moderate\n         2 = moderate or severe\n         3 = akinesis or dyskmem (sp?)\n      49 exeref: exercise radinalid (sp?) ejection fraction\n      50 exerwm: exercise wall (sp?) motion \n      51 thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n      52 thalsev: not used\n      53 thalpul: not used\n      54 earlobe: not used\n      55 cmo: month of cardiac cath (sp?)  (perhaps \"call\")\n      56 cday: day of cardiac cath (sp?)\n      57 cyr: year of cardiac cath (sp?)\n      58 num: diagnosis of heart disease (angiographic disease status)\n         -- Value 0: < 50% diameter narrowing\n         -- Value 1: > 50% diameter narrowing\n         (in any major vessel: attributes 59 through 68 are vessels)\n      59 lmt\n      60 ladprox\n      61 laddist\n      62 diag\n      63 cxmain\n      64 ramus\n      65 om1\n      66 om2\n      67 rcaprox\n      68 rcadist\n      69 lvx1: not used\n      70 lvx2: not used\n      71 lvx3: not used\n      72 lvx4: not used\n      73 lvf: not used\n      74 cathef: not used\n      75 junk: not used\n      76 name: last name of patient \n         (I replaced this with the dummy string \"name\")\n \n 9. Missing Attribute Values: Several.  Distinguished with value -9.0.\n \n 10. Class Distribution:\n         Database:      0   1   2   3   4 Total\n           Cleveland: 164  55  36  35  13   303\n           Hungarian: 188  37  26  28  15   294\n         Switzerland:   8  48  32  30   5   123\n       Long Beach VA:  51  56  41  42  10   200', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:10', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3641/dataset_2190_cholesterol.arff', 'true', 118, 'chol', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:10'),
(119, 1, 0, 'sleep', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nData from StatLib (ftp stat.cmu.edu/datasets)\n\n Data from which conclusions  were  drawn  in  the  article  \"Sleep  in \n Mammals: Ecological and Constitutional Correlates\" by Allison, T.  and \n Cicchetti, D. (1976), _Science_, November 12, vol. 194,  pp.  732-734. \n Includes brain and body  weight,  life  span,  gestation  time,  time \n sleeping, and predation and danger indices for 62 mammals.\n \n \n \n Variables below (from left to right) for Mammals Data Set:\n \n species of animal\n \n body weight in kg\n \n brain weight in g\n \n slow wave (\"nondreaming\") sleep (hrs/day)\n \n paradoxical (\"dreaming\") sleep (hrs/day)\n \n total sleep (hrs/day)  (sum of slow wave and paradoxical sleep)\n \n maximum life span (years)\n \n gestation time (days)\n \n predation index (1-5)\n                 1 = minimum (least likely to be preyed upon)\n                 5 = maximum (most likely to be preyed upon)\n \n sleep exposure index (1-5)\n                 1 = least exposed (e.g. animal sleeps in a \n                     well-protected den)\n                 5 = most exposed\n \n overall danger index (1-5)\n                 (based on the above two indices and other information)\n                 1 = least danger (from other animals)\n                 5 = most danger (from other animals)\n \n Note: Missing values denoted by -999.0\n \n \n For more details, see\n \n Allison, Truett and Cicchetti, Domenic V. (1976), \"Sleep  in  Mammals: \n Ecological and Constitutional  Correlates\",  _Science_,  November  12, \n vol. 194, pp. 732-734.\n \n The above data set can be freely used for non-commercial purposes  and \n can be freely distributed (permission in  writing  obtained  from  Dr. \n Truett Allison).\n \n Submitted by Roger Johnson\n rwjohnso@silver.sdsmt.edu\n\n Total sleep treated as the class attribute. Attributes for slow\n wave and paradoxical sleep have been deleted. (The animal\'s\n name has also been deleted.)', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:12', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3642/dataset_2191_sleep.arff', 'true', 119, 'danger_index', NULL, NULL, NULL, 'public', NULL, NULL, 'corrected target', '2014-10-06 22:07:54'),
(120, 1, 0, 'triazines', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThe problem is to learn a regression equation/rule/tree to predict the\n activity from the descriptive structural attributes.  The data and\n methodology is described in detail in: - King, Ross .D., Hurst,\n Jonathan. D., and Sternberg, Michael.J.E. A comparison of artificial\n intelligence methods for modelling QSARs Applied Artificial\n Intelligence, 1994 (in press).  - Hurst, Jonathan. D., King, Ross\n .D. and Sternberg, Michael.J.E. Quantitative Structure-Activity\n Relationships by neural networks and inductive logic programming:\n 2. The inhibition of dihydrofolate reductase by triazines. Journal of\n Computer Aided Molecular Design, 1994 (in press).\n \n Original source: ?. \n Source: collection of regression datasets by Luis Torgo (ltorgo@ncc.up.pt) at\n http://www.ncc.up.pt/~ltorgo/Regression/DataSets.html\n Characteristics: 186 cases; 61 continuous variables', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:16', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3643/dataset_2192_triazines.arff', 'true', 120, 'activity', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:16'),
(121, 1, 0, 'autoPrice', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n All nominal attributes and instances with missing values are deleted.\n Price treated as the class attribute.\n\n As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction\n using instance-based learning with encoding length selection. In Progress\n in Connectionist-Based Information Systems. Singapore: Springer-Verlag.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n 1. Title: 1985 Auto Imports Database\n \n 2. Source Information:\n    -- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n    -- Date: 19 May 1987\n    -- Sources:\n      1) 1985 Model Import Car and Truck Specifications, 1985 Ward\'s\n         Automotive Yearbook.\n      2) Personal Auto Manuals, Insurance Services Office, 160 Water\n         Street, New York, NY 10038 \n      3) Insurance Collision Report, Insurance Institute for Highway\n         Safety, Watergate 600, Washington, DC 20037\n\n 3. Past Usage:\n    -- Kibler,~D., Aha,~D.~W., & Albert,~M. (1989).  Instance-based prediction\n       of real-valued attributes.  {it Computational Intelligence}, {it 5},\n       51--57.\n          -- Predicted price of car using all numeric and Boolean attributes\n          -- Method: an instance-based learning (IBL) algorithm derived from a\n             localized k-nearest neighbor algorithm.  Compared with a\n             linear regression prediction...so all instances\n             with missing attribute values were discarded.  This resulted with\n             a training set of 159 instances, which was also used as a test\n             set (minus the actual instance during testing).\n          -- Results: Percent Average Deviation Error of Prediction from Actual\n             -- 11.84% for the IBL algorithm\n             -- 14.12% for the resulting linear regression equation\n \n 4. Relevant Information:\n    -- Description\n       This data set consists of three types of entities: (a) the\n       specification of an auto in terms of various characteristics, (b)\n       its assigned insurance risk rating, (c) its normalized losses in use\n       as compared to other cars.  The second rating corresponds to the\n       degree to which the auto is more risky than its price indicates.\n       Cars are initially assigned a risk factor symbol associated with its\n       price.   Then, if it is more risky (or less), this symbol is\n       adjusted by moving it up (or down) the scale.  Actuarians call this\n       process \"symboling\".  A value of +3 indicates that the auto is\n       risky, -3 that it is probably pretty safe.\n \n       The third factor is the relative average loss payment per insured\n       vehicle year.  This value is normalized for all autos within a\n       particular size classification (two-door small, station wagons,\n       sports/speciality, etc...), and represents the average loss per car\n       per year.\n \n    -- Note: Several of the attributes in the database could be used as a\n             \"class\" attribute.\n \n 5. Number of Instances: 205\n \n 6. Number of Attributes: 26 total\n    -- 15 continuous\n    -- 1 integer\n    -- 10 nominal\n \n 7. Attribute Information:     \n      Attribute:                Attribute Range:\n      ------------------        -----------------------------------------------\n   1. symboling:                -3, -2, -1, 0, 1, 2, 3.\n   2. normalized-losses:        continuous from 65 to 256.\n   3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\n                                isuzu, jaguar, mazda, mercedes-benz, mercury,\n                                mitsubishi, nissan, peugot, plymouth, porsche,\n                                renault, saab, subaru, toyota, volkswagen, volvo\n   4. fuel-type:                diesel, gas.\n   5. aspiration:               std, turbo.\n   6. num-of-doors:             four, two.\n   7. body-style:               hardtop, wagon, sedan, hatchback, convertible.\n   8. drive-wheels:             4wd, fwd, rwd.\n   9. engine-location:          front, rear.\n  10. wheel-base:               continuous from 86.6 120.9.\n  11. length:                   continuous from 141.1 to 208.1.\n  12. width:                    continuous from 60.3 to 72.3.\n  13. height:                   continuous from 47.8 to 59.8.\n  14. curb-weight:              continuous from 1488 to 4066.\n  15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n  16. num-of-cylinders:         eight, five, four, six, three, twelve, two.\n  17. engine-size:              continuous from 61 to 326.\n  18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n  19. bore:                     continuous from 2.54 to 3.94.\n  20. stroke:                   continuous from 2.07 to 4.17.\n  21. compression-ratio:        continuous from 7 to 23.\n  22. horsepower:               continuous from 48 to 288.\n  23. peak-rpm:                 continuous from 4150 to 6600.\n  24. city-mpg:                 continuous from 13 to 49.\n  25. highway-mpg:              continuous from 16 to 54.\n  26. price:                    continuous from 5118 to 45400.\n \n 8. Missing Attribute Values: (denoted by \"?\")\n    Attribute #:   Number of instances missing a value:\n    2.             41\n    6.             2\n    19.            4\n    20.            4\n    22.            2\n    23.            2\n    26.            4%', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:18', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3644/dataset_2193_autoPrice.arff', 'true', 121, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:18'),
(122, 1, 0, 'detroit', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nData from StatLib (ftp stat.cmu.edu/datasets)\n\n This is the data set called `DETROIT\' in the book `Subset selection in\n regression\' by Alan J. Miller published in the Chapman & Hall series of\n monographs on Statistics & Applied Probability, no. 40.   The data are\n unusual in that a subset of three predictors can be found which gives a\n very much better fit to the data than the subsets found from the Efroymson\n stepwise algorithm, or from forward selection or backward elimination.\n \n The original data were given in appendix A of `Regression analysis and its\n application: A data-oriented approach\' by Gunst & Mason, Statistics\n textbooks and monographs no. 24, Marcel Dekker.   It has caused problems\n because some copies of the Gunst & Mason book do not contain all of the data,\n and because Miller does not say which variables he used as predictors and\n which is the dependent variable.   (HOM was the dependent variable, and the\n predictors were FTP ... WE)\n \n The data were collected by J.C. Fisher and used in his paper: \"Homicide in\n Detroit: The Role of Firearms\", Criminology, vol.14, 387-400 (1976)\n \n \n The data are on the homicide rate in Detroit for the years 1961-1973.\n FTP    - Full-time police per 100,000 population\n UEMP   - %  unemployed in the population\n MAN    - number of manufacturing workers in thousands\n LIC    - Number of handgun licences per 100,000 population\n GR     - Number of handgun registrations per 100,000 population\n CLEAR  - %  homicides cleared by arrests\n WM     - Number of white males in the population\n NMAN   - Number of non-manufacturing workers in thousands\n GOV    - Number of government workers in thousands\n HE     - Average hourly earnings\n WE     - Average weekly earnings\n \n HOM    - Number of homicides per 100,000 of population\n ACC    - Death rate in accidents per 100,000 population\n ASR    - Number of assaults per 100,000 population\n \n N.B. Each case takes two lines.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:21', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3645/dataset_2194_detroit.arff', 'true', 122, 'ASR', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:21'),
(123, 1, 0, 'quake', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nDataset from Smoothing Methods in Statistics \n (ftp stat.cmu.edu/datasets)\n\n Simonoff, J.S. (1996). Smoothing Methods in Statistics. New York: Springer-Verlag.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:24', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3646/dataset_2195_quake.arff', 'true', 123, 'richter', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:24'),
(124, 1, 0, 'cloud', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nData from StatLib (ftp stat.cmu.edu/datasets)\n\n These data are those collected in a cloud-seeding experiment in Tasmania\n between mid-1964 and January 1971.   Their analysis, using regression\n techniques and permutation tests, is discussed in:\n \n       Miller, A.J., Shaw, D.E., Veitch, L.G. & Smith, E.J. (1979).\n       `Analyzing the results of a cloud-seeding experiment in Tasmania\',\n       Communications in Statistics - Theory & Methods, vol.A8(10),\n       1017-1047.\n \n The rainfalls are period rainfalls in inches.   TE and TW are the east and\n west target areas respectively, while NC, SC and NWC are the corresponding\n rainfalls in the north, south and north-west control areas respectively.\n S = seeded, U = unseeded.\n\n Rain in eastern target region is being treated\n as the class attribute. (Attribute for rain\n in the western target region has been deleted.)', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:26', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3647/dataset_2196_cloud.arff', 'true', 124, 'TE', 'period', NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:26'),
(125, 1, 0, 'longley', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nData from StatLib (ftp stat.cmu.edu/datasets)\n\n The infamous Longley data, \"An appraisal of least-squares programs from\n  the point of view of the user\", JASA, 62(1967) p819-841.\n\n Variables are: Number of people employed   (usually the y variable)\n                GNP implicit price deflator\n                GNP\n                Unemployed\n                Armed forces\n                Non-institutionalized population >=14 years of age\n                Year\n\n Employment is being treated as the class\n attribute.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:28', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3648/dataset_2197_longley.arff', 'true', 125, 'employed', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:28'),
(126, 1, 0, 'diabetes_numeric', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\nThis data set concerns the study of the factors affecting patterns of\n insulin-dependent diabetes mellitus in children.  The objective is to\n investigate the dependence of the level of serum C-peptide on the\n various other factors in order to understand the patterns of residual\n insulin secretion. The response measurement is the logarithm of\n C-peptide concentration (pmol/ml) at the diagnosis, and the predictor\n measurements age and base deficit, a measure of acidity.\n\n Source: collection of regression datasets by Luis Torgo (ltorgo@ncc.up.pt) at\n http://www.ncc.up.pt/~ltorgo/Regression/DataSets.html\n Original source: Book Generalized Additive Models (p.304) by Hastie &\n Tibshirani, Chapman & Hall.  \n Characteristics: 43 cases; 3 continuous variables', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:30', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3649/dataset_2198_diabetes_numeric.arff', 'true', 126, 'c_peptide', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2014-04-23 13:17:30'),
(127, 1, 0, 'pharynx', '1', '1', '**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Case number deleted. \n\n As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction\n using instance-based learning with encoding length selection. In Progress\n in Connectionist-Based Information Systems. Singapore: Springer-Verlag.\n\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n Name:  Pharynx (A clinical Trial in the Trt. of Carcinoma of the Oropharynx).\n SIZE:  195 observations, 13 variables.\n \n \n \n DESCRIPTIVE ABSTRACT:\n \n The .dat file gives the data for a part of a large clinical trial\n carried out by the Radiation Therapy Oncology Group in the United States. \n The full study included patients with squamous carcinoma of 15 sites in \n the mouth and throat, with 16 participating institutions, though only data \n on three sites in the oropharynx reported by the six largest institutions \n are considered here. Patients entering the study were randomly assigned to \n one of two treatment groups, radiation therapy alone or radiation therapy \n together with a chemotherapeutic agent.  One objective of the study was to \n compare the two treatment policies with respect to patient survival.\n \n \n \n SOURCE:  The Statistical Analysis of Failure Time Data, by JD Kalbfleisch\n          & RL Prentice, (1980),  Published by John Wiley & Sons \n \n \n \n VARIABLE DESCRIPTIONS:\n \n The data are in free format.  That is, at least one blank space separates \n each variable in the .dat file.  The variables are as follows:\n \n \n Case:         Case Number\n Inst:         Participating Institution\n sex:          1=male, 2=female\n Treatment:    1=standard, 2=test\n Grade:        1=well differentiated, 2=moderately differentiated, \n               3=poorly differentiated,  9=missing\n Age:          In years at time of diagnosis\n Condition:    1=no disability, 2=restricted work, 3=requires assistance with\n               self care, 4=bed confined,  9=missing\n Site:         1=faucial arch, 2=tonsillar fossa, 3=posterior pillar,\n               4=pharyngeal tongue, 5=posterior wall\n T staging:    1=primary tumor measuring 2 cm or less in largest diameter,\n               2=primary tumor measuring 2 cm to 4 cm in largest diameter with\n               minimal infiltration in depth, 3=primary tumor measuring more \n               than 4 cm, 4=massive invasive tumor\n N staging:    0=no clinical evidence of node metastases, 1=single positive\n               node 3 cm or less in diameter, not fixed, 2=single positive\n               node more than 3 cm in diameter, not fixed, 3=multiple\n               positive nodes or fixed positive nodes \n Entry Date:   Date of study entry: Day of year and year\n Status:       0=censored,  1=dead\n Time:         Survival time in days from day of diagnosis \n \n \n \n \n \n \n STORY BEHIND THE DATA:\n \n Approximately 30% of the survival times are censored owing primarily to \n patients surviving to the time of analysis. Some patients were lost\n to follow-up because the patient moved or transferred to an institution not\n participating in the study, though these cases were relatively rare. From \n a statistical point of view, an important feature of these data is the \n considerable lack of homogeneity between individuals being studied. \n Of course, as part of the study design, certain criteria for patient \n eligibility had to be met which eliminated extremes in the extent of disease, \n but still many factors are not controlled.\n \n This study included measurements of many covariates which would be expected \n to relate to survival experience. Six such variables are given in the data \n (sex, T staging, N staging, age, general condition, and grade).   The site \n of the primary tumor and possible differences between participating \n institutions require consideration as well.\n      \n The T,N staging classification gives a measure of the extent of the tumor at \n the primary site and at regional lymph nodes. T=1, refers to a small primary \n tumor, 2 centimeters or less in largest diameter, whereas T=4 is a massive \n tumor with extension to adjoining tissue. T=2 and T=3 refer to intermediate\n cases. N=0 refers to there being no clinical evidence of a lymph node \n metastasis and N=1, N=2, N=3 indicate, in increasing magnitude, the extent of \n existing lymph node involvement. Patients with classifications T=1,N=0; \n T=1,N=1;  T=2,N=0; or T=2,N=1, or with distant metastases were excluded \n from study.\n \n The variable general condition gives a measure of the functional capacity of \n the patient at the time of diagnosis (1 refers to no disability whereas\n 4 denotes bed confinement; 2 and 3 measure intermediate levels). The variable\n grade is a measure of the degree of differentiation of the tumor (the degree\n to which the tumor cell resembles the host cell) from 1 (well differentiated) \n to 3 (poorly differentiated)\n \n In addition to the primary question whether the combined treatment mode is\n preferable to the conventional radiation therapy, it is of considerable \n interest to determine the extent to which the several covariates relate to\n subsequent survival.  It is also imperative in answering the primary question \n to adjust the survivals for possible imbalance that may be present in the \n study with regard to the other covariates. Such problems are similar to those \n encountered in the classical theory of linear regression and the analysis of \n covariance.  Again, the need to accommodate censoring is an important \n distinguishing point. In many situations it is also important to develop \n nonparametric and robust procedures since there is frequently little empirical\n or theoretical work to support a particular family of failure time \n distributions.', 'ARFF', NULL, NULL, NULL, '2014-04-23 13:17:32', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/3650/dataset_2199_pharynx.arff', 'true', 127, 'class', NULL, '\"Entry\"', NULL, 'public', NULL, NULL, 'set ignore feature', '2014-10-05 00:03:24'),
(128, 1, 0, 'iris', '1', '1', '', 'ARFF', 'R.A. Fisher', NULL, '1936', '2014-04-06 23:23:39', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/61/dataset_61_iris.arff', 'true', 128, 'class', NULL, NULL, 'http://digital.library.adelaide.edu.au/dspace/handle/2440/15227', 'public', NULL, 'https://archive.ics.uci.edu/ml/datasets/Iris', NULL, '2014-04-06 23:23:39'),
(129, 1, NULL, 'iris-challenge', '1', '1', '', 'ARFF', NULL, NULL, NULL, '2018-05-29 19:06:15', NULL, 'CC0', NULL, NULL, 'https://www.openml.org/data/download/19330175/iris-challenge.arff', 'true', 129, 'class', NULL, NULL, NULL, 'public', NULL, NULL, NULL, '2018-05-29 19:06:15'),
(130, 1, 0, 'iris', '2', '1', '', 'ARFF', 'R.A. Fisher', NULL, '1936', '2014-04-06 23:23:39', NULL, 'Public', NULL, NULL, 'https://www.openml.org/data/download/61/dataset_61_iris.arff', 'true', 130, 'class', NULL, NULL, 'http://digital.library.adelaide.edu.au/dspace/handle/2440/15227', 'private', NULL, 'https://archive.ics.uci.edu/ml/datasets/Iris', NULL, '2014-04-06 23:23:39');

